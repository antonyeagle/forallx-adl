%!TEX root = forallxadl.tex

% Æ: pervasive changes to the proof system from the earlier versions; the one here is pretty much the system of Halbach \emph{Logic Manual}, though transposed to a Fitch style system. We used to have a rule of ¬¬E as basic, and ¬E as derived, but proofs looked uglier. 

\part{Natural deduction for {\TFL}}
\label{ch.NDTFL}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{The very idea of natural deduction}\label{s:NDVeryIdea}

\section{Arguments and Reasoning Revisited} % (fold)
\label{sec:arguments_and_reasoning_revisited}

% section arguments_and_reasoning_revisited (end)

Way back in §\ref{s:Valid}, we said that an argument is valid iff it is impossible to make all of the premises true in a valuation, while the conclusion is false. 

In the case of \TFL, this led us to develop truth tables. Each line of a complete truth table corresponds to a valuation. So, when faced with a \TFL\ argument, we have a very direct way to assess whether it is possible to make all of the premises true and the conclusion false: just plod through the truth table.

But truth tables do not necessarily give us much \emph{insight}. Consider two arguments in \TFL:
	\begin{align*}
		P \eor Q, \enot P & \ttherefore Q\\
		P \eiff Q, P & \ttherefore Q.
	\end{align*}
Clearly, these are valid arguments. You can confirm that they are valid by constructing four-line truth tables. But we might say that two arguments, proceeding from different premises with different logical connectives involved, must make use of different principles of implication. What follows from a disjunction is not at all the same as what follows from a biconditional, and it might be nice to keep track of these differences. While a truth table can show \emph{that} an argument is valid, it doesn't really explain why the argument is valid. To explain why $P \eor Q, \enot P \ttherefore Q$ is valid, we have to say something about how disjunction and negation work and interact.

Certainly human reasoning treats disjunctions very differently from biconditionals. While logic is not really about human reasoning, which is more properly a subject matter for psychology, nevertheless we can formally study the different forms of reasoning involved in arguing from sentences with different structures, by asking: \emph{what would it be acceptable to conclude from a premise with a certain formal structure, supposing one cannot give up one's committment to that premise?} 


With truth tables, we only really care about the truth values assigned to whole sentences, since that is what ultimately determines whether there is an entailment. With natural deduction systems, we are thinking more directly about reasoning \emph{from} premises \emph{to} conclusions. We see what the premises ultimately commit us to, and see whether those consequences are sufficient to establish the conclusion. To do this, we manipulate sentences in accordance with rules that we regard as acceptable. Indeed, with just a small `starter pack' of rules of implication, just characterising what a sentence implies, or is implied by, in virtue of its main connective, we might hope to capture all valid arguments. 

\emph{This is a very different way of thinking about arguments.} 

Natural deduction systems promise to give us a better insight – or at least, a different insight – into how arguments work. Consider this pair of arguments:
	\begin{align*}
		\enot P \eor Q,  P & \ttherefore Q\\
		P \eif Q, P & \ttherefore Q.
	\end{align*} From a truth-table perspective, these arguments are indistinguishable: their premises are true in the same valuations, and so are their conclusions. Yet they are distinct from an implicational point of view, since one will involve consideration of the consequences of disjunctions and negations, and the other consideration of the consequences of the conditional. So the natural deduction perspective on arguments promises to give us a more fine-grained analysis of how valid arguments function. 

\section{Efficiency in Natural Deduction} % (fold)
\label{sec:efficiency_in_natural_deduction}

% section efficiency_in_natural_deduction (end)

The move to natural deduction might be motivated by more than the search for insight. It might also be motivated by \emph{practicality}. Consider:
	$$A_1 \eif C_1 \ttherefore (A_1 \eand A_2) \eif (C_1 \eor C_2).$$
To test this argument for validity, you might use a 16-line truth table. If you do it correctly, then you will see that there is no line on which all the premises are true and on which the conclusion is false. So you will know that the argument is valid. (But, as just mentioned, there is a sense in which you will not know \emph{why} the argument is valid.) But now consider:\label{longtt}
	\begin{align*}
		A_1 \eif C_1 \ttherefore\ & (A_1 \eand A_2 \eand A_3 \eand A_4 \eand A_5 \eand A_6 \eand A_7 \eand A_8 \eand A_9 \eand A_{10}) \eif \phantom{(}\\
				&(C_1 \eor C_2 \eor C_3 \eor C_4 \eor C_5 \eor C_6 \eor C_7 \eor C_8 \eor C_9 \eor C_{10}).
	\end{align*}
This argument is also valid – as you can probably tell – but to test it requires a truth table with $2^{20} = 1048576$ lines. In principle, we can set a machine to grind through truth tables and report back when it is finished. In practice, complicated arguments in \TFL\ can become \emph{intractable} if we use truth tables. But there is a very short natural deduction proof of this argument – just 6 lines. You can see it on page \pageref{ndshort}, though it won't make much sense if you skip the intervening pages.

When we get to \FOL, though, the problem gets dramatically worse. There is nothing like the truth table test for \FOL. To assess whether or not an argument is valid, we have to reason about \emph{all} interpretations. But there are infinitely many possible interpretations. We cannot even in principle set a machine to grind through infinitely many possible interpretations and report back when it is finished: it will \emph{never} finish. We either need to come up with some more efficient way of reasoning about all interpretations, or we need to look for something different. Since we already have some motivation for considering the role in arguments of particular premises, rather than all the premises collectively as in the truth-table approach, we will opt for the `something different' path – natural deduction.

\section{Our System of Natural Deduction and its History} % (fold)
\label{sec:our_system_of_natural_deduction_and_its_history}

% section our_system_of_natural_deduction_and_its_history (end)

Rather than reasoning directly about all valuations (in the case of \TFL) or all interpretations (in the case of \FOL), we shall try to select a few basic rules of implication. Some of these will govern the behaviour of the sentential connectives. Others will govern the behaviour of the quantifiers and identity. The resulting system of rules will give us a new way to think about the validity of arguments. 

The modern development of natural deduction dates from simultaneous and unrelated papers by Gerhard Gentzen and Stanisław Jaśkowski in the 1930s.\footnote{Gerhard Gentzen (1934) `Untersuchungen über das logische Schließen', translated as `Investigations into Logical Deduction' in M. E. Szabo (ed.), \emph{The Collected Works of Gerhard Gentzen}, North-Holland, 1969. Stanisław Jaśkowski (1934). `On the rules of suppositions in formal logic', reprinted in Storrs McCall (ed.), \emph{Polish logic 1920–39}, Oxford University Press, 1967.} However, the natural deduction system that we shall consider is based on slightly later work by Frederic Fitch.\footnote{Frederic Fitch (1952) \emph{Symbolic Logic: An introduction}, Ronald Press Company. 

In the design of the present proof system, I drew on earlier versions of \forallx, but also on the natural deduction systems of Jon Barwise and John Etchemendy (1992) \emph{The Language of First-Order Logic}, CSLI; and of Volker Halbach (2010) \emph{The Logic Manual}, Oxford University Press.} 

Natural deduction was so-called because the rules of implication it codifies were seen as reflecting `natural' forms of human reasoning. It must be admitted that no one spontaneously and instinctively reasons in a way that conforms to the rules of natural deduction. But there is one place where these forms of inference are widespread – mathematical reasoning. And it will not surprise you to learn that these systems of inference were introduced initially to codify good practice in mathematical proofs. Don't worry, though: we won't expect that you are already a fluent mathematician. Though some of the rules might be a bit stilted and formal for everyday use, the rationale for each of them is transparent and can be easily understood even by those without extensive mathematical training.


One further thing about the rules we shall give is that they are extremely \emph{simple}. At every stage in a proof, it is trivial to see which rules apply, how to apply them, and what the result of applying them is. While constructing proofs as a whole might take some thought, the individual steps are the kind of thing that can be undertaken by a completely automated process. The development of formal proofs in the early years of the twentieth century emphasised this feature, as a part of a general quest to remove the need for `insight' or `intuition' in mathematical reasoning. As the philosopher and mathematician Alfred North Whitehead expressed his conception of the field, `the ultimate goal of mathematics is to eliminate any need for intelligent thought'. You will see I hope that natural deduction does require some intelligent thought. But you will also see that, because the steps in a proof are trivial and demonstrably correct, that finding a formal proof for a claim is a royal road to mathematical knowledge. 

\keyideas{
	\item A formal proof system can be theoretically useful, as it might give insight into \emph{why} an argument is valid by showing \emph{how} the conclusion can be derived from the premises.
	\item It might also be practically useful, because it can be much faster to produce a single proof demonstrating an entailment than it would be to show that \emph{all} valuations or interpretations making the premises true also make the conclusion true.
	\item A natural deduction proof system aims to use natural and obviously correct rules, which can contribute to the project of establishing the conclusions of proofs as certain knowledge. 
}



\chapter{The Idea of a Formal Proof: Reasoning from Assumptions}\label{c:ass}
We will develop a \define{natural deduction} system. The fundamental idea is that a formal proof involves reasoning from \emph{assumptions} – seeing which sentences follows from which, in virtue of their main connectives. For each connective, there will be \define{introduction} rules, that allow us to prove a sentence that has that connective as the main connective, and \define{elimination} rules, that allow us to prove something given a sentence that has that connective as the main connective. 

Very abstractly, any \define{formal proof} is a sequence of sentences, each accompanied by a `commentary' on that sentence. There are lots of different systems for constructing formal proofs in \TFL, including axiomatic systems, semantic tableaux, Gentzen-style natural deduction, and sequent calculus. Different systems may be more efficient, or simpler to use, or better when one is reasoning about formal proofs rather than constructing them. The system we use here is best for some purposes – it is easy to understand that the rules are correct, it is fairly straightforward, and it has the nice feature that one can reuse one proof within the course of constructing another. All formal proof systems share two important features: \begin{itemize}
	\item The rules are unambiguous; and
	\item The rules apply at some stage of a proof only because of the syntactic structure of the sentences involved.. This makes them ideal for computers to use. 
\end{itemize} These two features make it easy to design a computer program that produces formal proofs, as long as it can parse and analyse expressions of \TFL. No consideration of meanings need be involved, which makes it quite unlike the earlier ways we had of analysing arguments in \TFL, such as truth-tables, which essentially involved understanding the truth-functions that characterise the meanings of the logical connectives of the language.

 In a natural deduction system, some of those sentences may be initial \define{assumptions}, or suppositions that we are making `for the sake of argument'. Other sentences will follow from earlier sentences in the formal proof by applying certain rules we will detail below. The commentary tells us whether the sentence is an assumption, or whether it follows from one of the rules from an earlier sentence or sentences, and specifies which sentence(s) it follows from, and which assumptions it relies on. The last line of the formal proof is the conclusion. Some of the initial assumptions may remain as assumptions or suppositions even at the end of the formal proof –  these \define{undischarged assumptions} will be the premises of an argument that the formal proof demonstrates to be valid and whose conclusion is the last line of the proof. Henceforth, I shall simply call these `proofs', but you should be aware that there are \emph{informal proofs} too.\footnote{Many of the arguments we offer in our metalanguage, quasi-mathematical arguments \emph{about} our formal languages, are proofs. Sometimes people call formal proofs `deductions' or `derivations', to ensure that no one will confuse the metalanguage activity of proving things about our logical languages with the activity of constructing arguments within those languages. But it seems unlikely that anyone in this course will be confused on this point, since we are not offering very many proofs in the metalanguage in the first place!}

We will use a particular graphical representation of natural deduction proofs, one which makes use of `nesting' of sentences to vividly represent which assumptions a particular sentence in a proof is relying on at any given stage, and uses a device of horizontal marks to distinguish assumptions from derived sentences. It will be easier to see how this works by considering some examples!

So consider:
	$$\enot (A \eor B) \ttherefore \enot A \eand \enot B.$$
We shall start a proof by writing an assumption:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)}
\end{proof}
Note that we have numbered the premise, since we shall want to refer back to it. Indeed, every line on a proof is numbered, so that we can refer back to it.



Note also that we have drawn a line underneath the premise. Everything written above the line is an \emph{assumption}. Everything written below the line will either be something which follows from the assumptions we have already made, or it will be some new assumption. 

The vertical line at the side marks the scope of this assumption: any subsequent sentence which is adjacent to a continuation of this vertical line is understood to lie in the \define{range} of this assumption. We might call these vertical lines \define{assumption lines}. If some line in a proof is in the range of an assumption, we will say that it is an \define{active assumption} at that point in the proof. If we are going to write down some sentence that follows from an assumption, we will involve first extending the corresponding assumption line to indicate that it rests on this assumption, or at least is playing a role in the argument connected with this assumption. As we will see, not every sentence in the range of an assumption will intuitively \emph{depend} on the assumption, but many will. \label{nondependence}

We are hoping to conclude that `$\enot A \eand \enot B$'; so we are hoping ultimately to conclude our proof with
\begin{proof}
\hypo{a1}{\enot (A \eor B)}
\have[ ]{c}{\vdots}
	\have[n]{con}{\enot A \eand \enot B}
\end{proof}
for some number $n$. It doesn't matter which line we end on, but we would obviously prefer a short proof to a long one!

Suppose we wanted to consider an argument with more than one premise:
$$A\eor B, \enot (A\eand C), \enot (B \eand \enot D) \ttherefore \enot C\eor D.$$
If our argument has more than one premise, we can write more than one assumption down on successive lines, as long as the small horizontal line comes after all of the assumptions we are making at some stage. The argument has three premises, so we start by writing them all down, numbered, and drawing a line under them. We are hoping to conclude with $C \eor D$ at the end, so we put that down too, though we cannot yet fill in the middle of the proof:
\begin{proof}
	\hypo{a1}{A \eor B}
	\hypo{a2}{\enot (A\eand C)}
	\hypo{a3}{\enot (B \eand \enot D)}
\have[ ]{m}{\vdots}
	\have[n]{con}{\enot C \eor D}
\end{proof}
Note that the single vertical line to the left indicates the range of all of the assumptions lying above the small horizontal line. We could, if we want, have introduced each of the premises as a separate assumption, with its own range: 
\begin{proof}
	\hypo{a1}{A \eor B}
	\open\hypo{a2}{\enot (A\eand C)}
	\open\hypo{a3}{\enot (B \eand \enot D)}
\have[ ]{m}{\vdots}
	\have[n]{con}{\enot C \eor D}
\end{proof}
This can sometimes be useful when we want to see which conclusions follow from some but not all of the premises. It looks a bit different to the earlier proof, but in fact represents the same dependencies: the conclusion is in the range of three assumptions, all of which remain in force throughout the proof.


What remains to do is to explain each of the rules that we can use along the way from premises to conclusion. The rules are divided into two families: those rules that involve making or getting rid of further assumptions that are made `for the sake of argument', and those that do not. The latter class of rules are simpler, so we will begin with those.

\keyideas{
	\item Any formal proof is a sequence of sentences, each of which follows by some relatively simple rules from previous sentences or is licensed in some other way. The final sentence is the conclusion of the proof.
	\item The rules are unambiguous and apply only because of the syntactic structure of the sentences involved: no consideration of meanings need be involved. This makes them ideal for computers to use. 
	\item A formal natural deduction proof is a graphical representation of reasoning from assumptions, in accordance with a strict set of rules for deriving further claims and managing which assumptions are active (`undischarged') at a given point in the proof.
	\item 
	}

\chapter{Basic rules for \textnormal{\TFL}: Rules Without Subproofs}\label{s:BasicTFLns}


\section{Conjunction Introduction}\label{conjint}
Suppose I want to show that Ludwig is reactionary \emph{and} libertarian. One obvious way to do this would be as follows: first I show that Ludwig is reactionary; then I show that Ludwig is libertarian; then I put these two demonstrations together, to obtain the conjunction.

Our natural deduction system will capture this thought straightforwardly. In the example given, I might adopt the following symbolisation key to represent the argument in \TFL:
	\begin{ekey}
		\item[R] Ludwig is reactionary
		\item[L] Ludwig is libertarian
	\end{ekey}
Perhaps I am working through a proof, and I have obtained `$R$' on line 8 and `$L$' on line 15. Then on any subsequent line I can obtain `$(R \eand L)$' thus:
\begin{proof}
	\have[8]{a}{R}
	\have[ ]{v}{\vdots}
	\have[15]{b}{L}
	\have[16]{c}{(R \eand L)} \ai{a, b}
\end{proof}
Note that every line of our proof must either be an assumption, or must be justified by some rule. We add the commentary `$\eand$I 8, 15' here to indicate that the line is obtained by the rule of conjunction introduction ($\eand$I) applied to lines 8 and 15. 

Since the order of conjuncts does not matter in a conjunction, I could equally well have obtained `$(L \eand R)$' as `$(R \eand L)$'. I can use the same rule with the commentary reversed, to reflect the reversed order of the conjuncts:
\begin{proof}
	\have[8]{a}{R}
	\have[ ]{v}{\vdots}
	\have[15]{b}{L}
	\have[16]{c}{(L \eand R)} \ai{b, a}
\end{proof}
 More generally, here is our \define{conjunction introduction} rule: if we have obtained \meta{A} and \meta{B} by some stage in a proof, whether by proof or assumption, that justifies us in introducing their conjunction. Abstractly, the rule looks like this:
\factoidbox{
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[n]{c}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[\ ]{e}{(\meta{A}\eand\meta{B})} \ai{a, c}
\end{proof}}
To be clear, the statement of the rule is \emph{schematic}. It is not itself a proof. `$\meta{A}$' and `$\meta{B}$' are not sentences of \TFL. Rather, they are symbols in the metalanguage, which we use when we want to talk about any sentence of \TFL\ (see §\ref{s:UseMention}). Similarly, `$m$' and `$n$' are not a numerals that will appear on any actual proof. Rather, they are symbols in the metalanguage, which we use when we want to talk about any line number of any proof. In an actual proof, the lines are numbered `$1$', `$2$', `$3$', and so forth. But when we define the rule, we use variables to emphasise that the rule may be applied at any point. The rule requires only that we have both conjuncts available to us somewhere in the proof, earlier than the line that results from the application of the rule. They can be separated from one another, and they can appear in any order. So $m$ might be less than $n$, or greater than $n$. Indeed, $m$ might even equal $n$, as in this proof:
\begin{proof}
	\hypo[1]{a}{P}
	\have[2]{b}{P\eand P} \ai{a, a}
\end{proof}

Note that the rule involves extending the vertical line to cover the newly introduced sentence. This is because what has been derived depends on the same assumptions as what it was derived from, and so it must also be in the range of those assumptions. \factoidbox{All of the rules in this section justify a new claim which inherits all the assumptions of anything from which it has been derived by a natural deduction rule.} 

\section{Conjunction Elimination}\label{conjelim}

The above rule is called `conjunction \emph{introduction}' because it introduces a sentence with `$\eand$' as its main connective into our proof, prior to which it may have been absent. Correspondingly, we also have a rule that \emph{eliminates} a conjunction. Not that the earlier conjunction is somehow removed! It's just that we use a sentence whose main connective is a conjunction to justify further sentences in which that conjunction does not feature.

Suppose you have shown that Ludwig is both reactionary and libertarian. You are entitled to conclude that Ludwig is reactionary. Equally, you are entitled to conclude that Ludwig is libertarian. Putting these observations together, we obtain our \define{conjunction elimination} rules:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eand\meta{B})}
	\have[\ ]{}{\vdots}
	\have[\ ]{a}{\meta{A}} \ae{ab}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eand\meta{B})}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{B}} \ae{ab}
\end{proof}
\end{minipage}
}
The point is simply that, when you have a conjunction on some line of a proof, you can obtain either of the conjuncts by {\eand}E later on. There are two rules, because each conjunction justifies us in deriving either of its conjuncts. We could have called them {\eand}E-\textsc{left} and {\eand}E-\textsc{right}, to distinguish them, but in the following we will mostly not distinguish them.\footnote{Why do we have two rules at all, rather than one rule that allows us to derive either conjunct? The answer is that we want our rules to have an unambiguous result when applied to some prior lines of the proof. This is important if, for example, we are implementing a computer system to produce formal proofs.}


 One point might be worth emphasising: you can only apply this rule when conjunction is the main connective. So you cannot derive `$D$' just from `$C \eor (D \eand E)$'! Nor can you derive `$D$' directly from `$C \eand (D \eand E)$', because it is not one of the conjuncts of the main connective of this sentence. You would have to first obtain `$(D \eand E)$' by {\eand}E, and then obtain `$D$' by a second application of that rule, as in this proof: \begin{proof}
 	\hypo{a}{C \eand (D \eand E)} 
 	\have{b}{D \eand E}\ae{a}
 	\have{c}{D}\ae{b}
 \end{proof}

Even with just these two rules, we can start to see some of the power of our formal proof system. Consider this tricky-looking argument:
\begin{earg}
\item[] $\bigl( (A\eor B)\eif(C\eor D) \bigr) \eand \bigl( (E \eor F) \eif (G\eor H) \bigr)$
\item[\ttherefore] $\bigl( (E \eor F) \eif (G\eor H) \bigr) \eand \bigl( (A\eor B)\eif(C\eor D) \bigr)$
\end{earg} Dealing with this argument using truth-tables would be a very tedious exercise, given that there are 8 sentence letters in the premise and we would thus require a $2^{8}=256$ line truth table! But we can deal with it swiftly using our natural deduction rules.

The main connective in both the premise and conclusion of this argument is `$\eand$'. In order to provide a proof, we begin by writing down the premise, which is our assumption. We draw a line below this: everything after this line must follow from our assumptions by (successive applications of) our rules of implication. So the beginning of the proof looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}}
\end{proof}
From the premise, we can get each of its conjuncts by {\eand}E. The proof now looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}}
	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
\end{proof}
So by applying the {\eand}I rule to lines 3 and 2 (in that order), we arrive at the desired conclusion. The finished proof looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}}

	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
	\have{ba}{{[}(E \eor F) \eif (G\eor H){]} \eand {[}(A\eor B)\eif(C\eor D){]}} \ai{b,a}
\end{proof}
This is a very simple proof, but it shows how we can chain rules of proof together into longer proofs. Our formal proof requires just four lines, a far cry from the 256 lines that would have been required had we approached the argument using the techniques from chapter \ref{ch.TruthTables}.

It is worth giving another example. Way back in §\ref{s:MoreBracketingConventions}, we noted that this argument is valid:
	$$A \eand (B \eand C) \ttherefore (A \eand B) \eand C.$$
To provide a proof corresponding to this argument, we start by writing:
\begin{proof}
	\hypo{ab}{A \eand (B \eand C)}
\end{proof}
From the premise, we can get each of the conjuncts by applying $\eand$E twice. And we can then apply $\eand$E twice more, so our proof looks like:
\begin{proof}
	\hypo{ab}{A \eand (B \eand C)}
	\have{a}{A} \ae{ab}
	\have{bc}{B \eand C} \ae{ab}
	\have{b}{B} \ae{bc}
	\have{c}{C} \ae{bc}
\end{proof}
But now we can merrily reintroduce conjunctions in the order we want them, so that our final proof is:
\begin{proof}
	\hypo{abc}{A \eand (B \eand C)}
	\have{a}{A} \ae{abc}
	\have{bc}{B \eand C} \ae{abc}
	\have{b}{B} \ae{bc}
	\have{c}{C} \ae{bc}
	\have{ab}{A \eand B}\ai{a, b}
	\have{con}{(A \eand B) \eand C}\ai{ab, c}
\end{proof}
Recall that our official definition of sentences in \TFL\ only allowed conjunctions with two conjuncts. When we discussed semantics, we became a bit more relaxed, and allowed ourselves to drop inner brackets in long conjunctions, since the order of the brackets did not affect the truth table. The proof just given suggests that we could also drop inner brackets in all of our proofs. However, this is not standard, and we shall not do this. Instead, we shall return to the more austere bracketing conventions. (Though we will allow ourselves to drop outermost brackets most of the time, for legibility.)

Our conjunction rules correspond to intuitively correct patterns of implication. But they are also demonstrably good in another sense. Each of our rules can be vindicated by considering facts about entailment. Each of these schematic entailments is easily demonstrated: \begin{itemize}
	\item $\meta{A}, \meta{B} \entails \meta{A} \eand \meta{B}$;
	\item 	$\meta{A} \eand \meta{B} \entails \meta{A}$;
	\item $\meta{A} \eand \meta{B} \entails \meta{B}$.
\end{itemize} For example, the first of these says that \meta{A} and \meta{B} separately suffice to entail the truth of their conjunction. This justifies the proof rule of conjunction introduction, since at a stage in the proof where we are assuming both \meta{A} and \meta{B} to be true, we are then permitted to conclude that $\meta{A}\eand\meta{B}$ is true – just as conjunction introduction says we can.

It can be recognised, then, that our proof rules correspond to valid arguments in \TFL, and so our conjunction rules will never permit us to derive a false sentence from true sentences. There is no guarantee of course that the assumptions we make in our formal proofs are in fact true – only that if they were true, what we derive from them would also be true. So despite the fact that our proof rules are a \emph{syntactic} procedure, that rely only on recognising the main connective of a sentence and applying an appropriate rule to introduce or eliminate it, each of our rules corresponds to an acceptable entailment.


\section{Conditional Elimination}\label{condelim}
Consider the following argument:
	\begin{quote}
		If Jane is smart then she is fast. Jane is smart. So Jane is fast.
	\end{quote}
This argument is certainly valid. If you have a conditional claim, that commits you to the consequent given the antecedent, and you also have the antecedent, then you have sufficient material to derive the consequent. 

This suggests a straightforward \define{conditional elimination} rule ({\eif}E):
\factoidbox{
\begin{proof}
	\have[m]{ab}{(\meta{A}\eif\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{B}} \ce{ab,a}
\end{proof}}
This rule is also sometimes called \emph{modus ponens}. Again, this is an elimination rule, because it allows us to obtain a sentence that may not contain `$\eif$', having started with a sentence that did contain `$\eif$'. Note that the conditional, and the antecedent, can be separated from one another, and they can appear in any order. However, in the commentary for $\eif$E, we always cite the conditional first, followed by the antecedent.

Here is an illustration of the rules we have so far in action, applied to this intuitively correct argument: $$P, \bigl((P \eif Q)\eand(P \eif R)\bigr) \ttherefore (R \eand Q).$$ \begin{proof}
	\hypo{a}{P}
	\open
	\hypo{b}{\bigl((P \eif Q)\eand(P \eif R)\bigr)}
	\have{c}{(P \eif Q)}\ae{b}
	\have{d}{(P \eif R)}\ae{b}
	\have{e}{Q}\ce{c,a}
	\have{f}{R}\ce{d,a}
	\have{g}{(R \eand Q)}\ai{f,e}
\end{proof}

The correctness of our proof rule of conditional elimination is supported by the easily demonstrated validity of the corresponding entailment: \begin{itemize}
	\item  $\meta{A}, \meta{A}\eif\meta{B}\entails \meta{B}$.
\end{itemize} So applying this rule can never produce false conclusions if we began with true assumptions.

\section{Biconditional Elimination}\label{bielim}

The \define{biconditional elimination} rule ({\eiff}E) lets you do a much the same as the conditional rule. Unofficially, a biconditional is like two conditionals running in each direction. So, thought of informally, our biconditional rules correspond to the left-to-right conditional elimination rule, the other of which corresponds to a right-to-left application of conditional elimination. 

If we know that Alice is coming to the party iff Bob is, then if we knew that either of them was coming, we'd know that the other was coming. If you have the left-hand subsentence of the biconditional, you can obtain the right-hand subsentence. If you have the right-hand subsentence, you can obtain the left-hand subsentence. So we have these two instances of the rule:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eiff\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{B}} \be{ab,a}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eiff\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{a}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{A}} \be{ab,a}
\end{proof}
\end{minipage}
}
Note that the biconditional, and the right or left half, can be distant from one another in the proof, and they can appear in any order. However, in the commentary for $\eiff$E, we always cite the biconditional first. 

Here is an example of the biconditional rules in action, demonstrating the following argument: $$P, (P\eiff Q), (Q \eif R) \ttherefore R.$$ \begin{proof}
	\hypo{pq}{(P\eiff Q)}
	\open \hypo{p}{P}
	\open \hypo{qr}{(Q \eif R)}
	\have{q}{Q}\be{pq,p}
	\have{r}{R}\ce{qr,q}
\end{proof}

Note the way that our conjunction and conditional elimination rules can be used to parallel the biconditional elimination rules:

\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}} \begin{proof}
	\hypo{a}{((P \eif Q)\eand (Q \eif P))}
	\hypo{p}{Q}
	\have{pq}{(Q \eif P)}\ae{a}
	\have{q}{P}\ce{pq,p}
\end{proof} & \begin{proof}
	\hypo{a}{(P \eiff Q)}
	\hypo{p}{Q}
	\have{q}{P}\be{a,p}
\end{proof}\end{tabular}



The correctness of our proof rules of biconditional elimination is supported by the easily demonstrated validity of the corresponding entailments:
\begin{itemize}
	\item $\meta{A}\eiff\meta{B},\meta{A}\vDash\meta{B}$;
	\item  $\meta{A}\eiff\meta{B},\meta{B}\vDash\meta{A}$.
\end{itemize}


\section{Disjunction Introduction}\label{disjint}

Suppose Ludwig is reactionary. Then Ludwig is either reactionary or libertarian. After all, to say that Ludwig is either reactionary or libertarian is to say something weaker than to say that Ludwig is reactionary. (\meta{A} is weaker than \meta{B} if \meta{A} follows from \meta{B}, but not \emph{vice versa}.) 

Let me emphasise this point. Suppose Ludwig is reactionary. It follows that Ludwig is \emph{either} reactionary \emph{or} a kumquat. Equally, it follows that \emph{either} Ludwig is reactionary \emph{or} that kumquats are the only fruit.  Equally, it follows that \emph{either} Ludwig is reactionary \emph{or} that God is dead. Many of these things would be strange \emph{inferences} to draw. Since the truth of the assumption does guarantee that the disjunction is true, there is nothing \emph{logically} wrong with the implications. This can be so even if drawing these implications may violate all sorts of implicit conversational norms, or that inferring in accordance with logic in this manner would be more likely a sign of psychosis than rationality.

Armed with all this, I present the \define{disjunction introduction} rule(s):
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{ab}{\meta{A}\eor\meta{B}}\oi{a}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{ba}{\meta{B}\eor\meta{A}}\oi{a}
\end{proof}
\end{minipage}}
Notice that \meta{B} can be \emph{any} sentence of \TFL\ whatsoever. So the following is a perfectly good proof:
\begin{proof}
	\hypo{m}{M}
	\have{mmm}{M \eor \Bigl( \bigl( (A\eiff B) \eif (C \eand D)\bigr)  \eiff \bigl( E \eand F\bigr) \Bigr)}\oi{m}
\end{proof}
Using a truth table to show this would have taken 128 lines. 


\section{Reiteration}\label{reit}

The last natural deduction rule in this category is \define{reiteration} (R). This just allows us to repeat an assumption or claim \meta{A} we have already established, so long as the current stage of the proof remains in the range of any assumption which \meta{A} is in the range of.
\factoidbox{\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{c}{\vdots}
	\have[\ ]{b}{\meta{A}} \by{R}{a}
\end{proof}}
Such a rule is obviously legitimate; but one might well wonder how such a rule could ever be useful. Here is an example of it in action: \begin{proof}
	\hypo{p}{P}
	\hypo{ppq}{((P \eand P) \eif Q)}
	\have{p1}{P}\by{R}{p}
	\have{pp}{(P \eand P)}\ai{p,p1}
	\have{q}{Q}\ce{ppq,pp}
\end{proof}
This rule is unnecessary at this point in the proof (we could have applied conjunction introduction and cited line 1 twice in our commentary), but it can be easier in practice to have two distinct lines to which to apply conjunction introduction. The real benefits of reiteration come when we have multiple subproofs, as we will see in the following section (§\ref{s.subproof}) – particularly when it comes to the negation rules. But we will also see later in §\ref{der.reit} that, strictly speaking, we don't need the reiteration rule – though for convenience we will keep it.




\chapter{Basic rules for \textnormal{\TFL}: Rules With Subproofs}\label{s:BasicTFLs}

We've already seen in §\ref{c:ass} how to start a proof by making assumptions. But the true power of natural deduction relies on its rules governing when you can make additional assumptions during the course of the proof, and how you can \emph{discharge} those assumptions when you no longer need them. 


\section{Additional assumptions and subproofs}\label{s.subproof}

In natural deduction, both making and discharging additional assumptions are handled using \define{subproofs}. These are subsidiary proofs within the main proof, which encapsulate that part of a larger proof that depends on an assumption that is not among the premises. (Conversely, we can think of the premises of an argument as those assumptions left undischarged at the conclusion of a proof.)

When we start a subproof, we draw another vertical line (to the right of any existing assumption lines) to indicate that we are no longer in the main proof. Then we write in the assumption upon which the subproof will be based. A subproof can be thought of as essentially posing this question: \emph{what could we show, if we also make this additional assumption?} We've already seen this in action earlier (§\ref{c:ass}), when we said that we could indicate the range of several premises in an argument by either attaching them all to one vertical assumption line, or introducing a new vertical line for each new assumption. In that case, we never got rid of the new assumptions: they remained as premises. 

What will be new in this section is that some rules take us \emph{back out} of a subproof. So the rules we will now consider are quite different from the rules covered in §\ref{s:BasicTFLns}, none of which have this feature of being able to escape from a previously introduced assumption.

When we are working within a subproof, we can refer to the additional assumption that we made in introducing the subproof, and to anything that we obtained from our original assumptions. (After all, those original assumptions are still in effect.) But at some point, we shall want to stop working with the additional assumption: we shall want to return from the subproof to the main proof. To indicate that we have returned to the main proof, the vertical line for the subproof comes to an end. At this point, we say that the subproof is \define{closed}. Having closed a subproof, we have set aside the additional assumption, so it will be illegitimate to draw upon anything that depends upon that additional assumption. This has been implicit in our discussion all along, but it is good to make it very clear:
\factoidbox{Any point in a natural deduction proof is in the range of some currently active assumptions, and the natural deduction rules can be applied only to sentences which do not rely on any other assumptions than those.

Equivalently, any rule can be applied to any earlier lines in a proof, \emph{except} for those lines which occur within a closed subproof. }
Closing a subproof is called \define{discharging} the assumptions of that subproof. So we can put the point this way: \emph{at no stage of a proof can you apply a rule to a sentence that occurs only in the range of an already discharged assumption}. 

Subproofs, then, allow us to think about what we could show, if we made additional assumptions. The point to take away from this is not surprising – in the course of a proof, we have to keep very careful track of what assumptions we are making, at any given moment. Our proof system does this very graphically, with those vertical assumption lines that indicate the range of an assumption. (Indeed, that's precisely why we have chosen to use \emph{this} proof system.)

When can we begin a new subproof? \emph{Whenever we want.} At any stage in a proof it is legitimate to assume something new, as long as we begin keeping track of what in our proof rests on this new assumption. No rule is needed to justify making an assumption, though there are guidelines that help us decide when it might be particularly appropriate to make a new assumption. Some of these are discussed in §\ref{c:proof.strat}.

Even without any new rules, the idea of opening subproofs by making new assumptions can be used to illustrate a remark I made above about when a claim depends on assumptions (p.\ \pageref{nondependence}). Consider this proof: \begin{proof}
	\hypo{pq}{(P \eand Q)}
	\have{q}{Q}\ae{pq}
	\open \hypo{r}{R}
	\have{q2}{Q}\by{R}{q}
\end{proof} In this proof, even though the occurrence of `$Q$' on line 4 occurs within the range of the assumption `$R$', it does not intuitively depend on it. We used reiteration to show that `$R$' is still derivable from active assumptions at line 4, but it does not follow that `$R$' depends in any robust way on every assumption that is active at line 4. 


\section{Conditional Introduction}\label{condint}

To illustrate the use of subproofs, we will begin with the rule of conditional introduction. It is fairly easy to motivate informally.\label{ci.motivate} The following argument in English should be valid:
	\begin{quote}
		Ludwig is reactionary. Therefore if Ludwig is libertarian, then Ludwig is both reactionary \emph{and} libertarian.
	\end{quote}
If someone doubted that this was valid, we might try to convince them otherwise by explaining ourselves as follows:
	\begin{quote}
		Assume that Ludwig is reactionary. Now, \emph{additionally} assume that Ludwig is libertarian. Then by conjunction introduction, it follows that Ludwig is both reactionary and libertarian. Of course, that only follows \emph{conditional} on the assumption that Ludwig is libertarian. But this just means that, if Ludwig is libertarian, then he is both reactionary and libertarian – at least, that follows given our initial assumption that he is reactionary.
	\end{quote}
Transferred into natural deduction format, here is the pattern of reasoning that we just used. We started with one premise, `Ludwig is reactionary', symbolised `$R$'. Thus:
	\begin{proof}
		\hypo{r}{R}
	\end{proof}
The next thing we did is to make an \emph{additional} assumption (`Ludwig is libertarian'), for the sake of argument. To indicate that we are no longer dealing \emph{merely} with our original assumption (`$R$'), but with some additional assumption, we continue our proof as follows:
	\begin{proof}
		\hypo{r}{R}
		\open
			\hypo{l}{L}
	\end{proof}
We are \emph{not} claiming, on line 2, to have proved `$L$' from line 1. We are just making another assumption. So we do not need to write in any justification for the additional assumption on line 2. We do, however, need to mark that it is an additional assumption. We do this in the usual way, by drawing a line under it (to indicate that it is an assumption) and by indenting it with a further assumption line (to indicate that it is additional). 

With this extra assumption in place, we are in a position to use {\eand}I. So we could continue our proof:
	\begin{proof}
		\hypo{r}{R}
		\open
			\hypo{l}{L}
			\have{rl}{R \eand L}\ai{r, l}
%			\close
%		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{proof} The two vertical lines to the left of line 3 show that `$R\eand L$' is in the range of both assumptions, and indeed depends on them collectively.

So we have now shown that, on the additional assumption, `$L$', we can obtain `$R \eand L$'. We can therefore conclude that, if `$L$' obtains, then so does `$R \eand L$'. Or, to put it more briefly, we can conclude `$L \eif (R \eand L)$':
	\begin{proof}
		\hypo{r}{R}
		\open
			\hypo{l}{L}
			\have{rl}{R \eand L}\ai{r, l}
			\close
		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{proof}
Observe that we have dropped back to using one vertical line.  We are no longer relying on the additional assumption, `$L$', since the conditional itself follows just from our original assumption, `$R$'. The use of conditional introduction has discharged the temporary assumption, so that the final line of this proof relies only on the initial assumption `$R$' – we made use of the assumption `$L$' only in the nested subproof, and the range of that assumption is restricted to sentences in that subproof. 

The general pattern at work here is the following. We first make an additional assumption, A; and from that additional assumption, we prove B. In that case, we have established the following: If is does in fact turn out that A, then it also turns out that B. This is wrapped up in the rule for \define{conditional introduction}:
\factoidbox{
	\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}} 
			\have[\ ]{} \vdots
			\have[j]{b}{\meta{B}}
		\close
		\have[\ ]{}{\vdots}
		\have[\ ]{ab}{(\meta{A}\eif\meta{B})}\ci{a-b}
	\end{proof}}
There can be as many or as few lines as you like between lines $i$ and $j$. Notice that in our presentation of the rule, discharging the assumption \meta{A} takes us out of the subproof in which \meta{B} is derived from \meta{A}. If \meta{A} is the initial assumption of a proof, then discharging it may well leave us with a conditional claim that depends on \emph{no undischarged assumptions at all}. We see an example in this proof, where the main proof, marked by the leftmost vertical line, features no horizontal line marking an assumption:
\begin{proof}
	\open \hypo{a}{P \eand P}
	\have{b}{P}\ae{a}
	\close
	\have{}{(P\eand P) \eif P}\ci{a-b}
\end{proof} It might come as no surprise that the conclusion of this proof – being provable from no undischarged assumptions at all - turns out to be a tautology. 

It will help to offer a further  illustration of {\eif}I in action. Suppose we want to consider the following:
	$$P \eif Q, Q \eif R \ttherefore P \eif R.$$
We start by listing \emph{both} of our premises. Then, since we want to arrive at a conditional (namely, `$P \eif R$'), we additionally assume the antecedent to that conditional. Thus our main proof starts:
\begin{proof}
	\hypo{pq}{P \eif Q}
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
	\close
\end{proof}
Note that we have made `$P$' available, by treating it as an additional assumption. But now, we can use {\eif}E on the first premise. This will yield `$Q$'. And we can then use {\eif}E on the second premise. So, by assuming `$P$' we were able to prove `$R$', so we apply the {\eif}I rule – discharging `$P$' – and finish the proof. Putting all this together, we have:
\label{HSproof}
\begin{proof}
	\hypo{pq}{P \eif Q}
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof}


We now have all the rules we need to show that the argument on page \pageref{longtt} is valid. Here is the six line proof,  some 175,000 times shorter  than the corresponding truth table:\label{ndshort}

\hspace{-2.7cm}
\begin{minipage}{\textwidth}\small\begin{proof}
	\hypo{a1c1}{A_{1} \eif C_{1}}
	\open
		\hypo{ai}{(A_1 \eand A_2 \eand A_3 \eand A_4 \eand A_5 \eand A_6 \eand A_7 \eand A_8 \eand A_9 \eand A_{10})}
		\have{a1}{A_{1}}\ae{ai}
		\have{c1}{C_{1}}\ce{a1c1,a1}
		\have{ci}{(C_1 \eor C_2 \eor C_3 \eor C_4 \eor C_5 \eor C_6 \eor C_7 \eor C_8 \eor C_9 \eor C_{10})}\oi{c1}
		\close
	\have{c}{(A_1 \eand A_2 \eand A_3 \eand A_4 \eand A_5 \eand A_6 \eand A_7 \eand A_8 \eand A_9 \eand A_{10})\eif (C_1 \eor C_2 \eor C_3 \eor C_4 \eor C_5 \eor C_6 \eor C_7 \eor C_8 \eor C_9 \eor C_{10})}\ci{ai-ci}
\end{proof}\end{minipage}

\subsection{Import-Export}
Our rules so far can be used to demonstrate two important and rather controversial principles governing the conditional. The principle of \define{importation} is the claim that from `if $P$ then, if also $Q$, then $R$' it follows that `if $P$ and also $Q$, then $R$'. The principle of \define{exportation} is the converse, that from `if $P$ and also $Q$, then $R$' it follows that `if $P$, then if also $Q$, then $R$'. First, we prove importation holds for our conditional: \begin{proof}
	\hypo{ipiqr}{(P \eif (Q \eif R))}
	\open
	\hypo{paq}{(P \eand Q)}
	\have{p}{P}\ae{paq}
	\have{qr}{(Q \eif R)}\ce{ipiqr,p}
	\have{q}{Q}\ae{paq}
	\have{r}{R}\ce{qr,q}
	\close
	\have{c}{((P\eand Q)\eif R)}\ci{paq-r}
\end{proof}
Second, we show exportation holds. Here, we need to open two nested subproofs:
\begin{proof}
	\hypo{paqr}{((P\eand Q)\eif R)}
	\open
	\hypo{p}{P}
	\open
	\hypo{q}{Q}
	\have{pq}{(P \eand Q)}\ai{p,q}
	\have{r}{R}\ce{paqr,pq}
	\close
	\have{qr}{(Q \eif R)}\ci{q-r}
	\close
	\have{pqr}{(P \eif (Q \eif R))}\ci{p-qr}
\end{proof}


\section{Pitfalls of additional assumptions}


The rule $\eif$I invoked the idea of making additional assumptions in the range of an assumption. These need to be handled with some care.

Consider this proof:
\begin{proof}
	\hypo{a}{A}
	\open
		\hypo{b1}{B}
		\have{bb}{B \eand B}\ai{b1, b1}
		\have{b2}{B} \ae{bb}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
\end{proof}
This is perfectly in keeping with the rules we have laid down already. And it should not seem particularly strange. Since `$B \eif B$' is a tautology, no particular premises should be required to prove it. 

But suppose we now tried to continue the proof as follows:
\begin{proof}
	\hypo{a}{A}
	\open
		\hypo{b1}{B}
		\have{bb}{B \eand B}\ai{b1, b1}
		\have{b2}{B} \ae{bb}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
	\have{b}{B}\by{naughty attempt to invoke $\eif$E}{con, b2}
\end{proof}
If we were allowed to do this, it would be a disaster. It would allow us to prove any atomic sentence letter from any other atomic sentence letter. But if you tell me that Anne is fast (symbolised by `$A$'), I shouldn't be able to conclude that Queen Boudica stood twenty-feet tall (symbolised by `$B$')! So we must be prohibited from doing this. But how are we to implement the prohibition?

This stipulation rules out the disastrous attempted proof above. The rule of $\eif$E requires that we cite two individual lines from earlier in the proof. In the purported proof, above, one of these lines (namely, line 4) occurs within a subproof that has (by line 6) been closed. This is illegitimate. 

Once we have started thinking about what we can show by making additional assumptions, nothing stops us from posing the question of what we could show if we were to make \emph{even more} assumptions? This might motivate us to introduce a subproof within a subproof. Here is an example which only uses the rules of proof that we have considered so far:
\begin{proof}
\hypo{a}{A}
\open
	\hypo{b}{B}
	\open
		\hypo{c}{C}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif (C \eif (A \eand B))}\ci{b-cab}
\end{proof}
Notice that the commentary on line 4 refers back to the initial assumption (on line 1) and an assumption of a subproof (on line 2). This is perfectly in order, since neither assumption has been discharged at the time (i.e., by line 4).

Again, though, we need to keep careful track of what we are assuming at any given moment. For suppose we tried to continue the proof as follows:
\begin{proof}
\hypo{a}{A}
\open
	\hypo{b}{B}
	\open
		\hypo{c}{C}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif(C \eif (A \eand B))}\ci{b-cab}
\have{bcab}{C \eif (A \eand B)}\by{naughty attempt to invoke $\eif$I}{c-ab}
\end{proof}
This would be awful. If I tell you that Anne is smart, you should not be able to derive that, if Cath is smart (symbolised by `$C$') then \emph{both} Anne is smart and Queen Boudica stood 20-feet tall! But this is just what such a proof would suggest, if it were permissible.

The essential problem is that the subproof that began with the assumption `$C$' depended crucially on the fact that we had assumed `$B$' on line 2. By line 6, we have \emph{discharged} the assumption `$B$': we have stopped asking ourselves what we could show, if we also assumed `$B$'. So it is simply cheating, to try to help ourselves (on line 7) to the subproof that began with the assumption `$C$'. Thus we stipulate, much as before:
\factoidbox{Any rule whose commentary requires mentioning an entire subproof can mention any earlier subproof, \emph{except} for those subproofs which occur within some \emph{other} closed subproof.}
The attempted disastrous proof violates this stipulation. The subproof of lines 3–4 occurs within a subproof that ends on line 5. So it cannot be invoked in line 7.

It is always permissible to open a subproof with any assumption. However, there is some strategy involved in picking a useful assumption. Starting a subproof with an arbitrary, wacky assumption would just waste lines of the proof. In order to obtain a conditional by {\eif}I, for instance, you must assume the antecedent of the conditional in a subproof. 

Equally, it is always permissible to close a subproof and discharge its assumptions. However, it will not be helpful to do so, until you have reached something useful.

Recall the proof of the argument $$P \eif Q, Q \eif R \ttherefore P \eif R$$ from page \pageref{HSproof}. One thing to note about the proof there is that because there are two assumptions with the same range in the main proof, it is not easily possible to discharge just one of them using the {\eif}I rule. For that rule only applies to a one-assumption subproof. If we wanted to discharge another of our assumptions, we shall have to put the proof into the right form, with each assumption made individually as the head of its own subproof:
\begin{proof}
	\hypo{pq}{P \eif Q}
	\open
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof} The conclusion is now in the range of both assumptions, as in the earlier proof – but now it is also possible to discharge these assumptions if we wish: \begin{proof}
	\open \hypo{pq}{P \eif Q}
	\open
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
	\close
	\have{qrpr}{(Q\eif R)\eif(P\eif R)}\ci{qr-pr}
	\close
	\have{}{(P\eif Q)\eif((Q\eif R)\eif(P\eif R))}\ci{pq-qrpr}
\end{proof} 
While it is permissible, and often convenient, to have several assumptions with the same range and without nesting, I recommend always trying to construct your proofs so that each assumption begins its own subproof. That way, if you later wish to apply rules which discharge a single assumption, you may always do so.

\section{Conditional introduction and the English conditional} \label{cond.proof}%Æ added

We motivated the conditional introduction rule back on page \pageref{ci.motivate} by giving a English argument, using the English word `if'. Now, {\eif}I is a stipulated rule for our conditional connective \eif; it doesn't really need motivation since we could simply postulate that such a rule is part of our formal proof system. It is justified, if justification is needed, by the Deduction Theorem (a result noted in §\ref{ded.thm}). (Likewise, $\eif$E may be justified by a schematic truth table demonstration that $\meta{A}, \meta{A}\eif\meta{C} \vDash \meta{C}$.)

But if we are to offer a motivation for our rule in English, then we must be relying on the plausibility of this English analog of {\eif}I, known as \define{conditional proof}: \factoidbox{
	If you can establish $\meta{C}$, given the assumption that $\meta{A}$ and perhaps some supplementary assumptions $\meta{B}_{1},…,\meta{B}_{n}$, then you can establish `if $\meta{A}$, $\meta{C}$' solely on the basis of the assumptions $\meta{B}_{1},…,\meta{B}_{n}$.
} Conditional proof captures a significant aspect of the English `if': the way that  conditional helps us neatly summarise reasoning from assumptions, and then store that reasoning for later use in a conditional form.



But if conditional proof is a good rule for English `if', then we can argue that `if' is actually synonymous with `$\to$':\ \begin{quote}
Suppose that $\meta{A} \eif \meta{C}$. Assume $\meta{A}$. We can now derive $\meta{C}$, by {\eif}E. But we have now established $\meta{C}$ on the basis of the assumption $\meta{A}$, together with the supplementary assumption $\meta{A}\eif \meta{C}$. By conditional proof, then, we can establish `if $\meta{A}$, $\meta{C}$' on the basis of the supplementary assumption $\meta{A}\eif \meta{C}$ alone. But that of course means that we can derive (in English) an English conditional sentence from a \TFL\ conditional sentence, with the appropriate interpretation of the constituents \meta{A} and \meta{C}. Since the English conditional sentence obviously suffices for the \TFL\ conditional sentence, we have shown them to be synonymous. 
\end{quote}  

Our discussion in §\ref{s:ParadoxesOfMaterialConditional} seemed to indicate that the English conditional was not synonymous with `\eif'. That is hard to reconcile with the above argument. Most philosophers have concluded that, contrary to appearances, conditional proof is not always a good way of reasoning for English `if'. Here is an example which shows this, though it requires some background.

It is clearly valid in English, though rather pointless, to argue from a claim to itself. So when \meta{C} is some English sentence, $\meta{C} \ttherefore \meta{C}$ is a valid argument in English. And we cannot make a valid argument invalid by adding more premises:  if premises we already have are conclusive grounds for the conclusion, adding more premises while keeping the conclusive grounds cannot make the argument less conclusive. So $\meta{C},\meta{A} \ttherefore \meta{C}$ is a valid argument in English. 

If conditional proof were a good way of arguing in English, we could convert this valid argument into another valid argument with this form: $\meta{C} \ttherefore \text{if }\meta{A}, \meta{C}$. But then conditional proof would then allow us to convert this valid argument: \begin{earg}
	\item[\ex{ski1}] I will go skiing tomorrow;
	\item[\ex{ski2}] I break my leg tonight;
	\item[So:] I will go skiing tomorrow. \qquad(which is just repeating \ref{ski1} again)
\end{earg} into this intuitively invalid argument:
\begin{earg}
	\item[\ref{ski1}.] I will go skiing tomorrow;
	\item[So:] If I break my leg tonight, I will go skiing tomorrow.
\end{earg} This is invalid, because even if the premise is true, the conclusion seems to be actually false. If conditional proof enables us to convert a valid English argument into an invalid argument, so much the worse for conditional proof. 

There is much more to be said about this example. What is beyond doubt is that {\eif}I is a good rule for \TFL, regardless of the fortunes of conditional proof in English. It does seem that instances of conditional proof in mathematical reasoning are all acceptable, which again shows the roots of natural deduction as a formalisation of existing mathematical practice. This would suggest that \eif\ might be a good representation of mathematical uses of `if'.



\section{Biconditional Introduction} \label{biint} 
The rules for the biconditional will be like double-barrelled versions of the rules for the conditional.

In order to prove `$W \eiff X$', for instance, you must be able to prove `$X$' on the assumption `$W$' \emph{and} prove `$W$' on the assumption `$X$'. The \define{biconditional introduction} rule ({\eiff}I) therefore requires two subproofs. Schematically, the rule works like this:
\factoidbox{
\begin{proof}
	\open
		\hypo[i]{a1}{\meta{A}}
		\have[\ ]{}{\vdots}
		\have[j]{b1}{\meta{B}}
	\close
	\open
		\hypo[k]{b2}{\meta{B}}
		\have[\ ]{}{\vdots}
		\have[l]{a2}{\meta{A}}
	\close
	\have[\ ]{}{\vdots}
	\have[\ ]{ab}{\meta{A}\eiff\meta{B}}\bi{a1-b1,b2-a2}
\end{proof}}
There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first. Again, this rule permits us to discharge assumptions, and the same restrictions on making use of claims derived in a closed subproof outside of that subproof apply.


The acceptability of our proof rules is grounded in the fact that they will never lead us from truth to falsehood. The acceptability of the biconditional introduction rule is demonstrated by the following fact, which may be shown by consideration of schematic truth tables: \begin{itemize}
	\item If $\meta{C}_{1},…,\meta{C}_{n},\meta{A}\vDash\meta{B}$ and $\meta{C}_{1},…,\meta{C}_{n},\meta{B}\vDash\meta{A}$, then $\meta{C}_{1},…,\meta{C}_{n}\vDash\meta{A}\eiff\meta{B}$.
\end{itemize}



\section{Disjunction Elimination}\label{disjelim}




The disjunction elimination rule is, though, slightly trickier. Suppose that either Ludwig is reactionary or he is libertarian. What can you conclude? Not that Ludwig is reactionary; it might be that he is libertarian instead. And equally, not that Ludwig is libertarian; for he might merely be reactionary. Disjunctions, just by themselves, are hard to work with. 

But suppose that we could somehow show both of the following: first, that Ludwig's being reactionary entails that he is an Austrian economist: second, that Ludwig's being libertarian entails that he is an Austrian economist. Then if we know that Ludwig is either reactionary or libertarian, then we know that, whichever he is, Ludwig is an Austrian economist.\footnote{This we might call `no matter whether' reasoning: if each of \meta{A} and \meta{B} imply \meta{C}, then \emph{no matter whether} \meta{A} \emph{or} \meta{B}, still \meta{C}.} This insight can be expressed in the following rule, which is our \define{disjunction elimination} ($\eor$E) rule:
\factoidbox{
	\begin{proof}
		\have[m]{ab}{\meta{A}\eor\meta{B}}
		\open
			\hypo[i]{a}{\meta{A}} {}
			\have[\ ]{}{\vdots}
			\have[j]{c1}{\meta{C}}
		\close
		\open
			\hypo[k]{b}{\meta{B}}{}
			\have[\ ]{}{\vdots}
			\have[l]{c2}{\meta{C}}
		\close
		\have[\ ]{}{\vdots}
		\have[ ]{c}{\meta{C}}\oe{ab, a-c1,b-c2}
	\end{proof}}
This is obviously a bit clunkier to write down than our previous rules, but the point is fairly simple. Suppose we have some disjunction, $\meta{A} \eor \meta{B}$. Suppose we have two subproofs, showing us that $\meta{C}$ follows from the assumption that $\meta{A}$, and that $\meta{C}$ follows from the assumption that $\meta{B}$. Then we can derive $\meta{C}$ itself. As usual, there can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs and the disjunction can come in any order, and do not have to be adjacent.

Some examples might help illustrate this. Consider this argument:
$$(P \eand Q) \eor (P \eand R) \ttherefore P.$$
An example proof might run thus:
	\begin{proof}
		\hypo{prem}{(P \eand Q) \eor (P \eand R) }
			\open
				\hypo{pq}{P \eand Q}
				\have{p1}{P}\ae{pq}
			\close
			\open
				\hypo{pr}{P \eand R}
				\have{p2}{P}\ae{pr}
			\close
		\have{con}{P}\oe{prem, pq-p1, pr-p2}
	\end{proof}
Here is a slightly harder example. Consider:
	$$ A \eand (B \eor C) \ttherefore (A \eand B) \eor (A \eand C).$$
Here is a proof corresponding to this argument:
	\begin{proof}
		\hypo{aboc}{A \eand (B \eor C)}
		\have{a}{A}\ae{aboc}
		\have{boc}{B \eor C}\ae{aboc}
		\open
			\hypo{b}{B}
			\have{ab}{A \eand B}\ai{a,b}
			\have{abo}{(A \eand B) \eor (A \eand C)}\oi{ab}
		\close
		\open
			\hypo{c}{C}
			\have{ac}{A \eand C}\ai{a,c}
			\have{aco}{(A \eand B) \eor (A \eand C)}\oi{ac}
		\close
	\have{con}{(A \eand B) \eor (A \eand C)}\oe{boc, b-abo, c-aco}
	\end{proof}
Don't be alarmed if you think that you wouldn't have been able to come up with this proof yourself. The ability to come up with novel proofs will come with practice. The key question at this stage is whether, looking at the proof, you can see that it conforms with the rules that we have laid down. And that just involves checking every line, and making sure that it is justified in accordance with the rules we have laid down.

Again, our disjunction rules are supported by some valid \TFL\ argument forms: \begin{itemize}
	\item $\meta{A}\vDash\meta{A}\eor\meta{B}$ and $\meta{B}\vDash\meta{A}\eor\meta{B}$;
	\item If $\meta{D}_{1},…,\meta{D}_{n},\meta{A}\eor\meta{B},\meta{A}\vDash\meta{C}$ and $\meta{D}_{1},…,\meta{D}_{n},\meta{A}\eor\meta{B},\meta{B}\vDash\meta{C}$, then $\meta{D}_{1},…,\meta{D}_{n},\meta{A}\eor\meta{B}\vDash\meta{C}$.
\end{itemize}


% \section{Contradiction}
% We have only one connective left to deal with: negation. But we shall not tackle negation directly. Instead, we shall first think about \emph{contradiction}. 

% An effective form of argument is to argue your opponent into contradicting themselves. At that point, you have them on the ropes. They have to give up at least one of their assumptions. We are going to make use of this idea in our proof system, by adding a new symbol, `$\ered$', to our proofs. This should be read as something like `contradiction!'\ or `reductio!'\ or `but that's absurd!'  And the rule for introducing this symbol is that we can use it whenever we explicitly contradict ourselves, i.e., whenever we find both a sentence and its negation appearing in our proof:
% \factoidbox{
% \begin{proof}
% \have[m]{a}{\meta{A}}
% \have[n]{na}{\enot\meta{A}}
% \have[ ]{bot}{\ered}\ri{a, na}
% \end{proof}}
% It does not matter what order the sentence and its negation appear in, and they do not need to appear on adjacent lines. However, we always cite the sentence first, followed by its negation. 

% Our elimination rule for `$\ered$' is known as \emph{ex falso quod libet}. This means `anything follows from a contradiction'. And the idea is precisely that: if we obtained a contradiction, symbolised by `$\ered$', then we can derive whatever we like. How can this be motivated, as a rule of argumentation? Well, consider the English rhetorical device `… and if \emph{that's} true, I'll eat my hat'. Since contradictions simply cannot be true, if one \emph{i}s true then not only will I eat my hat, I'll have it too.\footnote{Thanks to Adam Caulton for this.} Here is the formal rule:
% \factoidbox{\begin{proof}
% \have[m]{bot}{\ered}
% \have[ ]{}{\meta{A}}\re{bot}
% \end{proof}}
% Note that \meta{A} can be \emph{any} sentence whatsoever. 

% A final remark. I have said that `$\ered$' should be read as something like `contradiction!' But this does not tell us much about the symbol. There are, roughly, three ways to approach the symbol. 
% 	\begin{itemize}
% 		\item We might regard `$\ered$' as a new atomic sentence of \TFL, but one which can only ever have the truth value False. 
% 		\item We might regard `$\ered$' as an abbreviation for some canonical contradiction, such as `$A \eand \enot A$'. This will have the same effect as the above – obviously, `$A \eand \enot A$' only ever has the truth value False – but it means that, officially, we do not need to add a new symbol to \TFL.
% 		\item We might regard `$\ered$', not as a symbol of \TFL, but as something more like a \emph{punctuation mark} that appears in our proofs. (It is on a par with the line numbers and the vertical lines, say.)
% 	\end{itemize}
% There is something very philosophically attractive about the third option. But here I shall \emph{officially} plump for the second. `$\ered$' is to be read as abbreviating some canonical contradiction. This means that we can manipulate it, in our proofs, just like any other sentence.


\section{Negation}

Natural deduction rules for negation are tricky, in part because there are so many options. All of them stem from the same basic insight: when assuming \meta{A} goes awry in some way, conclude $\enot \meta{A}$; and likewise, if assuming  $\enot \meta{A}$ goes awry, conclude \meta{A}.

Our \define{negation introduction} rule fits this pattern very clearly: \factoidbox{
	\begin{proof}
	\open
	\hypo[i]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have[\ ]{}{\vdots}
	\have[\ ]{}{\enot\meta{A}}\ni{a-b,a-c}	
	\end{proof}
} Here, we can derive a sentence and its negation both within the range of the assumption that \meta{A}. So if \meta{A} were assumed, something contradictory would be derivable under that assumption. (We could apply conjunction introduction to lines $j$ and $k$ to make the contradiction explicit, but that wouldn't be strictly necessary.) Since contradictions are fundamentally unacceptable as the termination of a chain of argument, we must have begun with an inappropriate starting point when we assumed \meta{A}. So, in fact, we conclude, $\enot \meta{A}$, discharging our erroneous assumption that \meta{A}. There is no need for the line with \meta{B} on it to occur before the line with $\enot\meta{B}$ on it.

This rule is interesting, because it is \emph{almost} its own elimination rule too! Consider this schematic proof: \begin{proof}
	\open 
	\hypo[i]{a}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have{}{\enot\enot\meta{A}}\ni{a-b,a-c}
\end{proof} This proof terminates with a sentence that is logically equivalent to \meta{A}, discharing the assumption that $\enot\meta{A}$ because it leads to contradictory conclusions. This looks awfully close to a rule of negation elimination – if only we could find a way to replace a doubly-negated sentence $\enot\enot\meta{A}$ by the logically equivalent sentence \meta{A}, which would have eliminated the negation from the problematic assumption $\enot\meta{A}$.

In our system, we approach this problem by the brute force method – we allow ourselves to use the derivation of contradictory sentences from a negated sentence to motivate the elimination of that negation. This leads to our rule of \define{negation elimination}:
\factoidbox{
	\begin{proof}
	\open
	\hypo[i]{a}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have[\ ]{}{\vdots}
	\have[\ ]{}{\meta{A}}\ne{a-b,a-c}	
	\end{proof}
} 

To see our negation rules in action, consider:
	$$P \ttherefore (P \eand D) \eor (P \eand \enot D).$$
Here is a proof corresponding with the argument:
	\begin{proof}
		\hypo{a}{P}
		\open
			\hypo{b}{\enot \bigl((P \eand D) \eor (P \eand \enot D)\bigr)}
			\open
				\hypo{d}{D}
				\have{e}{(P \eand D)}\ai{a,d}
				\have{f}{(P \eand D) \eor (P \eand \enot D)}\oi{e}
			\close
			\have{nd}{\enot D}\ni{d-f,d-b}
			\have{pnd}{(P \eand \enot D)}\ai{a,nd}
			\have{ff}{\bigl((P \eand D) \eor (P \eand \enot D)\bigr)}\oi{pnd}
		\close
		\have{dn}{(P \eand D) \eor (P \eand \enot D)}\ne{b-ff}
	\end{proof} I make two comments. In line 6, the justification cites line 2 which lies outside the subproof. That is okay, since  the application of the rule lies within the range of the assumption of line 2. In line 9, the justification only cites the subproof from 2 to 8, rather than two ranges of line numbers. This is because in this application of our rule, we have the special case where the sentence such that both it and its negation can be derived from the assumption \emph{is} that assumption. It would be trivial to derive it from itself.

Well, consider:
\begin{proof}
	\hypo{a}{\enot(P \eor Q)}
	\open
		\hypo{p}{P}
		\have{na}{P \eor Q}\oi{p}
		\have{r}{\enot (P \eor Q)}\by{R}{a}
	\close
	\have{np}{\enot P}\ni{p-r}
\end{proof}
This is a fairly typical use of the R rule. If we had not used it, we would have had an awkward (though by no means incorrect) appeal to the instance of `$\enot(P\eor Q)$' in line 1 as part of the application of the $\enot$I rule to line 2. The use of reiteration makes it obvious that a contradiction really is derivable in the range of the assumption `$P$'. The primary use of reiteration is simply to make clear when a sentence falls in the range of an assumption.





The negation rules are supported by these valid \TFL\ argument forms: \begin{itemize}
	\item If $\meta{C}_{1},…,\meta{C}_{n},\meta{A} \vDash \meta{B}$ and $\meta{C}_{1},…,\meta{C}_{n},\meta{A} \vDash \enot\meta{B}$, then $\meta{C}_{1},…,\meta{C}_{n} \vDash \enot\meta{A}$;
	\item If $\meta{C}_{1},…,\meta{C}_{n},\enot\meta{A} \vDash \meta{B}$ and $\meta{C}_{1},…,\meta{C}_{n},\enot\meta{A} \vDash \enot\meta{B}$, then $\meta{C}_{1},…,\meta{C}_{n} \vDash \meta{A}$;
\end{itemize}

\emph{These are all of the basic rules for the proof system for \TFL.} Figure \ref{fig:vi} shows a long proof involving most of our rules in action. \begin{figure}
	\begin{proof}
	\open
	\hypo{npvq}{\enot P \eor Q}
		\open
		\hypo{np1}{\enot P}
			\open
			\hypo{p1}{P}
				\open
				\hypo{nq}{\enot Q}
				\have{c}{P\eand\enot P}\ai{p1,np1}
				\have{d}{P}\ae{c}
				\have{e}{\enot P}\ae{c}
				\close
			\have{q1}{Q}\ne{nq-e}
			\close
		\have{ipq1}{P \eif Q}\ci{p1-q1}
		\close
		\open
		\hypo{q2}{Q}
			\open
			\hypo{p2}{P}
			\have{qq}{Q \eand Q}\ai{q2,q2}
			\have{q3}{Q}\ae{qq}
			\close
		\have{ipq2}{P \eif Q}\ci{p2-q3}
		\close
		\have{ipq3}{P\eif Q}\oe{npvq,np1-ipq1,q2-ipq2}
		\close
		\open
		\hypo{ipq4}{P \eif Q}
			\open
			\hypo{nnpvq}{\enot(\enot P \eor Q)}
				\open
				\hypo{p3}{P}
				\have{q4}{Q}\ce{ipq4,p3}
				\have{npvq2}{\enot P \eor Q}\oi{q4}
				\close
			\have{np}{\enot P}\ni{p3-nnpvq,p3-npvq2}
			\have{npvq3}{\enot P \eor Q}\oi{np}
			\close
		\have{npvq4}{\enot P \eor Q}\ne{nnpvq-nnpvq,nnpvq-npvq3}
		\close
		\have{z}{(\enot P \eor Q)\eiff(P \eif Q)}\bi{npvq-ipq3,ipq4-npvq4}
\end{proof}\caption{Proof from no assumptions of $(\enot P \eor Q)\eiff(P \eif Q)$\label{fig:vi}}
\end{figure}



\section{Inferentialism}\label{inferentialism}

You will have noticed that our rules come in pairs: an introduction rule that tells you how to introduce a connective into a proof from what you have already, and an elimination rule that tells you how to remove it in favour of its consequences. These rules can be justified by consideration of the meanings we assigned to the connectives of \TFL\ in the schematic truth tables of §\ref{s:SchematicTruthTables}.

But perhaps we should invert this order of justification. After all, the proof rules already seem to capture (more or less) how `and', `or', `not', `if', and `iff' work in conversation. We make some claims and assumptions. The introduction and elimination rules summarise how we might proceed in our conversation against the background of those claims and assumptions. Many philosophers have thought that the meaning of an expression is entirely governed by how it is or might be used in a conversation by competent speakers – in slogan form, \emph{meaning is fixed by use}. If these proof rules describe how we might bring an expression into a conversation, and what we may do with it once it is there, then these proof rules describe the totality of facts on which meaning depends. The meaning of a connective, according to this \define{inferentialist} picture, is represented by its introduction and elimination rules – and not by the truth-function that a schematic truth table represents. On this view, it is the correctness of the schematic proof of $\meta{A}\vee\meta{B}$ from $\meta{A}$ which explains \emph{why} the schematic truth table for `$\eor$' has a T on every row on which at least one of its constituents gets a T. 

There is a significant debate on just this issue in the philosophy of language, about the nature of \emph{meaning}. Is the meaning of a word what it represents, the view sometimes called \define{representationalism}? Or is the meaning of a word, rather, given by some rules for how to use it, as inferentialism says? We cannot go deeply into this issue here, but I will say a little. The representationalist view seems to accomodate some expressions very well: the meaning of a name, for example, seems very plausibly to be identified with what it names; the meaning of a predicate might be thought of as the corresponding property. But inferentialism seems more natural as an approach to the logical connectives: 
\begin{quote}
	Anyone who has learnt to perform [conjunction introduction and conjunction elimination] knows the meaning of ‘and’, for there is simply nothing more to knowing the meaning of ‘and’ than being able to perform these inferences.\footnote{A N Prior (1961) `The Runabout Inference-Ticket', \emph{Analysis} \textbf{21}: 38.}
\end{quote} It seems rather unnatural, by contrast, to think that the meaning of `and' is some abstract mathematical `thing' represented by a truth table.


Can the inferentialist distinguish good systems of rules, such as those governing `and', from bad systems? The problem is that without appealing to truth tables or the like, we seem to be committed to the legitimacy of rather problematic connectives. The most famous example is Prior's `tonk' governed by these rules:

\begin{minipage}{0.35\textwidth}
	\begin{proof}
		\have[m]{a}{\meta{A}}
		\have[\ ]{}{\vdots}
		\have[n]{}{\meta{A}\text{ tonk }\meta{B}} \by{tonk-I}{a}
	\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
		\have[m]{a}{\meta{A}\text{ tonk }\meta{B}}
		\have[\ ]{}{\vdots}
		\have[n]{}{\meta{B}} \by{tonk-E}{a}
	\end{proof}
\end{minipage}

You will notice that `tonk' has an introduction rule like `\eor', and an elimination rule like `\eand'. Of course `tonk' is a connective we would not like in a language, since pairing the introduction and elimination rules would allow us to prove any arbitrary sentence from any assumption whatsover: \begin{proof}
	\hypo{a}{P}
	\have{b}{P\text{ tonk }Q}\by{tonk-I}{a}
	\have{c}{Q}\by{tonk-E}{b}
\end{proof}

If we are to rule out such deviant connectives as `tonk', Prior argues, we have to accept that  `an expression must have some independently determined meaning before we can discover whether inferences involving it are valid or invalid' (Prior, \emph{op}. \emph{cit}., p. 38). We cannot, that is, accept the inferentialist position that the rules of implication come first and the meaning comes second. Inferentialists have replied, but we must unfortunately leave this interesting debate here for now.\footnote{The interested reader might wish to start with this reply to Prior: Nuel D Belnap, Jr (1962) `Tonk, Plonk and Plink', \emph{Analysis} \textbf{22}: 130–34.}

\keyideas{
	\item The rules for our system are summarised on page \pageref{ProofRules}.
	\item It is important that we keep track of restrictions on when we can make use of claims derived in a subproof, since those subproofs may be making use of assumptions we are no longer accepting.
	\item Our proof rules match the interpretation of \TFL\ we have given – they will not permit us to say that some claim is provable from some assumptions when that claim isn't entailed by those assumptions. 
	\item The philosophical question of whether connectives are given meaning by their truth tables or by their natural deduction rules is an interesting one.
}

\practiceproblems

\problempart
The following two `proofs' are \emph{incorrect}. Explain the mistakes they make.
\begin{multicols}{2}
\begin{proof}
\hypo{abc}{\enot L \eif (A \eand L)}
\open
	\hypo{nl}{\enot L}
	\have{a}{A}\ce{abc, nl}
\close
\open
	\hypo{l}{L}
	\have{red}{L \eand \enot L}\ai{l, nl}
	\open
		\hypo{lala}{\enot A}
		\have{lal}{L}\ci{red}
		\have{lla}{\enot L}\ce{red}
	\close
	\have{a2}{A}\ni{lala-lal,lala-lla}
\close
\have{con}{A}\oe{nl-a, l-a2}
\end{proof}

\begin{proof}
\hypo{abc}{A \eand (B \eand C)}
\hypo{bcd}{(B \eor C) \eif D}
\have{b}{B}\ae{abc}
\have{bc}{B \eor C}\oi{b}
\have{d}{D}\ce{bc, bcd}
\end{proof}
\end{multicols}
\problempart
The following three proofs are missing their commentaries (rule and line numbers). Add them, to turn them into bona fide proofs. Additionally, write down the argument that corresponds to each proof.
\begin{multicols}{2}
\begin{proof}
\hypo{ps}{P \eand S}
\have{p}{P}%\ae{ps}
\have{s}{S}%\ae{ps}
\open
\hypo{nsor}{S \eif R}
\have{r}{R}%\ce{nsor, s}
\have{re}{R \eor E}%\oi{r}
\end{proof}

\begin{proof}
\hypo{ad}{A \eif D}
\open
	\hypo{ab}{A \eand B}
	\have{a}{A}%\ae{ab}
	\have{d}{D}%\ce{ad, a}
	\have{de}{D \eor E}%\oi{d}
\close
\have{conc}{(A \eand B) \eif (D \eor E)}%\ci{ab-de}
\end{proof}

\begin{proof}
\hypo{nlcjol}{\enot L \eif (J \eor L)}
\open
\hypo{nl}{\enot L}
\have{jol}{J \eor L}%\ce{nlcjol, nl}
\open
	\hypo{j}{J}
	\have{jj}{J \eand J}%\ai{j}
	\have{j2}{J}%\ae{jj}
\close
\open
	\hypo{l}{L}
	\open
		\hypo{nj}{\enot J}
	\close
	\have{j3}{J}%\ne{nj–l,nj-nl}
\close
\have{conc}{J}%\oe{jol, j-j2, l-j3}
\end{proof}
\end{multicols}

\solutions
\problempart
\label{pr.solvedTFLproofs}
Give a proof for each of the following arguments:
\begin{earg}
\item $J\eif\enot J \ttherefore \enot J$
\item $Q\eif(Q\eand\enot Q) \ttherefore \enot Q$
\item $A\eif (B\eif C) \ttherefore (A\eand B)\eif C$
\item $K\eand L \ttherefore K\eiff L$
\item $(C\eand D)\eor E \ttherefore E\eor D$
\item $A\eiff B, B\eiff C \ttherefore A\eiff C$
\item $\enot F\eif G, F\eif H \ttherefore G\eor H$
\item $(Z\eand K) \eor (K\eand M), K \eif D \ttherefore D$
\item $P \eand (Q\eor R), P\eif \enot R \ttherefore Q\eor E$
\item $S\eiff T \ttherefore S\eiff (T\eor S)$
\item $\enot (P \eif Q) \ttherefore \enot Q$
\item $\enot (P \eif Q) \ttherefore P$
\end{earg}




\chapter{Proof-theoretic concepts}\label{s:ProofTheoreticConcepts}

We shall introduce some new vocabulary. The following expression:
$$\meta{A}_1, \meta{A}_2, …, \meta{A}_n \proves \meta{C}$$
means that there is some proof which starts with assumptions among $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$ and ends with $\meta{C}$ (and no undischarged assumptions other than those we started with). We can read this claim as `\meta{C} is provable from $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$'.

The symbol `$\proves$' is called the \emph{single turnstile}. I want to emphasise that this is not the {double turnstile} symbol (`$\entails$') that we used to symbolise entailment in chapters \ref{ch.TruthTables} and \ref{ch.semantics}. The single turnstile, `$\proves$', concerns the existence of formal proofs; the double turnstile, `$\entails$', concerns the existence of valuations (or interpretations, when used for \FOL). \emph{They are very different notions.}\footnote{They are very different, but if we've designed our proof system well, we shouldn't be able to prove a conclusion from some assumptions \emph{unless} that conclusion validly follows from those assumptions. And if we are really fortunate, we should be able to provide a proof corresponding to any valid argument. More on this in §\ref{sec:soundcomp}.}

Derivatively, we shall write:
$$\proves \meta{A}$$
to mean that there is a proof of $\meta{A}$ which ends up having no undischarged assumptions. We now define:
	\factoidbox{
		$\meta{A}$ is a \define{theorem} iff $\proves \meta{A}$
	}
To illustrate this, suppose I want to prove that `$\enot (A \eand \enot A)$' is a theorem. So I must start my proof without \emph{any} assumptions. However, since I want to prove a sentence whose main connective is a negation, I shall want to  immediately begin a subproof, with the additional assumption `$A \eand \enot A$', and show that this leads to contradictory consequences. All told, then, the proof looks like this:
	\begin{proof}
		\open
			\hypo{con}{A \eand \enot A}
			\have{a}{A}\ae{con}
			\have{na}{\enot A}\ae{con}
		\close
		\have{lnc}{\enot (A \eand \enot A)}\ni{con-a,con-na}
	\end{proof}
We have therefore proved `$\enot (A \eand \enot A)$' on no (undischarged) assumptions. This particular theorem is an instance of what is sometimes called \emph{the Law of Non-Contradiction}, that for any \meta{A}, $\enot(\meta{A}\eand\enot\meta{A})$.

To show that something is a theorem, you just have to find a suitable proof. It is typically much harder to show that something is \emph{not} a theorem. To do this, you would have to demonstrate, not just that certain proof strategies fail, but that \emph{no} proof is possible. Even if you fail in trying to prove a sentence in a thousand different ways, perhaps the proof is just too long and complex for you to make out. Perhaps you just didn't try hard enough.

Here is another new bit of terminology:
	\factoidbox{
		Two sentences \meta{A} and \meta{B} are \define{provably equivalent} iff each can be proved from the other; i.e., both $\meta{A}\proves\meta{B}$ and $\meta{B}\proves\meta{A}$.
		}
As in the case of showing that a sentence is a theorem, it is relatively easy to show that two sentences are provably equivalent: it just requires a pair of proofs. Showing that sentences are \emph{not} provably equivalent would be much harder: it is just as hard as showing that a sentence is not a theorem. Notice that if we have two proofs, one demonstrating $\meta{A}\proves\meta{B}$ and the other demonstrating $\meta{B}\proves\meta{A}$, we can put them together as subproofs of a single proof, and apply $\eiff$I to provide a proof from no assumptions which demonstrates that $\proves\meta{A}\eiff\meta{B}$.

Here is a third, related, bit of terminology:
	\factoidbox{
		The sentences $\meta{A}_1,\meta{A}_2,…, \meta{A}_n$ are \define{jointly contrary} iff a contradiction can be proved from them, i.e., for some $\meta{B}$, $\meta{A}_1,\meta{A}_2,…, \meta{A}_n \proves \meta{B}\eand\enot\meta{B}$.
	}
It is easy to show that some sentences are jointly contrary: you just need to provide a proof of the contradiction from assuming all the sentences. Showing that some sentences are not jointly contrary is much harder. It would require more than just providing a proof or two; it would require showing that no proof of a certain kind is \emph{possible}.


This table summarises whether one or two proofs suffice, or whether we must reason about all possible (attempted) proofs.

\begin{center}
\begin{tabular}{l l l} \toprule 
%\cline{2-3}
 & \textbf{Yes} & \textbf{No}\\
 \midrule
%\cline{2-3}
theorem? & one proof & all possible proofs\\
%contradiction? &  one proof  & all possible proofs\\
equivalent? & two proofs & all possible proofs\\
not contrary? & all possible proofs & one proof\\
provable & one proof & all possible proofs\\

\bottomrule \end{tabular}
\end{center}

\keyideas{
	\item Our formal proof system allows us to introduce a notion of provability, symbolised `$\proves$'. This is distinct from entailment `$\entails$', but (if we've done our work correctly) they will parallel one another.
	\item We can introduce further notions in terms of provability: provable equivalence, joint contrariness, and being a theorem. 
}

\practiceproblems
\problempart
Show that each of the following sentences is a theorem:
\begin{earg}
\item $O \eif O$
\item $N \eor \enot N$
\item $J \eiff \bigl(J\eor (L\eand\enot L) \bigr)$
\item $((A \eif B) \eif A) \eif A$ 
\end{earg}

\problempart
Provide proofs to show each of the following:
\begin{earg}
\item $C\eif(E\eand G), \enot C \eif G \proves G$
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\end{earg}

\problempart
Show that each of the following pairs of sentences are provably equivalent:
\begin{earg}
\item $R \eiff E$, $E \eiff R$
\item $G$, $\enot\enot\enot\enot G$
\item $T\eif S$, $\enot S \eif \enot T$
\item $U \eif I$, $\enot(U \eand \enot I)$
\item $\enot (C \eif D), C \eand \enot D$
\item $\enot G \eiff H$, $\enot(G \eiff H)$ 
\end{earg}

\problempart
If you know that $\meta{A}\proves\meta{B}$, what can you say about $(\meta{A}\eand\meta{C})\proves\meta{B}$? What about $(\meta{A}\eor\meta{C})\proves\meta{B}$? Explain your answers.


\problempart In this section, I claimed that it is just as hard to show that two sentences are not provably equivalent, as it is to show that a sentence is not a theorem. Why did I claim this? (\emph{Hint}: think of a sentence that would be a theorem iff \meta{A} and \meta{B} were provably equivalent.)





\chapter{Proof strategies}\label{c:proof.strat}
There is no simple recipe for proofs, and there is no substitute for practice. Here, though, are some rules of thumb and strategies to keep in mind.

\paragraph{Work backwards from what you want.}
The ultimate goal is to obtain the conclusion. Look at the conclusion and ask what the introduction rule is for its main connective. This gives you an idea of what should happen \emph{just before} the last line of the proof. Then you can treat this line as if it were your goal. Ask what you could do to get to this new goal.

For example: If your conclusion is a conditional $\meta{A}\eif\meta{B}$, plan to use the {\eif}I rule. This requires starting a subproof in which you assume \meta{A}. The subproof ought to end with \meta{B}. So, what can you do to get $\meta{B}$?


\paragraph{Work forwards from what you have.}
When you are starting a proof, look at the premises; later, look at the sentences that you have obtained so far. Think about the elimination rules for the main operators of these sentences. These will tell you what your options are.

For a short proof, you might be able to eliminate the premises and introduce the conclusion. A long proof is formally just a number of short proofs linked together, so you can fill the gap by alternately working back from the conclusion and forward from the premises.

\paragraph{Try proceeding indirectly.}
If you cannot find a way to show $\meta{A}$ directly, try starting by assuming $\enot \meta{A}$. If a contradiction follows, then you will be able to obtain $\meta{A}$ by $\enot$E. This will often be a good way of proceeding when the conclusion you are aiming at has a \emph{disjunction} as its main connective.  

\paragraph{Persist.}
These are guidelines, not laws. Try different things. If one approach fails, then try something else. 

For example: suppose you tried to follow the idea `work backwards from what you want' in establishing `$P, P \eif (P \eif Q) \proves P \eif Q$'. You would be tempted to start a subproof from the assumption `$P$', and while that proof strategy would eventually succeed, you would have done better to simply apply $\eif$E and terminate after one proof step.

By contrast, suppose you tried to follow the idea `work forwards from what you have' in trying to establish `$(P \eor (Q \eor (R \eor S))) \proves P \eif P$'. You might begin an awkward nested series of subproofs to apply $\eor$E to the disjunctive premise. But beginning with the conclusion might prompt you instead to simply open a subproof from the assumption $P$, and the subsequent proof will make no use of the premise at all, as the conclusion is a theorem.

Neither of these heuristics is sacrosanct. You will get a sense of how to construct proofs efficiently and fluently with practice. Unfortunately there is no quick substitute for practice.


\keyideas{
	\item The nature of natural deduction proofs means that it is sometimes easier to make progress by applying rules to the assumptions, and at other times easier to try and figure out where a conclusion could have come from. 
	\item One may have to try a number of different things in the course of constructing the same proof – there is no simple algorithm to capture logical reasoning in this system.
}






\chapter{Derived rules for \textnormal{\TFL}}\label{s:Further}\label{s:Derived}
In §§\ref{s:BasicTFLs}–\ref{s:BasicTFLns}, we introduced the basic rules of our proof system for \TFL. In this section, we shall consider some alternative or additional rules for our system.

None of these rules adds anything fundamentally to our system. They are all \define{derived} rules, which means that anything we can prove by using them, we could have proved using just the rules in our original official system of natural deduction proofs. Any of these rules is a \emph{conservative} addition to our proof system, because none of them would enable us to prove anything we could not already prove. (Adding the rules for `tonk' from §\ref{inferentialism}, by contrast, would allow us to prove many new things – any system which includes those rules is not a conservative extension of our original system of proof rules.)

 But sometimes adding new rules can shorten proofs, or make them more readable and user-friendly. And some of them are of interest in their own right, as arguably independently plausible rules of implication as they stand, or as alternative rules we could have taken as basic instead.

\section{Reiteration}\label{der.reit}
The first derived rule is actually one of our main proof rules: \emph{reiteration}. It turns out that we need not have assumed a rule of reiteration. We can replace each application of the reiteration rule on some line $k+1$ (reiterating some prior line $m$) with the following combination of moves deploying just the \emph{other} basic rules of §§\ref{s:BasicTFLs}–\ref{s:BasicTFLns}:
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[k]{aa}{\meta{A} \eand \meta{A}}\ai{a}
	\have{a2}{\meta{A}}\ae{aa}
\end{proof}
To be clear: this is not a proof. Rather, it is a proof  \emph{scheme}. After all, it uses a variable, $\meta{A}$, rather than a sentence of \TFL. But the point is simple. Whatever sentences of \TFL\ we plugged in for $\meta{A}$, and whatever lines we were working on, we could produce a legitimate proof. So you can think of this as a recipe for producing proofs. Indeed, it is a recipe which shows us that, anything we can prove using the rule R, we can prove (with one more line) using just the \emph{basic} rules of §§\ref{s:BasicTFLs}–\ref{s:BasicTFLns}. So we can describe the rule R as a derived rule, since its justification is derived from our basic rules.

You might note that in lines 5–7 in the complicated proof in Figure~\ref{fig:vi}, we in effect made use of this proof scheme, introducing a conjunction from prior lines only to immediately eliminate again, just to ensure that the relevant sentences appeared directly in the range of the assumption `$Q$'.

\section{Disjunctive syllogism}
Here is a very natural argument form.
	\begin{quote}
		Mitt is either in Massachusetts or in DC. He is not in DC. So, he is in Massachusetts.
	\end{quote}
This inference pattern is called \define{disjunctive syllogism}. We could add it to our proof system:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\meta{A} \eor \meta{B}}
	\have[n]{nb}{\enot \meta{A}}
	\have[\ ]{con}{\meta{B}}\by{DS}{ab, nb}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\meta{A} \eor \meta{B}}
	\have[n]{nb}{\enot \meta{B}}
	\have[\ ]{con}{\meta{A}}\by{DS}{ab, nb}
\end{proof}
\end{minipage}}
This is, if you like, a new rule of disjunction elimination.
But there is nothing fundamentally new here. We can emulate the rule of disjunctive syllogism using our basic proof rules, as this schematic proof indicates:
\begin{proof}
	\have[m]{avb}{\meta{A}\eor\meta{B}}
	\have[n]{na}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\open
		\hypo[k]{a}{\meta{A}}
		\open
			\hypo{nb}{\enot\meta{B}}
			\have{ra}{\meta{A}}\by{R}{a}
			\have{rna}{\enot\meta{A}}\by{R}{na}
			\close
		\have{b}{\meta{B}}\ne{nb-ra, nb-rna}
		\close
		\open
		\hypo{b2}{\meta{B}}
		\have{b3}{\meta{B}}\by{R}{b2}
		\close
	\have{}{\meta{B}}\oe{avb,a-b,b2-b3}
\end{proof}
We have used the rule of reiteration in this schematic proof, but we already know that any uses of that rule can themselves be replaced by more roundabout proofs using conjunction introduction and elimination, if required. So adding disjunctive syllogism would not make any new proofs possible that were not already obtainable in our original system.

\section{\emph{Modus tollens}}
Another useful pattern of inference is embodied in the following argument:
	\begin{quote}
		If Hillary won the election, then she is in the White House. She is not in the White House. So she did not win the election.
	\end{quote}
This inference pattern is called \define{\emph{modus tollens}}. The corresponding rule is:
\factoidbox{\begin{proof}
	\have[m]{ab}{\meta{A}\eif\meta{B}}
	\have[n]{a}{\enot\meta{B}}
	\have[\ ]{b}{\enot\meta{A}}\mt{ab,a}
\end{proof}}
This is, if you like, a new rule of conditional elimination.

This rule is, again, a conservative addition to our stock of proof rules. Any application of it could be emulated by this form of proof using our original rules:
\begin{proof}
	\have[m]{ab}{\meta{A}\eif\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{}{\vdots}
		\open
		\hypo[k]{a}{\meta{A}}
		\have{b}{\meta{B}}\ce{ab, a}
		\have{nb1}{\enot B}\by{R}{nb}
		\close
	\have{no}{\enot\meta{A}}\ni{a-nb1}
\end{proof}
Again, we made a dispensible use of reiteration.

\section{Double Negation Elimination}

In \TFL, the double negation $\enot\enot\meta{A}$ is equivalent to \meta{A}. In natural languages, too, double negations tend to cancel out – Malcolm is not unaware that his leadership is under threat iff he is aware that it is. That said, you should be aware that context and emphasis can prevent them from doing so. Consider: `Jane is not \emph{not} happy'. Arguably, one cannot derive `Jane is happy', since the first sentence should be understood as meaning the same as  `Jane is not \emph{un}happy'. This is compatible with `Jane is in a state of profound indifference'. As usual, moving to \TFL\ forces us to sacrifice certain nuances of English expressions – we have, in \TFL, just one resource for translating negative expressions like `not' and the suffix `un-', even if they are not synonyms in English.

Obviously we can show that $\meta{A} \vdash \enot\enot\meta{A}$ by means of the following proof:
\begin{proof}
\hypo{a}{\meta{A}}
\open
\hypo{b}{\enot\meta{A}}
\close
\have{c}{\enot\enot\meta{A}}\ni{b-a,b-b}	
\end{proof}

There is a proof rule that corresponds to the other direction of this equivalence, the rule of \define{double negation elimination}:
\factoidbox{
\begin{proof}
		\have[i]{a}{\enot\enot\meta{A}}
		\have[\ ]{}{\vdots}
		\have[\ ]{}{\meta{A}}\dne{a}
	\end{proof}}


This rule is redundant, given the proof rules of \TFL: \begin{proof}
	\hypo{a}{\enot\enot\meta{A}}
	\open
	\hypo{b}{\enot\meta{A}}
	\close
	\have{c}{\meta{A}}\ne{b-b,b-a}
\end{proof}
Anything we can prove using the $\enot\enot$E rule can be proved almost as briefly using just $\enot$E.

Note that we could \emph{replace} the $\enot$E rule by $\enot\enot$E. For we can emulate any application of $\enot$E by an application of $\enot$I, followed by a single use of $\enot\enot$E:
 \begin{proof}
	\open 
	\hypo[i]{a}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have{d}{\enot\enot\meta{A}}\ni{a-b,a-c}
	\have{}{\meta{A}} \dne{d}
\end{proof}
This proof shows that, in a system with {\enot}I and {\enot\enot}E, we do not need any separate elimination rule for a single negation – the effect of any such rule could be perfectly simulated by the above schematic proof. The addition of a single negation elimination rule would not allow us to prove any more than we already can. So in a sense, the rules of double negation elimination and negation elimination are equivalent, at least given the other rules in our system.

\section{\emph{Tertium non datur}}

Suppose that we can show that if it's sunny outside, then Bill will have brought an umbrella (for fear of burning). Suppose we can also show that, if it's not sunny outside, then Bill will have brought an umbrella (for fear of rain). Well, there is no third way for the weather to be. So, \emph{whatever the weather}, Bill will have brought an umbrella. 

This line of thinking motivates the following rule:
\factoidbox{\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}}
			\have[j]{c1}{\meta{B}}
		\close
		\open
			\hypo[k]{b}{\enot\meta{A}}
			\have[l]{c2}{\meta{B}}
		\close
		\have[\ ]{ab}{\meta{B}}\tnd{a-c1,b-c2}
	\end{proof}}
The rule is sometimes called \define{\emph{tertium non datur}}, which means `no third way'. There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first.

\emph{Tertium non datur} is able to be emulated using just our original proof rules. Here is a schematic proof which demonstrates this:
\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}}
			\have[j]{c1}{\meta{B}}
		\close
		\open
			\hypo[k]{b}{\enot\meta{A}}
			\have[l]{c2}{\meta{B}}
		\close
		\have[\ ]{}{\vdots}
		\have[m]{ab}{\meta{A}\eif\meta{B}}\ci{a-c1}
		\have{nab}{\enot\meta{A}\eif\meta{B}}\ci{b-c2}
		\open
			\hypo{nb}{\enot\meta{B}}
			\open
				\hypo{aa}{\meta{A}}
				\have{b2}{\meta{B}}\ce{ab,aa}
				\have{nb2}{\enot\meta{B}}\by{R}{nb}
				\close
			\have{na}{\enot\meta{A}}\ni{aa-nb2}
			\have{b3}{\meta{B}}\ce{nab,na}
			\close
		\have{b4}{\meta{B}}\ne{nb-b3}
	\end{proof}
Again a dispensible use of reiteration occurs in this proof just to make it more readable.



\section{De Morgan Rules}
Our final additional rules are called De Morgan's Laws. (These are named after the nineteenth century logician August De Morgan.) The first two De Morgan rules show the provable equivalence of a negated conjunction and a disjunction of negations.
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\enot (\meta{A} \eand \meta{B})}
	\have[\ ]{dm}{\enot \meta{A} \eor \enot \meta{B}}\dem{ab}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\enot \meta{A} \eor \enot \meta{B}}
	\have[\ ]{dm}{\enot (\meta{A} \eand \meta{B})}\dem{ab}
\end{proof}
\end{minipage}}
The second pair of De Morgan rules are dual to the first pair: they show the provable equivalence of a negated disjunction and a conjunction of negations.
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\enot (\meta{A} \eor \meta{B})}
	\have[\ ]{dm}{\enot \meta{A} \eand \enot \meta{B}}\dem{ab}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\enot \meta{A} \eand \enot \meta{B}}
	\have[\ ]{dm}{\enot (\meta{A} \eor \meta{B})}\dem{ab}
\end{proof}
\end{minipage}}

\emph{These are all of the additional rules of our proof system for \TFL.}

The De Morgan rules are no genuine addition to the power of our original natural deduction system. Here is a demonstration of how we could derive the first De Morgan rule:
 	\begin{proof}
		\have[k]{nab}{\enot (\meta{A} \eand \meta{B})}
		\open
		\hypo[m]{n}{\enot(\enot\meta{A}\eor\enot\meta{B})}
			\open
			\hypo{na}{\enot\meta{A}}
			\have{navnb}{\enot\meta{A}\eor\enot\meta{B}}\oi{na}
			\close
		\have{a}{\meta{A}}\ne{na-navnb,na-n}
			\open
			\hypo{nb}{\enot\meta{B}}
			\have{nav2}{\enot\meta{A}\eor\enot\meta{B}}\oi{nb}
			\close
		\have{b}{\meta{B}}\ne{nb-nav2,nb-n}
		\have{aab}{\meta{A}\eand\meta{B}}\ai{a,b}
		\close
	\have{c}{\enot\meta{A}\eor\enot\meta{B}}\ne{n-aab,n-nab}
	\end{proof}
Here is a demonstration of how we could derive the second De Morgan rule:
 	\begin{proof}
		\have[k]{nab}{\enot \meta{A} \eor \enot \meta{B}}
		\open
		\hypo[m]{na}{\enot \meta{A}}
			\open
			\hypo{ab}{\meta{A} \eand \meta{B}}
			\have{a}{\meta{A}}\ae{ab}
			\close
		\have{naab}{\enot(\meta{A}\eand\meta{B})}\ni{ab-a,ab-na}
			\close
			\open
		\hypo{nb}{\enot\meta{B}}
			\open
			\hypo{ab2}{\meta{A} \eand \meta{B}}
			\have{b}{\meta{B}}\ae{ab2}
			\close
			\have{naab2}{\enot(\meta{A}\eand\meta{B})}\ni{ab2-b,ab2-nb}
			\close
		\have{c}{\enot(\meta{A}\eand\meta{B})}\oe{nab,na-naab,nb-naab2}
	\end{proof}
Similar demonstrations can be offered explaining how we could derive the third and fourth De Morgan rules. These are left as exercises.



\keyideas{
	\item Our official system of rules can be augmented by additional rules that are strictly speaking unneccessary – nothing is provable with them that couldn't have been proved without them – but that can nevertheless be used sometimes to speed up proofs.
	\item Only make use of derived rules when you are told you may do so.
	\item Some derived rules - such as the rule of double negation elimination – can even be used in place of a rule of our original system, given a different system but with the same things being provable.
}

\practiceproblems
\solutions
\problempart
\label{pr.justifyTFLproof}
The following proofs are missing their commentaries (rule and line numbers). Add them wherever they are required: you may use any of the original or derived rules, as appropriate.

\begin{minipage}{0.35\textwidth}
\begin{proof}
\hypo{1}{W \eif \enot B}
\hypo{2}{A \eand W}
\hypo{2b}{B \eor (J \eand K)}
\have{3}{W}{}
\have{4}{\enot B} {}
\have{5}{J \eand K} {}
\have{6}{K}{}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
\begin{proof}
\hypo{1}{L \eiff \enot O}
\hypo{2}{L \eor \enot O}
\open
	\hypo{a1}{\enot L}
	\have{a2}{\enot O}{}
	\have{a3}{L}{}
	\have{a4}{\enot L}{}
\close
\have{3b}{\enot\enot L}{}
\have{3}{L}{}
\end{proof}\end{minipage}

\begin{proof}
\hypo{1}{Z \eif (C \eand \enot N)}
\hypo{2}{\enot Z \eif (N \eand \enot C)}
\open
	\hypo{a1}{\enot(N \eor  C)}
	\have{a2}{\enot N \eand \enot C} {}
	\have{a6}{\enot N}{}
	\have{b4}{\enot C}{}
		\open
		\hypo{b1}{Z}
		\have{b2}{C \eand \enot N}{}
		\have{b3}{C}{}
		\have{red}{\enot C}{}
	\close
	\have{a3}{\enot Z}{}
	\have{a4}{N \eand \enot C}{}
	\have{a5}{N}{}
\close
\have{3b}{\enot\enot(N \eor C)}{}
\have{3}{N \eor C}{}
\end{proof}


\problempart 
Give a proof for each of these arguments:
\begin{earg}
\item $E\eor F$, $F\eor G$, $\enot F \ttherefore E \eand G$
\item $M\eor(N\eif M) \ttherefore \enot M \eif \enot N$
\item $(M \eor N) \eand (O \eor P)$, $N \eif P$, $\enot P \ttherefore M\eand O$
\item $(X\eand Y)\eor(X\eand Z)$, $\enot(X\eand D)$, $D\eor M$ \ttherefore $M$
\end{earg}


\problempart
Provide proof schemes that justify the addition of the third and fourth De Morgan rules as derived rules. 



\problempart
The proofs you offered in response to question \textbf{A} above used derived rules. Replace the use of derived rules, in such proofs, with only basic rules. You will find some `repetition' in the resulting proofs; in such cases, offer a streamlined proof using only basic rules.  (This will give you a sense, both of the power of derived rules, and of how all the rules interact.)