%!TEX root = forallx-adl.tex

% Æ: pervasive changes to the proof system from the earlier versions; the one here is pretty much the system of Halbach \emph{Logic Manual}, though transposed to a Fitch style system. We used to have a rule of ¬¬E as basic, and ¬E as derived, but proofs looked uglier. 

\part{Natural Deduction for {\TFL}}
\label{ch.NDTFL}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{Proof and Reasoning}\label{s:NDVeryIdea}

\section{Arguments and Reasoning Revisited} % (fold)
\label{sec:arguments_and_reasoning_revisited}

% section arguments_and_reasoning_revisited (end)

Back in §\ref{s:Valid}, we said that a symbolised argument is valid iff it is not possible to make all of the premises true in a valuation, while the conclusion is false. 

In the case of \TFL, this led us to develop truth tables. Each row of a complete truth table corresponds to a valuation. So, when faced with a \TFL\ argument, we have a very direct way to assess whether it is possible to make all of the premises true and the conclusion false: just plod through the truth table.

But truth tables do not necessarily give us much \emph{insight}. Consider two arguments in \TFL:
	\begin{align*}
		P \eor Q, \enot P & \ttherefore Q\\
		P \eiff Q, \enot P & \ttherefore \enot Q.
	\end{align*}
Clearly, these are valid arguments. You can confirm that they are valid by constructing four-row truth tables. With truth tables, we only really care about the truth values assigned to whole sentences, since that is what ultimately determines whether there is an entailment. But we might say that these two arguments, proceeding from different premises with different logical connectives involved, must make use of different principles of \define{implication} – different principles about what \emph{follows from} what. What follows from a disjunction is not at all the same as what follows from a biconditional, and it might be nice to keep track of these differences. While a truth table can show \emph{that} an argument is valid, it doesn't really explain why the argument is valid. To explain why $P \eor Q, \enot P \ttherefore Q$ is valid, we have to say something about how disjunction and negation work and interact.

Certainly human reasoning treats disjunctions very differently from biconditionals. While logic is not really about human reasoning, which is more properly a subject matter for psychology, nevertheless we can formally study the different forms of reasoning involved in arguing from sentences with different structures, by asking: \emph{what would it be acceptable to conclude from premises with a certain formal structure, supposing one cannot give up one's committment to those premises?}

The role of reasoning should not be overstated. Many principles of good reasoning have no place in logic, because they concern how to make judgments on the basis of inconclusive evidence. Our focus here is on implication, whether or not it would be good reasoning to form beliefs in accordance with those implications. The idea here is that `$P \eor Q$' and `$\enot P$' implies `$Q$', whether or not it would be a good idea to belive `$Q$' if you believed those premises. To emphasise a point we made earlier (§\ref{ss:reasoning}): maybe once you notice that these premises imply `$Q$', what good reasoning demands is that you give up one of those premises.

\section{Formal Proof and Natural Deduction}

In special cases, thinking about reasoning can help us understand logical implication. Reasoning often occurs step-by-step: you accept a certain claim, and deduce some intermediate further claim, and from there derive a conclusion. There is an approach to logical argument that mirrors this step-by-step approach. The idea is to understand argument by applying very obvious, seemingly trivial, rules to sentences of a logical language in virtue of their structure, generating certain derived results, to which the rules can be applied again, and then again in turn to further derived results. Some of these rules will govern the behaviour of the sentential connectives. Others will govern the behaviour of the quantifiers and identity. The whole system of rules will govern the construction of a proof (or derivation) that proceeds validly \emph{from} some premises \emph{to} a conclusion.

\emph{This is a very different way of thinking about arguments.} Rather than thinking about the possible meanings of the argument using valuations or interpretations, we manipulate the sentences involved in the premises and conclusion to construct an object, the proof,  which directly demonstrates the validity of the argument. 

Very abstractly, a \define{formal proof} is a sequence of sentences, such that each sentence is justified by some rule of the proof system, possibly given some previous sentence or sentences. (The sentences may also be accompanied by a `commentary' explaining how that sentence is justified.) There are lots of different systems for constructing formal proofs in \TFL\ and \FOL, including among others axiomatic systems, semantic tableaux, Gentzen-style natural deduction, and sequent calculus. Different systems may be more efficient, or simpler to use, or better when one is reasoning about formal proofs rather than constructing them. 
 All formal proof systems share two important features: \begin{itemize}
	\item The rules are unambiguous; and
	\item The rules apply at some stage of a proof because of the syntactic structure of the sentences involved – no consideration of valuations or interpretations is involved.
\end{itemize} These two features make it easy to design a computer program that produces formal proofs, as long as it can parse and analyse the syntax of expressions. No consideration of meanings need be involved, which makes it quite unlike the earlier ways we had of analysing arguments in \TFL, such as truth-tables, which essentially involved understanding the truth-functions that characterise the meanings of the logical connectives of the language.

The proof system we adopt is called \define{natural deduction}. It is an attractive system in many ways, in part because the rules it uses are designed to be very simple and to emulate certain obvious and natural patterns of reasoning. This makes it useful for some purposes – it is often easy to understand that the rules are correct, and it has the very nice feature that one can reuse one formal proof within the course of constructing another. Don't be misled, however: the formal proofs constructed by using natural deduction are stylised and abstracted from ordinary reasoning. Natural deduction may be slightly less artificial than using a truth table, but it is not in any sense a psychologically realistic model of reasoning. (For one thing, many `natural' instincts in human reasoning correspond to invalid patterns of argument.)

One specific way in which natural deduction is supposed to improve over truth table techniques is in the insight into how an argument works that it can provide. Rather than just discovering that one sentence cannot be true without another being true, we see almost literally how to break down premises into their consequences, and then build up from those intermediate consequences to the conclusion, where all the steps of deconstruction and reconstruction involve the use of simple and obviously correct implications. Though this doesn't mimic human reasoning perfectly, it resembles it sufficiently well that we often seem to better understand how an argument works once we've constructed a natural deduction proof of it, even if we already knew it to be valid.  Consider this pair of valid arguments:
	\begin{align*}
		\enot P \eor Q,  P & \ttherefore Q\\
		P \eif Q, P & \ttherefore Q.
	\end{align*} 
From a truth-table perspective, these arguments are indistinguishable: their premises are true in the same valuations, and so are their conclusions. Yet they are distinct from an implicational point of view, since one will involve consideration of the consequences of disjunctions and negations, and the other consideration of the consequences of the conditional. The natural deduction proofs demonstrating the validity of these arguments reflect the different connectives involved, and promise us a more fine-grained analysis of how valid arguments function.

Formal proofs are obviously useful in demonstrating validity. If you manage to construct a proof in a well-designed proof system, you'll know the corresponding argument is valid. But the aim of a good proof system is not just that all of the formal proofs it permits correspond to valid arguments: it is also that every valid argument has a proof. Our system of natural deduction rules has this property, but I won't be able to demonstrate that with complete rigour – I return to this idea in §\ref{sec:soundcomp}


\section{Efficiency in Natural Deduction} % (fold)
\label{sec:efficiency_in_natural_deduction}

% section efficiency_in_natural_deduction (end)

The use of natural deduction can be motivated by more than the search for insight. It might also be motivated by \emph{practicality}. Consider:
	$$A_1 \eif C_1 \ttherefore (A_1 \eand A_2) \eif (C_1 \eor C_2).$$
To test this argument for validity, you might use a 16-row truth table. If you do it correctly, then you will see that there is no row on which all the premises are true and on which the conclusion is false. So you will know that the argument is valid. (But, as just mentioned, there is a sense in which you will not know \emph{why} the argument is valid.) But now consider:\phantomsection\label{longtt}
	\begin{align*}
		A_1 \eif C_1 \ttherefore\ & (A_1 \eand A_2 \eand A_3 \eand A_4 \eand A_5 \eand A_6 \eand A_7 \eand A_8 \eand A_9 \eand A_{10}) \eif \phantom{(}\\
				&(C_1 \eor C_2 \eor C_3 \eor C_4 \eor C_5 \eor C_6 \eor C_7 \eor C_8 \eor C_9 \eor C_{10}).
	\end{align*}
This argument is also valid – as you can probably tell – but to test it requires a truth table with $2^{20} = 1048576$ rows. In principle, we can set a machine to grind through truth tables and report back when it is finished. In practice, complicated arguments in \TFL\ can become \emph{intractable} if we use truth tables. But there is a very short natural deduction proof of this argument – just 6 rows. You can see it on page \pageref{ndshort}, though it won't make much sense if you skip the intervening pages.

When we get to \FOL, though, the problem gets dramatically worse. There is nothing like the truth table test for \FOL. To assess whether or not an argument is valid, we have to reason about \emph{all} interpretations. But there are infinitely many possible interpretations. We cannot even in principle set a machine to grind through infinitely many possible interpretations and report back when it is finished: it will \emph{never} finish. We either need to come up with some more efficient way of reasoning about all interpretations, or we need to look for something different. Since we already have some motivation for considering the role in arguments of particular premises, rather than all the premises collectively as in the truth-table approach, we will opt for the `something different' path – natural deduction.

\section{Our System of Natural Deduction and its History} % (fold)
\label{sec:our_system_of_natural_deduction_and_its_history}

% section our_system_of_natural_deduction_and_its_history (end)

The modern development of natural deduction dates from research in the 1930s by Gerhard Gentzen and, independently, Stanisław Jaśkowski.\footnote{Gerhard Gentzen (1935) `Untersuchungen über das logische Schließen', translated as `Investigations into Logical Deduction' in M. E. Szabo, ed. (1969) \emph{The Collected Works of Gerhard Gentzen}, North-Holland. Stanisław Jaśkowski (1934) `On the rules of suppositions in formal logic', reprinted in Storrs McCall, ed. (1967) \emph{Polish logic 1920–39}, Oxford University Press.} However, the natural deduction system that we shall consider is based on slightly later work by Frederic Fitch.\footnote{Frederic Fitch (1952) \emph{Symbolic Logic: An introduction}, Ronald Press Company. 

In the design of the present proof system, I drew on earlier versions of \forallx, but also on the natural deduction systems of Jon Barwise and John Etchemendy (1992) \emph{The Language of First-Order Logic}, CSLI; and of Volker Halbach (2010) \emph{The Logic Manual}, Oxford University Press.} 

Natural deduction was so-called because the rules of implication it codifies were seen as reflecting `natural' forms of human reasoning. It must be admitted that no one spontaneously and instinctively reasons in a way that conforms to the rules of natural deduction. But there is one place where these forms of inference are widespread – mathematical reasoning. And it will not surprise you to learn that these systems of inference were introduced initially to codify good practice in mathematical proofs. Don't worry, though: we won't expect that you are already a fluent mathematician. Though some of the rules might be a bit stilted and formal for everyday use, the rationale for each of them is transparent and can be easily understood even by those without extensive mathematical training.

One further thing about the rules we shall give is that they are extremely \emph{simple}. At every stage in a proof, it is trivial to see which rules apply, how to apply them, and what the result of applying them is. While constructing proofs as a whole might take some thought, the individual steps are the kind of thing that can be undertaken by a completely automated process. The development of formal proofs in the early years of the twentieth century emphasised this feature, as a part of a general quest to remove the need for `insight' or `intuition' in mathematical reasoning. As the philosopher and mathematician Alfred North Whitehead expressed his conception of the field, `the ultimate goal of mathematics is to eliminate any need for intelligent thought'. You will see I hope that natural deduction does require some intelligent thought. But you will also see that, because the steps in a proof are trivial and demonstrably correct, that finding a formal proof for a claim is a royal road to mathematical knowledge. 

If we have a correct natural deduction proof of an argument, we can often do more than simply report that the argument is valid. Often the natural deduction proof mirrors the intuitive line of reasoning that justifies the valid argument, or even leads to its discovery. Because of the prevalence of natural deduction in introductory logic texts like this one, many philosophers have internalised the rules of natural deduction in their own thought. So a good knowledge of natural deduction can be helpful in interpreting contemporary philosophy: oftentimes the prose presentation of an argument more or less exactly corresponds to some natural deduction proof in an appropriate symbolisation.


\keyideas{
\item Any formal proof is a sequence of sentences, each of which follows by some relatively simple rules from previous sentences or is licensed in some other way. The final sentence is the conclusion of the proof.
	\item The rules are unambiguous and apply only because of the syntactic structure of the sentences involved: no consideration of meanings need be involved. This makes them ideal for computers to use. 
	\item A formal proof system can be theoretically useful, as it might give insight into \emph{why} an argument is valid by showing \emph{how} the conclusion can be derived from the premises.
	\item It might also be practically useful, because it can be much faster to produce a single proof demonstrating an entailment than it would be to show that \emph{all} valuations or interpretations making the premises true also make the conclusion true.
	\item A natural deduction proof system aims to use natural and obviously correct rules, which can contribute to the project of establishing the conclusions of proofs as certain knowledge, and can help in understanding the informal writing of those with a knowledge of logic.
}

\practiceproblems
\problempart Is the purely syntactic nature of formal proof a virtue or a vice? Can we be sure that any class of `good' arguments that is identified on purely syntactic grounds corresponds to an interesting category?

\problempart Are formal proofs always more efficient than truth table arguments? Does reasoning about \TFL\ sentences using valuations never give understanding?

\chapter{The Idea of Natural Deduction}\label{c:ass}

\section{Assumptions and their Consequences}

The fundamental idea behind natural deduction is that formal proofs begin from \define{assumptions}, and the rules for constructing a formal proof either involve introducing a new assumption or apply to previously generated sentences to produce further claims, which are then themselves the subject of the proof rules. The rules apply to a sentence because of its main connective only (so they are indifferent to what other connectives occur within a sentence). For each main connective there are two proof rules:
\begin{itemize}
 	\item an \define{elimination} rule, that applies to a sentence having that connective as the main connective, and allows us to add some further sentence(s) to our proof; and
 	\item an \define{introduction} rule, that applies to some sentence(s) in our proof, and allows us to add some further sentence having that connective as the main connective to our proof. 
 \end{itemize} Some of these rules also have a further effect of removing a previous assumption, or \emph{discharging} it. A natural deduction proof is just a sequence of sentences constructed by making assumptions or using these introduction and elimination rules:
 \factoidbox{Any sequence of sentences, where every sentence is either (i) an assumption, whether discharged or undischarged, or (ii) follows from earlier sentences by the natural deduction proof rules, is a formal natural deduction proof.}
 Henceforth, I shall simply call these `proofs', but you should be aware that there are \emph{informal proofs} too.\footnote{Many of the arguments we offer in our metalanguage, quasi-mathematical arguments \emph{about} our formal languages, are proofs. Sometimes people call formal proofs `deductions' or `derivations', to ensure that no one will confuse the metalanguage activity of proving things about our logical languages with the activity of constructing arguments within those languages. But it seems unlikely that anyone in this course will be confused on this point, since we are not offering very many proofs in the metalanguage in the first place!}

Below (§\ref{fitch}), I will introduce a system for representing natural deduction proofs that will make clear which sentences are assumptions, when those assumptions are made and discharged, and also provide a commentary explaining how the proof was constructed, i.e., which rules and sentences are used to justify others. The commentary isn't strictly necessary for a correct formal proof, but it is essential in learning how those proofs work. 

\section{Assumptions and Suppositions}

Let's look as these initial assumptions. In ordinary reasoning, an assumption is a claim we might be accepting without having a full justification for it. We might believe it, or it might be a supposition that we are making `for the sake of argument'. In natural deduction, which isn't really about belief, assumptions are understood in this second, suppositional, sense. A natural deduction proof begins with a supposed assumption: the rules then tell us what we can derive from this assumption. We can make additional assumptions whenever we like in the course of the proof.

Suppose we have a natural deduction proof. But what is it a proof \emph{of}? Well, the last sentence in the sequence is in some sense where we've ended up:  the result of the proof. It can be considered the conclusion of the proof. But even if there is a proof with conclusion \meta{C}, that doesn't mean that we have proved \meta{C} and should therefore come to believe it. For we have neglected the role of assumptions. Some of the proof rules discharge previously made assumptions, but not every assumption has to be discharged in a correctly formed proof. These remaining `active' or \define{undischarged} assumptions are the suppositions on which the correctness of the proof conclusion depends. So a natural deduction proof is \emph{conditional}: it establishes the conclusion, conditional on the truth of any undischarged assumptions. 

This structure establishes a nice relationship between proofs and arguments. A given proof is a \define{proof of an argument} $\meta{A}_{1} … \meta{A}_{n} \ttherefore \meta{C}$ if the final sentence in the proof is \meta{C}, and each of the undischarged assumptions is among the premises of the argument $\meta{A}_{i}$. Note that if there is a proof of $\meta{A} \ttherefore \meta{C}$, that will also be a proof of $\meta{A},\meta{B} \ttherefore \meta{C}$, and any other argument with the conclusion \meta{C} and including \meta{A} as a premise. This is related to the fact that if you have a valid argument, adding extra premises can't make the argument invalid. (Likewise, if you have a correctly constructed proof, making additional assumptions can't make it incorrect.) 

Looking above, you can see that a single assumption is already a natural deduction proof. If we assume `$P$', and stop there, we have a correctly formed proof of the argument $P \ttherefore P$. The premise is the undischarged assumption; the conclusion is the last sentence in the proof. It doesn't matter that, in this case, the undischarged assumption \emph{is} the last sentence! Since this argument is obviously valid, though trivial, we have some assurance that our simplest proofs are correct, and don't generate fallacious proofs of invalid arguments. Likewise, this proof would also be a proof of the argument $P, Q \therefore P$.

We could make more assumptions. If we had the sequence of sentences `$P$' followed by `$Q$', they would both be undischarged assumptions, and the conclusion would be `$Q$'. So this would be a proof of the argument $P, Q \ttherefore Q$.

Admittedly there isn't much we can do if the only rule we have is the one that allows us to make an assumption whenever we want. We shall want some other rules. But first I'll introduce a way of depicting natural deduction proofs that makes the role of assumptions very clear.

\section{Representing Natural Deduction Proofs}\label{fitch}


We will use a particular graphical representation of natural deduction proofs, one which makes use of `nesting' of sentences to vividly represent which assumptions a particular sentence in a proof is relying on at any given stage, and uses a device of horizontal marks to distinguish assumptions from derived sentences. It will be easier to see how this works with some examples.

A natural deduction proof is a sequence of sentences. We will write this sequence vertically, so each successive sentences occupies its own line. We mark a sentence as an assumption by underlining it. So let's consider a very simple proof, the one-line proof from the assumption `$P$': \begin{proof}
	\hypo{p} P
\end{proof}
In this proof, the horizontal line marks that the sentence above it is an assumption, and not justified by earlier sentences in the proof. Everything written below the line will either be something which follows (directly or indirectly) from the assumptions we have already made, or it will be some new assumption.  We don't need a special indication for the conclusion: it's just the last line. 

There is also a vertical line at the left, the \define{assumption line}. This indicates the \define{range} of the assumption. This vertical line should be continued downwards whenever we extend the proof by applying the natural deduction rules, unless and until a rule that discharges the assumption is applied. Then we discontinue the vertical line. We'll have to wait until §\ref{s:BasicTFLs} to see real examples of rules that discharge assumptions.\footnote{We'll also see there that sometimes we can discharge \emph{all} the assumptions in a proof, and we will have an assumption line with no horizontal assumption marker, and so no undischarged assumptions attached to it. So an assumption line really marks the range of \emph{zero or more} assumptions.} 

When a sentence is in the range of an assumption, that generally means the assumption will be playing some role in justifying that sentence. This isn't always the case, however, and we will see that not every sentence in the range of an assumption intuitively \emph{depends} on the assumption. Not every undischarged assumption is essential to the derivation of a given sentence. \phantomsection\label{nondependence} This again mirrors a feature of valid arguments: the truth of the conclusion of a a valid argument doesn't always require the truth of every premise (e.g., $A, B \vDash A$, but `$B$' isn't really playing an essential role here). 

Whenever we make a new assumption, we underline it and introduce a new assumption line. So this is a proof of the argument $P, Q \ttherefore Q$: \begin{proof}
	\hypo{p} P
	\open
	\hypo{q} Q
\end{proof}
Here you see we've extended the assumption line adjacent to `$P$', and introduced a new assumption line for `$Q$'. 

You see that we also number the sentences in the proof. These numbers are not strictly part of the proof, but are part of the commentary, and help us remember which sentences we are referring to when we explain how subsequent sentences added to the proof are justified.

We now have enough to describe our first natural deduction proof rule. It is the rule that says we can extend any proof by making a new assumption. In abstract generality, here is our \define{new assumption} rule\label{newass}: if we have any natural deduction proof, we can extend it by making a new assumption from any sentence. Graphically, we add the new sentence at the bottom of the proof, indenting it in the range of a new assumption line, and extending the range of all existing undischarged assumptions.  Abstractly, the rule looks like this:
\factoidbox{
\begin{proof}
	\have[\ ]{}{\vdots}
	\have[n]{}{\meta{A}}
	\open
	\hypo[n+1]{e}{\meta{B}}
	\have[\ ]{}{\vdots}
\end{proof}}
That is, whenever we have a proof, whatever its contents, we may make an arbitrary assumption to extend that proof. We will discuss new assumptions, and the special family of rules that handles discharging assumptions, in §\ref{s.subproof}.

Introducing a new assumption line for each new assumption is best practice. That allows each assumption potentially to be discharged independently of all the others. But sometimes you know that you won't be discharging some assumptions, and that they will all remain active throughout the proof. (Every sentence in the proof will be in the range of those assumptions.) In that case, you can use a single assumption line for a number of assumptions. For example, if we know we're going to make and retain the assumptions `$P$' and `$Q$', we can write them on successive lines, draw a single assumption line, and a single horizontal line marking that any sentences above it are the assumptions attached to that assumption line: \begin{proof}
	\hypo{p} P
	\hypo{q} Q
	\have[ ]{v} \vdots
\end{proof}

For any given sentence in a proof, you can easily see the undischarged assumptions on which that sentence depends: just look at which assumptions are attached to the assumptions lines to its left. If some line in a proof is in the range of an assumption, we will say that it is an \define{active assumption} at that point in the proof. Likewise, for a given assumption you can use the assumption line to easily see which sentences are in the range of that assumption.

Let's consider a couple more examples of how to set up up a proof.	$$\enot (A \eor B) \ttherefore \enot A \eand \enot B.$$
We start a proof by writing an assumption:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)}
\end{proof} We are hoping to conclude that `$\enot A \eand \enot B$'; so we are hoping ultimately to conclude our proof with
\begin{proof}
\hypo{a1}{\enot (A \eor B)}
\have[ ]{c}{\vdots}
	\have[n]{con}{\enot A \eand \enot B}
\end{proof}
for some number $n$. It doesn't matter which line we end on, but we would obviously prefer a short proof to a long one! We don't have any rules yet, so we cannot fill in the middle of this proof.

Suppose we had an argument with more than one premise:
$$A\eor B, \enot (A\eand C), \enot (B \eand \enot D) \ttherefore \enot C\eor D.$$
If our argument has more than one premise, we can use either single or multiple assumption lines: \begin{multicols}{2}\noindent
\begin{proof}
	\hypo{a1}{A \eor B}
	\open\hypo{a2}{\enot (A\eand C)}
	\open\hypo{a3}{\enot (B \eand \enot D)}
\have[ ]{m}{\vdots}
	\have[n]{con}{\enot C \eor D}
\end{proof}

\begin{proof}
	\hypo{a1}{A \eor B}
	\hypo{a2}{\enot (A\eand C)}
	\hypo{a3}{\enot (B \eand \enot D)}
\have[ ]{m}{\vdots}
	\have[n]{con}{\enot C \eor D}
\end{proof}
\end{multicols}
Again, these represent the same proof; the right hand form is a conventional shorthand for the official form on the left.


What remains to do is to explain each of the rules that we can use along the way from premises to conclusion. The rules are divided into two families: those rules that involve making or getting rid of further assumptions that are made `for the sake of argument', and those that do not. The latter class of rules are simpler, so we will begin with those in §\ref{s:BasicTFLns}, and turning to the others in §\ref{s:BasicTFLs}. After introducing the rules, I will return in §\ref{c:putting} to the two incomplete proofs above, to see how they may be completed.

\keyideas{
	\item A formal natural deduction proof is a graphical representation of argument from assumptions, in accordance with a strict set of rules for deriving further claims and keeping track of which assumptions are active (`undischarged') at a given point in the proof.
	\item A correctly formed natural deduction proof can be extended by making an arbitrary new assumption at any point.
	}

\practiceproblems
\problempart
If the following a correctly formed proof in our natural deduction system?
\begin{proof}
	\hypo{a}{A}
	\hypo{c}{((B \eiff) \eor A)}
	\open
	\hypo{b}{\enot A}
	\open
	\hypo{d}{D \eif \enot D}
\end{proof}

\problempart
Which of the following could, given the right rules, be turned into a proof corresponding to the argument $$\enot(P \eand Q), P \ttherefore \enot Q?$$
\begin{multicols}{2}\noindent
	   \begin{enumerate}
    	\item\begin{proof}
    		\hypo{a}{\enot(P \eand Q)}
    		\open
    		\hypo{b}{P}
    		\have[\ ]{}{\vdots}
    		\have[n]{c}{\enot Q}
    	\end{proof}
    	\item\begin{proof}
    		\hypo{a}{\enot(P \eand Q)}
    		\open
    		\hypo{b}{P}
    		\have[\ ]{}{\vdots}
    		\close
    		\have[n]{c}{\enot Q}
    	\end{proof}
    	\item\begin{proof}
    		\hypo{a}{\enot Q}
    		\have[\ ]{}{\vdots}
    		\have[n]{b}{P}
    		\have{c}{\enot(P \eand Q)}
    	\end{proof}
    	\item\begin{proof}
    		\hypo{a}{P}
    		\open
    		\hypo{b}{\enot(P \eand Q)}
    		\have[\ ]{}{\vdots}
    		\have[n]{c}{\enot Q}
    	\end{proof}
    \end{enumerate}
\end{multicols}
 

\chapter{Basic Rules for \textnormal{\TFL}: Rules without Subproofs}\label{s:BasicTFLns}


\section{Conjunction Introduction}\label{conjint}
Suppose I want to show that Ludwig is reactionary \emph{and} libertarian. One obvious way to do this would be as follows: first I show that Ludwig is reactionary; then I show that Ludwig is libertarian; then I put these two demonstrations together, to obtain the conjunction.

Our natural deduction system will capture this thought straightforwardly. In the example given, I might adopt the following symbolisation key to represent the argument in \TFL:
	\begin{ekey}
		\item[R] Ludwig is reactionary
		\item[L] Ludwig is libertarian
	\end{ekey}
Perhaps I am working through a proof, and I have obtained `$R$' on line 8 and `$L$' on line 15. Then on any subsequent line I can obtain `$(R \eand L)$' thus:
\begin{proof}
	\have[8]{a}{R}
	\have[ ]{v}{\vdots}
	\have[15]{b}{L}
	\have[16]{c}{(R \eand L)} \ai{a, b}
\end{proof}
Note that every line of our proof must either be an assumption, or must be justified by some rule. We add the commentary `$\eand$I 8, 15' here to indicate that the line is obtained by the rule of conjunction introduction ($\eand$I) applied to lines 8 and 15. Note the derived conjunction depends on the collective assumptions of the two conjuncts.

Since the order of conjuncts does not matter in a conjunction, I could equally well have obtained `$(L \eand R)$' as `$(R \eand L)$'. I can use the same rule with the commentary reversed, to reflect the reversed order of the conjuncts:
\begin{proof}
	\have[8]{a}{R}
	\have[ ]{v}{\vdots}
	\have[15]{b}{L}
	\have[16]{c}{(L \eand R)} \ai{b, a}
\end{proof}
 More generally, here is our \define{conjunction introduction} rule: if we have obtained \meta{A} and \meta{B} by some stage in a proof under some shared assumptions – whether by proof or assumption – that justifies us in introducing their conjunction, which inherits those same assumptions.  Abstractly, the rule looks like this:
\factoidbox{
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[n]{c}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[\ ]{e}{(\meta{A}\eand\meta{B})} \ai{a, c}
\end{proof}}
To be clear, the statement of the rule is \emph{schematic}. It is not itself a proof. `$\meta{A}$' and `$\meta{B}$' are not sentences of \TFL. Rather, they are symbols in the metalanguage, which we use when we want to talk about any sentence of \TFL\ (see §\ref{s:UseMention}). Similarly, `$m$' and `$n$' are not a numerals that will appear on any actual proof. Rather, they are symbols in the metalanguage, which we use when we want to talk about any line number of any proof. In an actual proof, the lines are numbered `$1$', `$2$', `$3$', and so forth. But when we define the rule, we use variables to emphasise that the rule may be applied at any point. The rule requires only that we have both conjuncts available to us somewhere in the proof, earlier than the line that results from the application of the rule. They can be separated from one another, and they can appear in any order. So $m$ might be less than $n$, or greater than $n$. Indeed, $m$ might even equal $n$, as in this proof:
\begin{proof}
	\hypo[1]{a}{P}
	\have[2]{b}{P\eand P} \ai{a, a}
\end{proof}

Note that the rule involves extending the vertical line to cover the newly introduced sentence. This is because what has been derived depends on the same assumptions as what it was derived from, and so it must also be in the range of those assumptions. \factoidbox{All of the rules in this section justify a new claim which inherits all the assumptions of anything from which it has been derived by a natural deduction rule.} The two starting conjuncts needn't have the same assumptions, but the derived conjunction inherits their joint assumptions: 
\begin{proof}
	\hypo{a}{P}
	\open
	\hypo{q}{Q}
	\have{b}{(P\eand Q)} \ai{a, q}
\end{proof}

\section{Conjunction Elimination}\label{conjelim}

The above rule is called `conjunction \emph{introduction}' because it introduces a sentence with `$\eand$' as its main connective into our proof, prior to which it may have been absent. Correspondingly, we also have a rule that \emph{eliminates} a conjunction. Not that the earlier conjunction is somehow removed! It's just that we use a sentence whose main connective is a conjunction to justify further sentences in which that conjunction does not feature.

Suppose you have shown that Ludwig is both reactionary and libertarian. You are entitled to conclude that Ludwig is reactionary. Equally, you are entitled to conclude that Ludwig is libertarian. Putting these observations together, we obtain our \define{conjunction elimination} rules:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eand\meta{B})}
	\have[\ ]{}{\vdots}
	\have[\ ]{a}{\meta{A}} \ae{ab}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eand\meta{B})}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{B}} \ae{ab}
\end{proof}
\end{minipage}
}
The point is simply that, when you have a conjunction on some line of a proof, you can obtain either of the conjuncts by {\eand}E later on. There are two rules, because each conjunction justifies us in deriving either of its conjuncts. We could have called them {\eand}E-\textsc{left} and {\eand}E-\textsc{right}, to distinguish them, but in the following we will mostly not distinguish them.\footnote{Why do we have two rules at all, rather than one rule that allows us to derive either conjunct? The answer is that we want our rules to have an unambiguous result when applied to some prior lines of the proof. This is important if, for example, we are implementing a computer system to produce formal proofs.}


 One point might be worth emphasising: you can only apply this rule when conjunction is the main connective. So you cannot derive `$D$' just from `$C \eor (D \eand E)$'! Nor can you derive `$D$' directly from `$C \eand (D \eand E)$', because it is not one of the conjuncts of the main connective of this sentence. You would have to first obtain `$(D \eand E)$' by {\eand}E, and then obtain `$D$' by a second application of that rule, as in this proof: \begin{proof}
 	\hypo{a}{C \eand (D \eand E)} 
 	\have{b}{D \eand E}\ae{a}
 	\have{c}{D}\ae{b}
 \end{proof}

Even with just these two rules, we can start to see some of the power of our formal proof system. Consider this tricky-looking argument:
\begin{earg}
\item[] $\bigl( (A\eor B)\eif(C\eor D) \bigr) \eand \bigl( (E \eor F) \eif (G\eor H) \bigr)$
\item[\ttherefore] $\bigl( (E \eor F) \eif (G\eor H) \bigr) \eand \bigl( (A\eor B)\eif(C\eor D) \bigr)$
\end{earg} Dealing with this argument using truth-tables would be a very tedious exercise, given that there are 8 sentence letters in the premise and we would thus require a $2^{8}=256$ line truth table! But we can deal with it swiftly using our natural deduction rules.

The main connective in both the premise and conclusion of this argument is `$\eand$'. In order to provide a proof, we begin by writing down the premise, which is our assumption. We draw a line below this: everything after this line must follow from our assumptions by (successive applications of) our rules of implication. So the beginning of the proof looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}}
\end{proof}
From the premise, we can get each of its conjuncts by {\eand}E. The proof now looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}}
	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
\end{proof}
So by applying the {\eand}I rule to lines 3 and 2 (in that order), we arrive at the desired conclusion. The finished proof looks like this:
\begin{proof}
	\hypo{ab}{{[}(A\eor B)\eif(C\eor D){]} \eand {[}(E \eor F) \eif (G\eor H){]}}

	\have{a}{{[}(A\eor B)\eif(C\eor D){]}} \ae{ab}
	\have{b}{{[}(E \eor F) \eif (G\eor H){]}} \ae{ab}
	\have{ba}{{[}(E \eor F) \eif (G\eor H){]} \eand {[}(A\eor B)\eif(C\eor D){]}} \ai{b,a}
\end{proof}
This is a very simple proof, but it shows how we can chain rules of proof together into longer proofs. Our formal proof requires just four lines, a far cry from the 256 lines that would have been required had we approached the argument using the techniques from chapter \ref{ch.TruthTables}.

It is worth giving another example. Way back in §\ref{s:MoreParentheticalConventions}, we noted that this argument is valid:
	$$A \eand (B \eand C) \ttherefore (A \eand B) \eand C.$$
To provide a proof corresponding to this argument, we start by writing:
\begin{proof}
	\hypo{ab}{A \eand (B \eand C)}
\end{proof}
From the premise, we can get each of the conjuncts by applying $\eand$E twice. And we can then apply $\eand$E twice more, so our proof looks like:
\begin{proof}
	\hypo{ab}{A \eand (B \eand C)}
	\have{a}{A} \ae{ab}
	\have{bc}{B \eand C} \ae{ab}
	\have{b}{B} \ae{bc}
	\have{c}{C} \ae{bc}
\end{proof}
But now we can merrily reintroduce conjunctions in the order we want them, so that our final proof is:
\begin{proof}
	\hypo{abc}{A \eand (B \eand C)}
	\have{a}{A} \ae{abc}
	\have{bc}{B \eand C} \ae{abc}
	\have{b}{B} \ae{bc}
	\have{c}{C} \ae{bc}
	\have{ab}{A \eand B}\ai{a, b}
	\have{con}{(A \eand B) \eand C}\ai{ab, c}
\end{proof}
Recall that our official definition of sentences in \TFL\ only allowed conjunctions with two conjuncts. When we discussed semantics, we became a bit more relaxed, and allowed ourselves to drop inner parentheses in long conjunctions, since the order of the parentheses did not affect the truth table. The proof just given suggests that we could also drop inner parentheses in all of our proofs. However, this is not standard, and we shall not do this. Instead, we shall return to the more austere parenthetical conventions. (Though we will allow ourselves to drop outermost parentheses most of the time, for legibility.)

Our conjunction rules correspond to intuitively correct patterns of implication. But they are also demonstrably good in another sense. Each of our rules can be vindicated by considering facts about entailment. Each of these schematic entailments is easily demonstrated: \begin{itemize}
	\item $\meta{A}, \meta{B} \entails \meta{A} \eand \meta{B}$;
	\item 	$\meta{A} \eand \meta{B} \entails \meta{A}$;
	\item $\meta{A} \eand \meta{B} \entails \meta{B}$.
\end{itemize} For example, the first of these says that \meta{A} and \meta{B} separately suffice to entail the truth of their conjunction. This justifies the proof rule of conjunction introduction, since at a stage in the proof where we are assuming both \meta{A} and \meta{B} to be true, we are then permitted to conclude that $\meta{A}\eand\meta{B}$ is true – just as conjunction introduction says we can.

It can be recognised, then, that our proof rules correspond to valid arguments in \TFL, and so our conjunction rules will never permit us to derive a false sentence from true sentences. There is no guarantee of course that the assumptions we make in our formal proofs are in fact true – only that if they were true, what we derive from them would also be true. So despite the fact that our proof rules are a \emph{syntactic} procedure, that rely only on recognising the main connective of a sentence and applying an appropriate rule to introduce or eliminate it, each of our rules corresponds to an acceptable entailment.


\section{Conditional Elimination}\label{condelim}
Consider the following argument:
	\begin{quote}
		If Jane is smart then she is fast. Jane is smart. So Jane is fast.
	\end{quote}
This argument is certainly valid. If you have a conditional claim, that commits you to the consequent given the antecedent, and you also have the antecedent, then you have sufficient material to derive the consequent. 

This suggests a straightforward \define{conditional elimination} rule ({\eif}E):
\factoidbox{
\begin{proof}
	\have[m]{ab}{(\meta{A}\eif\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{B}} \ce{ab,a}
\end{proof}}
This rule is also sometimes called \idefine{modus ponens}. Again, this is an elimination rule, because it allows us to obtain a sentence that may not contain `$\eif$', having started with a sentence that did contain `$\eif$'. Note that the conditional, and the antecedent, can be separated from one another, and they can appear in any order. However, in the commentary for $\eif$E, we always cite the conditional first, followed by the antecedent.

Here is an illustration of the rules we have so far in action, applied to this intuitively correct argument: $$P, \bigl((P \eif Q)\eand(P \eif R)\bigr) \ttherefore (R \eand Q).$$ \begin{proof}
	\hypo{b}{\bigl((P \eif Q)\eand(P \eif R)\bigr)}
	\open
	\hypo{a}{P}
	\have{c}{(P \eif Q)}\ae{b}
	\have{d}{(P \eif R)}\ae{b}
	\have{e}{Q}\ce{c,a}
	\have{f}{R}\ce{d,a}
	\have{g}{(R \eand Q)}\ai{f,e}
\end{proof}\phantomsection\label{proof.within}

The correctness of our proof rule of conditional elimination is supported by the easily demonstrated validity of the corresponding entailment: \begin{itemize}
	\item  $\meta{A}, \meta{A}\eif\meta{B}\entails \meta{B}$.
\end{itemize} So applying this rule can never produce false conclusions if we began with true assumptions.

\section{Biconditional Elimination}\label{bielim}

The \define{biconditional elimination} rule ({\eiff}E) lets you do a much the same as the conditional rule. Unofficially, a biconditional is like two conditionals running in each direction. So, thought of informally, our biconditional rules correspond to the left-to-right conditional elimination rule, the other of which corresponds to a right-to-left application of conditional elimination. 

If we know that Alice is coming to the party iff Bob is, then if we knew that either of them was coming, we'd know that the other was coming. If you have the left-hand subsentence of the biconditional, you can obtain the right-hand subsentence. If you have the right-hand subsentence, you can obtain the left-hand subsentence. So we have these two instances of the rule:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eiff\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{B}} \be{ab,a}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A}\eiff\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{a}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[\ ]{b}{\meta{A}} \be{ab,a}
\end{proof}
\end{minipage}
}
Note that the biconditional, and the right or left half, can be distant from one another in the proof, and they can appear in any order. However, in the commentary for $\eiff$E, we always cite the biconditional first. 

Here is an example of the biconditional rules in action, demonstrating the following argument: $$P, (P\eiff Q), (Q \eif R) \ttherefore R.$$ \begin{proof}
	\hypo{pq}{(P\eiff Q)}
	\open \hypo{p}{P}
	\open \hypo{qr}{(Q \eif R)}
	\have{q}{Q}\be{pq,p}
	\have{r}{R}\ce{qr,q}
\end{proof}

Note the way that our conjunction and conditional elimination rules can be used to parallel the biconditional elimination rules:

\begin{tabular}{p{0.45\textwidth}p{0.45\textwidth}} \begin{proof}
	\hypo{a}{((P \eif Q)\eand (Q \eif P))}
	\hypo{p}{Q}
	\have{pq}{(Q \eif P)}\ae{a}
	\have{q}{P}\ce{pq,p}
\end{proof} & \begin{proof}
	\hypo{a}{(P \eiff Q)}
	\hypo{p}{Q}
	\have{q}{P}\be{a,p}
\end{proof}\end{tabular}



The correctness of our proof rules of biconditional elimination is supported by the easily demonstrated validity of the corresponding entailments:
\begin{itemize}
	\item $\meta{A}\eiff\meta{B},\meta{A}\vDash\meta{B}$;
	\item  $\meta{A}\eiff\meta{B},\meta{B}\vDash\meta{A}$.
\end{itemize}


\section{Disjunction Introduction}\label{disjint}

Suppose Ludwig is reactionary. Then Ludwig is either reactionary or libertarian. After all, to say that Ludwig is either reactionary or libertarian is to say something weaker than to say that Ludwig is reactionary. (\meta{A} is weaker than \meta{B} if \meta{A} follows from \meta{B}, but not \emph{vice versa}.) 

Let me emphasise this point. Suppose Ludwig is reactionary. It follows that Ludwig is \emph{either} reactionary \emph{or} a kumquat. Equally, it follows that \emph{either} Ludwig is reactionary \emph{or} that kumquats are the only fruit.  Equally, it follows that \emph{either} Ludwig is reactionary \emph{or} that God is dead. Many of these things would be strange \emph{inferences} to draw. Since the truth of the assumption does guarantee that the disjunction is true, there is nothing \emph{logically} wrong with the implications. This can be so even if drawing these implications may violate all sorts of implicit conversational norms, or that inferring in accordance with logic in this manner would be more likely a sign of psychosis than rationality.

Armed with all this, I present the \define{disjunction introduction} rule(s):
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{ab}{(\meta{A}\eor\meta{B})}\oi{a}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{ba}{(\meta{B}\eor\meta{A})}\oi{a}
\end{proof}
\end{minipage}}
Notice that \meta{B} can be \emph{any} sentence of \TFL\ whatsoever. So the following is a perfectly good proof:
\begin{proof}
	\hypo{m}{M}
	\have{mmm}{M \eor \Bigl( \bigl( (A\eiff B) \eif (C \eand D)\bigr)  \eiff \bigl( E \eand F\bigr) \Bigr)}\oi{m}
\end{proof}
Using a truth table to show this would have taken 128 lines. 

Here is an example, to show our rules in action: \begin{proof}
	\hypo{ab}{(A \eand B)}
	\have{a}{A}\ae{ab}
	\have{b}{B}\ae{ab}
	\have{ac}{(A \eor C)}\oi{a}
	\have{bc}{(B \eor C)}\oi{b}
	\have{abc}{((A \eor C)\eand(B\eor C))}\ai{ac,bc}
\end{proof}

This disjunction rule is supported by the following valid \TFL\ argument forms: \begin{itemize}
	\item $\meta{A}\vDash\meta{A}\eor\meta{B}$; and 
	\item $\meta{B}\vDash\meta{A}\eor\meta{B}$.
\end{itemize}


The rule of disjunction introduction is one place where `natural' deduction doesn't seem to live up to its name. Is this implication really one we would naturally make?

Despite appearances, maybe we do sometimes reason like this. Consider this argument: `I won't ever eat meat again. Well, either I won't, or it will be an accident!' (But perhaps this is better thought of as a retraction of my initial over-bold claim, rather than an inference from it.)

Nevertheless, even if the rule is artificial, that doesn't make it incorrect. We can see, clearly, that it corresponds to a valid argument. So perhaps the problem with it as a piece of reasoning is due to something other than invalidity. \begin{itemize}
    	\item Sometimes disjunction introduction looks like you are `throwing away' information that you already have – you know enough to treat `$P$' as a premise, but you end up assenting only to the weaker claim `$P \eor Q$'. But can this be the full story? Conjunction elimination seems to involve the same sort of inference from a logically stronger to a logically weaker claim, and that doesn't arouse nearly as much animosity as disjunction introduction. 
	\item An alternative explanation: maybe the introduced disjunct seems \emph{irrelevant}, because the content of the sentence to which the rule is applied has in general nothing to do with the disjunct introduced. This is not the case with conjunction elimination, where the result of applying the rule is clearly related to the sentence it is applied to.
    		\end{itemize} 
These considerations of relevance or information value lie beyond logic proper. They concern what we, as thinkers and hearers, can conclude about the speaker's state of mind, given that they have said something with a particular content. This is the domain of that part of linguistics known as \define{pragmatics}, the study of meaning in context. Most theories of pragmatics predict that disjunction introduction is valid but often \emph{conversationally inappropriate}. So Paul Grice, for example, says that if you are in a position to contribute the information of a claim $\meta{P}$ to a conversation, and you are a \emph{cooperative} speaker, then you will not contribute the weaker information `\meta{P} or \meta{Q}', even though you should still regard it as true.\footnote{Grice's discussion of disjunction is at pp. 44–6 in H P Grice (1989) \emph{Studies in the Way of Words}, Harvard University Press; see also §4 of Maria Aloni (2016) `Disjunction' in Edward N Zalta, ed., \emph{The Stanford Encyclopedia of Philosophy} \httpurl{plato.stanford.edu/entries/disjunction/\#DisjConv}.}

\section{Reiteration}\label{reit}

The last natural deduction rule in this category is \define{reiteration} (R). This just allows us to repeat an assumption or claim \meta{A} we have already established, so long as the repeated sentence remains in the range of any assumption which the original was in the range of.
\factoidbox{\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[\ ]{c}{\vdots}
	\have[\ ]{b}{\meta{A}} \by{R}{a}
\end{proof}}
Such a rule is obviously legitimate; but one might well wonder how such a rule could ever be useful. Here is an example of it in action: \begin{proof}
	\hypo{p}{P}
	\hypo{ppq}{((P \eand P) \eif Q)}
	\have{p1}{P}\by{R}{p}
	\have{pp}{(P \eand P)}\ai{p,p1}
	\have{q}{Q}\ce{ppq,pp}
\end{proof}
This rule is unnecessary at this point in the proof (we could have applied conjunction introduction and cited line 1 twice in our commentary), but it can be easier in practice to have two distinct lines to which to apply conjunction introduction. The real benefits of reiteration come when we have multiple subproofs, as we will see in the following section (§\ref{s.subproof}) – particularly when it comes to the negation rules. But we will also see later in §\ref{der.reit} that, strictly speaking, we don't need the reiteration rule – though for convenience we will keep it. And once we are able to discharge assumptions, reiteration carries some risks (§\ref{pitfalls}).

\keyideas{
	\item Natural deduction gives us rules that tell us how to infer \emph{from} a sentence with a certain main connective (elimination rules), and rules that tell us how to infer \emph{to} a sentence with a certain main connective (introduction rules).
	\item Proofs using the rules so far retain all the assumptions made during the course of the proof, and so correspond to an argument with those assumptions as premises and the final line of the proof as a conclusion.
	\item Reiteration is an optional rule, but useful for housekeeping in proofs.
}

\practiceproblems
\problempart
The following `proof' is \emph{incorrect}. Explain the mistakes it makes.

\begin{proof}
\hypo{abc}{A \eand (B \eand C)}
\hypo{bcd}{(B \eor C) \eif D}
\have{b}{B}\ae{abc}
\have{bc}{B \eor C}\oi{b}
\have{d}{D}\ce{bc, bcd}
\end{proof}

\problempart
Is the following purported proof correct?

\begin{proof}
	\hypo{a}{A}
	\open
	\have{b}{B}
	\have{a2}{A}\by{R}{a}
\end{proof}

\problempart
The following proof is missing its commentary. Please supply the correct annotations on each line that needs one: 

\begin{proof}
\hypo{ps}{P \eand S}
\have{p}{P}%\ae{ps}
\have{s}{S}%\ae{ps}
\open
\hypo{nsor}{S \eif R}
\have{r}{R}%\ce{nsor, s}
\have{re}{R \eor E}%\oi{r}
\end{proof}


\problempart
Give natural deduction proofs for the following arguments: \begin{earg}
	\item $P \ttherefore ((P \eor Q) \eand P)$;
	\item $((P \eand Q) \eand (R \eand P)) \ttherefore ((R \eand Q) \eand P)$;
	\item $(A \eif (A \eif B)), A \ttherefore B$;
	\item $(B \eiff (A \eiff B)), B \ttherefore A$.
\end{earg}



\chapter{Basic Rules for \textnormal{\TFL}: Rules with Subproofs}\label{s:BasicTFLs}

We've already seen in §\ref{c:ass} how to start a proof by making assumptions. But the true power of natural deduction relies on its rules governing when you can make additional assumptions during the course of the proof, and how you can \emph{discharge} those assumptions when you no longer need them. 


\section{Additional Assumptions and Subproofs}\label{s.subproof}

In natural deduction, both making and discharging additional assumptions are handled using \define{subproofs}. These are subsidiary proofs within the main proof, which encapsulate that part of a larger proof that depends on an assumption that is not among the premises. (Conversely, we can think of the premises of an argument as those assumptions left undischarged at the conclusion of a proof.)

When we start a subproof, we draw another vertical line (to the right of any existing assumption lines) to indicate that we are no longer in the main proof. Then we write in the assumption upon which the subproof will be based. A subproof can be thought of as essentially posing this question: \emph{what could we show, if we also make this additional assumption?} We've already seen this in action earlier (§\ref{c:ass}), when we said that we could indicate the range of several premises in an argument by either attaching them all to one vertical assumption line, or introducing a new vertical line for each new assumption. In that case, we never got rid of the new assumptions: they remained as premises. 

What will be new in this section is that some rules take us \emph{back out} of a subproof. So the rules we will now consider are quite different from the rules covered in §\ref{s:BasicTFLns}, none of which have this feature of being able to escape from a previously introduced assumption.

When we are working within a subproof, we can refer to the additional assumption that we made in introducing the subproof, and to anything that we obtained from our original assumptions. (After all, those original assumptions are still in effect.) But at some point, we shall want to stop working with the additional assumption: we shall want to return from the subproof to the main proof. To indicate that we have returned to the main proof, the vertical line for the subproof comes to an end. At this point, we say that the subproof is \define{closed}. Having closed a subproof, we have set aside the additional assumption, so it will be illegitimate to draw upon anything that depends upon that additional assumption. This has been implicit in our discussion all along, but it is good to make it very clear:
\factoidbox{\phantomsection\label{subproof.rule}Any point in a natural deduction proof is in the range of some (zero or more) currently active assumptions, and the natural deduction rules can be applied to extend the proof from that point only by appealing to prior sentences which rely at most on those same assumptions (or perhaps fewer).

Equivalently, any rule can be applied to any earlier lines in a proof, \emph{except} for those lines which occur within a closed subproof. }
Closing a subproof is called \define{discharging} the assumptions of that subproof. So we can put the point this way: \emph{at no stage of a proof can you apply a rule to a sentence that occurs only in the range of an already discharged assumption}. 

Subproofs, then, allow us to think about what we could show, if we made additional assumptions. The point to take away from this is not surprising – in the course of a proof, we have to keep very careful track of what assumptions we are making, at any given moment. Our proof system does this very graphically, with those vertical assumption lines that indicate the range of an assumption. (Indeed, that's precisely why we have chosen to use \emph{this} proof system.) 

When you discharge an assumption, closing a subproof, you generally introduce some further new sentence. That sentence can be thought of as a summary of the subproof, in the context of the other undischarged assumptions. This is particularly evident if you think about the conditional introduction rule we are about to introduced.

When can we begin a new subproof? \emph{Whenever we want.} That is the upshot of the New Assumption rule from §\ref{newass}. At any stage in a proof it is legitimate to assume something new, as long as we begin keeping track of what in our proof rests on this new assumption. We don't need any reason to justify making an assumption, but that doesn't mean it's a good idea to introduce them  
haphazardly. Some guidelines to help decide when it might be particularly appropriate to make a new assumption are discussed in §\ref{c:proof.strat}.

The idea of opening subproofs by making new assumptions can be used to illustrate a remark I made above about when a claim depends on assumptions (p.\ \pageref{nondependence}). Consider this proof: \begin{proof}
	\hypo{pq}{(P \eand Q)}
	\have{q}{Q}\ae{pq}
	\open \hypo{r}{R}
	\have{q2}{Q}\by{R}{q}
\end{proof} In this proof, even though the occurrence of `$Q$' on line 4 occurs within the range of the assumption `$R$', it does not intuitively depend on it. We used reiteration to show that `$Q$' is still derivable from active assumptions at line 4, but it does not follow that `$Q$' depends in any robust way on every assumption that is active at line 4. 

\section{Conditional Introduction}\label{condint}

To illustrate the use of subproofs, we will begin with the rule of conditional introduction. It is fairly easy to motivate informally.\phantomsection\label{ci.motivate} The following argument in English should be valid:
	\begin{quote}
		Ludwig is reactionary. Therefore if Ludwig is libertarian, then Ludwig is both reactionary \emph{and} libertarian.
	\end{quote}
If someone doubted that this was valid, we might try to convince them otherwise by explaining ourselves as follows:
	\begin{quote}
		Assume that Ludwig is reactionary. Now, \emph{additionally} assume that Ludwig is libertarian. Then by conjunction introduction, it follows that Ludwig is both reactionary and libertarian. Of course, that only follows \emph{conditional} on the assumption that Ludwig is libertarian. But this just means that, if Ludwig is libertarian, then he is both reactionary and libertarian – at least, that follows given our initial assumption that he is reactionary.
	\end{quote}

This kind of reasoning is vital for understanding conditional claims. As the Cambridge philosopher Frank Ramsey pointed out: \begin{quote}
	If two people are arguing ‘If \meta{P}, will \meta{Q}?’ and are both in doubt as to \meta{P}, they are adding \meta{P} hypothetically to their stock of knowledge and arguing on that basis about \meta{Q}….\footnote{F P Ramsey (1929), ‘General Propositions and Causality’, at p. 155 in F P Ramsey (1990) \emph{Philosophical Papers}, D H Mellor, ed., Cambridge University Press.}
\end{quote}
Ramsey’s idea is that if we can reach the conclusion that \meta{C} on the basis of the hypothetical supposition that \meta{A} (generally together with some other assumptions) then we would be entitled to judge, given the other assumptions alone, that if \meta{A} turns out to be true, then \meta{C} will also turn out to be true – for short, that if \meta{A} then \meta{C}. This observation of Ramsey's – that conditionals embody the categorical content of hypothetical reasoning – has been important for many accounts of the English conditional, not all of them wholly congenial to the idea that `if' is to be understood as `$\eif$'. Yet the essence of his idea motivates the conditional introduction rule of natural deduction.

Transferred into natural deduction format, here is the pattern of reasoning that we just used. We started with one premise, `Ludwig is reactionary', symbolised `$R$'. Thus:
	\begin{proof}
		\hypo{r}{R}
	\end{proof}
The next thing we did is to make an \emph{additional} assumption (`Ludwig is libertarian'), for the sake of argument. To indicate that we are no longer dealing \emph{merely} with our original assumption (`$R$'), but with some additional assumption, we continue our proof as follows:
	\begin{proof}
		\hypo{r}{R}
		\open
			\hypo{l}{L}
	\end{proof}
We are \emph{not} claiming, on line 2, to have proved `$L$' from line 1. We are just making another assumption. So we do not need to write in any justification for the additional assumption on line 2. We do, however, need to mark that it is an additional assumption. We do this in the usual way, by drawing a line under it (to indicate that it is an assumption) and by indenting it with a further assumption line (to indicate that it is additional). 

With this extra assumption in place, we are in a position to use {\eand}I. So we could continue our proof:
	\begin{proof}
		\hypo{r}{R}
		\open
			\hypo{l}{L}
			\have{rl}{R \eand L}\ai{r, l}
%			\close
%		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{proof} The two vertical lines to the left of line 3 show that `$R\eand L$' is in the range of both assumptions, and indeed depends on them collectively.

So we have now shown that, on the additional assumption, `$L$', we can obtain `$R \eand L$'. We can therefore conclude that, if `$L$' obtains, then so does `$R \eand L$'. Or, to put it more briefly, we can conclude `$L \eif (R \eand L)$':
	\begin{proof}
		\hypo{r}{R}
		\open
			\hypo{l}{L}
			\have{rl}{R \eand L}\ai{r, l}
			\close
		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{proof}
Observe that we have dropped back to using one vertical line.  We are no longer relying on the additional assumption, `$L$', since the conditional itself follows just from our original assumption, `$R$'. The use of conditional introduction has discharged the temporary assumption, so that the final line of this proof relies only on the initial assumption `$R$' – we made use of the assumption `$L$' only in the nested subproof, and the range of that assumption is restricted to sentences in that subproof. Note that the conditional sentence `$L \eif (R \eand L)$' is a summary of what went on in the subproof, given the undischarged assumption `$R$': \emph{if you made the additional assumption $L$, then you could derive `$(R \eand L)$'}.


The general pattern at work here is the following. We first make an additional assumption, A; and from that additional assumption, we prove B. In that case, we have established the following: If is does in fact turn out that A, then it also turns out that B. This is wrapped up in the rule for \define{conditional introduction}:
\factoidbox{
	\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}} 
			\have[\ ]{} \vdots
			\have[j]{b}{\meta{B}}
		\close
		\have[\ ]{}{\vdots}
		\have[\ ]{ab}{(\meta{A}\eif\meta{B})}\ci{a-b}
	\end{proof}}
There can be as many or as few lines as you like between lines $i$ and $j$. Notice that in our presentation of the rule, discharging the assumption \meta{A} takes us out of the subproof in which \meta{B} is derived from \meta{A}. If \meta{A} is the initial assumption of a proof, then discharging it may well leave us with a conditional claim that depends on \emph{no undischarged assumptions at all}. We see an example in this proof, where the main proof, marked by the leftmost vertical line, features no horizontal line marking an assumption:
\begin{proof}
	\open \hypo{a}{P \eand P}
	\have{b}{P}\ae{a}
	\close
	\have{}{(P\eand P) \eif P}\ci{a-b}
\end{proof} It might come as no surprise that the conclusion of this proof – being provable from no undischarged assumptions at all – turns out to be a logical truth. 


It will help to offer a further  illustration of {\eif}I in action. Suppose we want to consider the following:
	$$P \eif Q, Q \eif R \ttherefore P \eif R.$$
We start by listing \emph{both} of our premises. Then, since we want to arrive at a conditional (namely, `$P \eif R$'), we additionally assume the antecedent to that conditional. Thus our main proof starts:
\begin{proof}
	\hypo{pq}{P \eif Q}
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
	\close
\end{proof}
Note that we have made `$P$' available, by treating it as an additional assumption. But now, we can use {\eif}E on the first premise. This will yield `$Q$'. And we can then use {\eif}E on the second premise. So, by assuming `$P$' we were able to prove `$R$', so we apply the {\eif}I rule – discharging `$P$' – and finish the proof. Putting all this together, we have:
\phantomsection\label{HSproof}
\begin{proof}
	\hypo{pq}{P \eif Q}
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof}

Let's consider another example, this one demonstrating why reiteration can be so useful in subproofs. We know that $P \ttherefore Q \eif P$ is a valid argument, from truth-tables. This is a proof: \begin{proof}
	\hypo{a}{P}
	\open
	\hypo{q}{Q}
	\have{p}{P}\by{R}{a}
	\close
	\have{pq}{(Q \eif P)}\ci{q-p}
\end{proof} Note that strictly speaking we needn't have used reiteration here: the assumption of `$P$' remains active at line 2, so technically we could apply {\eif}I to close the subproof and introduce the conditional immediately after line 2. But the use of reiteration makes it much clearer what is going on in the proof – even though all it does it repeat the earlier assumption and remind us that it is still an active assumption. 


We now have all the rules we need to show that the argument on page \pageref{longtt} is valid. Here is the six line proof,  some 175,000 times shorter  than the corresponding truth table:\phantomsection\label{ndshort}

\hspace{-2.25cm}\begin{minipage}{\textwidth}\small\begin{proof}
	\hypo{a1c1}{A_{1} \eif C_{1}}
	\open
		\hypo{ai}{(A_1 \eand A_2 \eand A_3 \eand A_4 \eand A_5 \eand A_6 \eand A_7 \eand A_8 \eand A_9 \eand A_{10})}
		\have{a1}{A_{1}}\ae{ai}
		\have{c1}{C_{1}}\ce{a1c1,a1}
		\have{ci}{(C_1 \eor C_2 \eor C_3 \eor C_4 \eor C_5 \eor C_6 \eor C_7 \eor C_8 \eor C_9 \eor C_{10})}\oi{c1}
		\close
	\have{c}{(A_1 \eand A_2 \eand A_3 \eand A_4 \eand A_5 \eand A_6 \eand A_7 \eand A_8 \eand A_9 \eand A_{10})\eif (C_1 \eor C_2 \eor C_3 \eor C_4 \eor C_5 \eor C_6 \eor C_7 \eor C_8 \eor C_9 \eor C_{10})}\ci{ai-ci}
\end{proof}\end{minipage}


\subsection{Import-Export}\label{import.export}
Our rules so far can be used to demonstrate two important principles governing the conditional. The principle of \define{importation} is the claim that from `if $P$ then, if also $Q$, then $R$' it follows that `if $P$ and also $Q$, then $R$'. The principle of \define{exportation} is the converse, that from `if $P$ and also $Q$, then $R$' it follows that `if $P$, then if also $Q$, then $R$'. First, we prove importation holds for our conditional: \begin{proof}
	\hypo{ipiqr}{(P \eif (Q \eif R))}
	\open
	\hypo{paq}{(P \eand Q)}
	\have{p}{P}\ae{paq}
	\have{qr}{(Q \eif R)}\ce{ipiqr,p}
	\have{q}{Q}\ae{paq}
	\have{r}{R}\ce{qr,q}
	\close
	\have{c}{((P\eand Q)\eif R)}\ci{paq-r}
\end{proof}
Second, we show exportation holds. Here, we need to open two nested subproofs:
\begin{proof}
	\hypo{paqr}{((P\eand Q)\eif R)}
	\open
	\hypo{p}{P}
	\open
	\hypo{q}{Q}
	\have{pq}{(P \eand Q)}\ai{p,q}
	\have{r}{R}\ce{paqr,pq}
	\close
	\have{qr}{(Q \eif R)}\ci{q-r}
	\close
	\have{pqr}{(P \eif (Q \eif R))}\ci{p-qr}
\end{proof}

The principles of importation and exportation hold of the material conditional. But exportation, in particular, is quite controversial when it comes to English `if' (see §\ref{s:philosophy}).

\section{Some Pitfalls of Subproofs}\label{pitfalls}


Making additional assumptions in the range of an assumption needs to be handled with some care, as I said in §\ref{s.subproof}. Now that we have a rule that discharges assumptions in our repertoire, we can describe some of the potential pitfalls.

Consider this proof:
\begin{proof}
	\hypo{a}{A}
	\open
		\hypo{b1}{B}
		\have{bb}{B \eand B}\ai{b1, b1}
		\have{b2}{B} \ae{bb}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
\end{proof}
This is perfectly in keeping with the rules we have laid down already. And it should not seem particularly strange. Since `$B \eif B$' is a logical truth, no particular premises should be required to prove it – note that `$A$' plays no particular role in the proof apart from beginning it. 

But suppose we now tried to continue the proof as follows:
\begin{proof}
	\hypo{a}{A}
	\open
		\hypo{b1}{B}
		\have{bb}{B \eand B}\ai{b1, b1}
		\have{b2}{B} \ae{bb}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
	\have{b}{B}\by{naughty attempt to invoke $\eif$E}{con, b2}
\end{proof}
If we were allowed to do this, it would be a disaster. It would allow us to prove any atomic sentence letter from any other atomic sentence letter. But if you tell me that Anne is fast (symbolised by `$A$'), I shouldn't be able to conclude that Queen Boudica stood twenty-feet tall (symbolised by `$B$')! So we must be prohibited from doing this. The rule on page \pageref{subproof.rule} stipulation rules out the disastrous attempted proof above. The rule of $\eif$E requires that we cite two individual lines from earlier in the proof. In the purported proof, above, one of these lines (namely, line 4) occurs within a subproof that has (by line 6) been closed. This is illegitimate. 

A similar problem arises if we forget the restrictions on the rule of reiteration. Recall §\ref{reit} that we can reiterate an earlier sentence only if the same assumptions remain undischarged. If we forget this, we can construct illegal `proofs' such as the following:
\begin{proof}
	\open
	\hypo{a}{P}
	\open
	\hypo{b}{Q}
	\have{c}{P \eand Q} \ai{a,b}
	\close
	\have{d}{Q \eif (P\eand Q)} \ci{b-c}
	\have{e}{P \eand Q} \by{R}{c}
	\close
	\have{f}{P \eif (P \eand Q)}\ci{a-e}
\end{proof} This is certainly not a logical truth. What's gone wrong is that we reiterated `$P\eand Q$' without retaining the assumptions on which it was dependent. Naturally enough, it was dependent on both `$P$' and `$Q$', but it was reiterated into a context where the assumption `$Q$' had been discharged. This is illegitimate.

\section{Subproofs within Subproofs}

Once we have started thinking about what we can show by making additional assumptions, nothing stops us from posing the question of what we could show if we were to make \emph{even more} assumptions? This might motivate us to introduce a subproof within a subproof. Here is an example which only uses the rules of proof that we have considered so far:
\begin{proof}
\hypo{a}{A}
\open
	\hypo{b}{B}
	\open
		\hypo{c}{C}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif (C \eif (A \eand B))}\ci{b-cab}
\end{proof}
Notice that the commentary on line 4 refers back to the initial assumption (on line 1) and an assumption of a subproof (on line 2). This is perfectly in order, since neither assumption has been discharged at the time (i.e., by line 4).

Again, though, we need to keep careful track of what we are assuming at any given moment. For suppose we tried to continue the proof as follows:
\begin{proof}
\hypo{a}{A}
\open
	\hypo{b}{B}
	\open
		\hypo{c}{C}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif(C \eif (A \eand B))}\ci{b-cab}
\have{bcab}{C \eif (A \eand B)}\by{naughty attempt to invoke $\eif$I}{c-ab}
\end{proof}
This would be awful. If I tell you that Anne is smart, you should not be able to derive that, if Cath is smart (symbolised by `$C$') then \emph{both} Anne is smart and Queen Boudica stood 20-feet tall! But this is just what such a proof would suggest, if it were permissible.

The essential problem is that the subproof that began with the assumption `$C$' depended crucially on the fact that we had assumed `$B$' on line 2. By line 6, we have \emph{discharged} the assumption `$B$': we have stopped asking ourselves what we could show, if we also assumed `$B$'. So it is simply cheating, to try to help ourselves (on line 7) to the subproof that began with the assumption `$C$'. The attempted disastrous proof violates, as before, the rule in the box on page \pageref{subproof.rule}. The subproof of lines 3–4 occurs within a subproof that ends on line 5. Its assumptions are discharged before line 7, so they cannot be invoked in any rule which applies to produce line 7.

It is always permissible to open a subproof with any assumption. However, there is some strategy involved in picking a useful assumption. Starting a subproof with an arbitrary, wacky assumption would just waste lines of the proof. In order to obtain a conditional by {\eif}I, for instance, you must assume the antecedent of the conditional in a subproof. 

Equally, it is always permissible to close a subproof and discharge its assumptions. However, it will not be helpful to do so, until you have reached something useful.

Recall the proof of the argument $$P \eif Q, Q \eif R \ttherefore P \eif R$$ from page \pageref{HSproof}. One thing to note about the proof there is that because there are two assumptions with the same range in the main proof, it is not easily possible to discharge just one of them using the {\eif}I rule. For that rule only applies to a one-assumption subproof. If we wanted to discharge another of our assumptions, we shall have to put the proof into the right form, with each assumption made individually as the head of its own subproof:
\begin{proof}
	\hypo{pq}{P \eif Q}
	\open
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof} The conclusion is now in the range of both assumptions, as in the earlier proof – but now it is also possible to discharge these assumptions if we wish: \begin{proof}
	\open \hypo{pq}{P \eif Q}
	\open
	\hypo{qr}{Q \eif R}
	\open
		\hypo{p}{P}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
	\close
	\have{qrpr}{(Q\eif R)\eif(P\eif R)}\ci{qr-pr}
	\close
	\have{}{(P\eif Q)\eif((Q\eif R)\eif(P\eif R))}\ci{pq-qrpr}
\end{proof} 
While it is permissible, and often convenient, to have several assumptions with the same range and without nesting, I recommend always trying to construct your proofs so that each assumption begins its own subproof. That way, if you later wish to apply rules which discharge a single assumption, you may always do so.

\section{Proofs within Proofs}

One interesting feature of a natural deduction system like ours is that because we can make any assumption at any point, and thereafter continue in accordance with the rules, any correctly formed proof can be \emph{re-used} as a subproof in a later proof. For example, suppose we wanted to give a proof of this argument:  $$\bigl((P \eif Q)\eand(P \eif R)\bigr) \ttherefore (P \eif (R \eand Q)).$$ We begin by opening our proof by assuming the premise. We also note that the conclusion is a conditional, and so we'll assume that it is obtained by an instance of conditional introduction. That will give us this `skeleton' of a proof before we begin filling in the details:
 \begin{proof}
	\hypo{b}{\bigl((P \eif Q)\eand(P \eif R)\bigr)}
	\open
	\hypo{a}{P}
	\have[\ ]{c}{\vdots}
	\have[m]{g}{(R \eand Q)}\ai{f,e}
	\close
	\have{h}{(P \eif (R \eand Q))}\ci{a-g}
\end{proof}
Then we recall – perhaps! – that we already have a proof that looks very much like this. On page \pageref{proof.within} we have a proof that uses the same premise as ours, but also uses the premise `$P$' to derive `$(R \eand Q)$' – which is what we need. So we can simply copy that whole proof over to fill in the missing section of our proof:
 \begin{proof}
	\hypo{b}{\bigl((P \eif Q)\eand(P \eif R)\bigr)}
	\open
	\hypo{a}{P}
	\have{c}{(P \eif Q)}\ae{b}
	\have{d}{(P \eif R)}\ae{b}
	\have{e}{Q}\ce{c,a}
	\have{f}{R}\ce{d,a}
	\have{g}{(R \eand Q)}\ai{f,e}
	\close
	\have{h}{(P \eif (R \eand Q))}\ci{a-g}
\end{proof}
This is a very useful feature: for if you have proved something once, you can re-use that proof whenever you need to, as a subproof in some other proof. 

The converse isn't always true, because sometimes in a subproof you use an assumption from outside the subproof, and if you don't make the same assumption in your other proof, the displaced subproof may no longer correctly follow all the rules it uses. 



\section{Biconditional Introduction} \label{biint} 
The biconditional is like a two-way conditional. The introduction rule for the biconditional resembles two instances of conditional introduction, one for each direction.

In order to prove `$W \eiff X$', for instance, you must be able to prove `$X$' on the assumption `$W$' \emph{and} prove `$W$' on the assumption `$X$'. The \define{biconditional introduction} rule ({\eiff}I) therefore requires two subproofs to license the introduction. Schematically, the rule works like this:
\factoidbox{
\begin{proof}
	\open
		\hypo[i]{a1}{\meta{A}}
		\have[\ ]{}{\vdots}
		\have[j]{b1}{\meta{B}}
	\close
	\open
		\hypo[k]{b2}{\meta{B}}
		\have[\ ]{}{\vdots}
		\have[l]{a2}{\meta{A}}
	\close
	\have[\ ]{}{\vdots}
	\have[\ ]{ab}{(\meta{A}\eiff\meta{B})}\bi{a1-b1,b2-a2}
\end{proof}}
There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first. Again, this rule permits us to discharge assumptions, and the same restrictions on making use of claims derived in a closed subproof outside of that subproof apply.

We can now prove that a biconditional is like two conjoined conditionals. Using the conditional and biconditional rules, we can prove that a biconditional entails a conjunction of conditionals, and \emph{vice versa}: 

\begin{multicols}{2}\noindent
	 \begin{proof}
	\open \hypo{pq}{(A \eiff B)}
	\open \hypo{p}{A}
	\have{b}{B}\be{pq,p}
	\close
	\have{ab}{(A \eif B)}\ci{p-b}
	\open
	\hypo{b1}{B}
	\have{a}{A}\be{pq,b1}
	\close
	\have{ba}{(B\eif A)}\ci{b1-a}
	\have{aba}{\bigl((A \eif B) \eand (B \eif A)\bigr)}\ai{ab,ba}
\end{proof}
	\begin{proof}
		\hypo{aba}{\bigl((A \eif B) \eand (B \eif A)\bigr)}
		\have{ab}{(A \eif B)}\ae{aba}
		\open \hypo{a}{A}
		\have{b}{B}\ce{ab,a}
		\close
		\have{ba}{(B \eif A)}\ae{aba}
		\open \hypo{b1}{B}
		\have{a1}{A}\ce{ba,b1}
		\close
		\have{pq}{(A \eiff B)}\bi{a-b,b1-a1}
	\end{proof}
\end{multicols}

Another informative example demonstrates the logical equivalence of `$((P \eand Q) \eif R)$' and `$(P \eif (Q \eif R))$' given importation and exportation (page \pageref{import.export}). We will re-use both of our earlier proofs, stitching them together using biconditional introduction in the final line: \begin{proof}
\open	\hypo{ipiqr}{(P \eif (Q \eif R))}
	\open
	\hypo{paq}{(P \eand Q)}
	\have{p1}{P}\ae{paq}
	\have{qr1}{(Q \eif R)}\ce{ipiqr,p1}
	\have{q1}{Q}\ae{paq}
	\have{r1}{R}\ce{qr1,q1}
	\close
	\have{c}{((P\eand Q)\eif R)}\ci{paq-r1}
\close
\open
	\hypo{paqr}{((P\eand Q)\eif R)}
	\open
	\hypo{p}{P}
	\open
	\hypo{q}{Q}
	\have{pq}{(P \eand Q)}\ai{p,q}
	\have{r}{R}\ce{paqr,pq}
	\close
	\have{qr}{(Q \eif R)}\ci{q-r}
	\close
	\have{pqr}{(P \eif (Q \eif R))}\ci{p-qr}
	\close
	\have{bic}{\bigl((P\eand Q)\eif R)\eiff (P \eif (Q \eif R))\bigr)}\bi{paqr-pqr,ipiqr-c}
\end{proof}
Note the small gap between the nested vertical lines between lines 7 and 8 – that shows we have two subproofs here, not one. (That would also be indicated by the fact that the sentence on line 8 has a horizontal line under it – no vertical assumption line has two markers of where the assumptions cease.)



The acceptability of our proof rules is grounded in the fact that they will never lead us from truth to falsehood. The acceptability of the biconditional introduction rule is demonstrated by the following correct entailment: \begin{itemize}
	\item If $\meta{C}_{1},…,\meta{C}_{n},\meta{A}\vDash\meta{B}$ and $\meta{C}_{1},…,\meta{C}_{n},\meta{B}\vDash\meta{A}$, then $\meta{C}_{1},…,\meta{C}_{n}\vDash\meta{A}\eiff\meta{B}$.
\end{itemize}



\section{Disjunction Elimination}\label{disjelim}


The disjunction elimination rule is slightly trickier than those we've seen so far. Suppose that either Ludwig is reactionary or he is libertarian. What can you conclude? Not that Ludwig is reactionary; it might be that he is libertarian instead. And equally, not that Ludwig is libertarian; for he might merely be reactionary. It can be hard to draw a definite conclusion from a disjunction just by itself.

But suppose that we could somehow show both of the following: first, that Ludwig's being reactionary entails that he is an Austrian economist: second, that Ludwig's being libertarian \emph{also} entails that he is an Austrian economist. Then if we know that Ludwig is either reactionary or libertarian, then we know that, whichever he is, Ludwig is an Austrian economist. This we might call `no matter whether' reasoning: if each of \meta{A} and \meta{B} imply \meta{C}, then \emph{no matter whether} \meta{A} \emph{or} \meta{B}, still \meta{C}. Sometimes this kind of reasoning is called \emph{proof by cases}, since you start with the assumption that either of two cases holds, and then show something follows no matter which case is actual. 


This insight can be expressed in the following rule, which is our \define{disjunction elimination} ($\eor$E) rule:
\factoidbox{
	\begin{proof}
		\have[m]{ab}{(\meta{A}\eor\meta{B})}
		\have[\ ]{}{\vdots}
		\open
			\hypo[i]{a}{\meta{A}} {}
			\have[\ ]{}{\vdots}
			\have[j]{c1}{\meta{C}}
		\close
		\have[\ ]{}{\vdots}
		\open
			\hypo[k]{b}{\meta{B}}{}
			\have[\ ]{}{\vdots}
			\have[l]{c2}{\meta{C}}
		\close
		\have[\ ]{}{\vdots}
		\have[ ]{c}{\meta{C}}\oe{ab, a-c1,b-c2}
	\end{proof}}
This is obviously a bit clunkier to write down than our previous rules, but the point is fairly simple. Suppose we have some disjunction, $\meta{A} \eor \meta{B}$. Suppose we have two subproofs, showing us that $\meta{C}$ follows from the assumption that $\meta{A}$, and that $\meta{C}$ follows from the assumption that $\meta{B}$. Then we can derive $\meta{C}$ itself. As usual, there can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs and the disjunction can come in any order, and do not have to be adjacent.


Some examples might help illustrate the rule in action. Consider this argument:
$$(P \eand Q) \eor (P \eand R) \ttherefore P.$$
An example proof might run thus:
	\begin{proof}
		\hypo{prem}{(P \eand Q) \eor (P \eand R) }
			\open
				\hypo{pq}{P \eand Q}
				\have{p1}{P}\ae{pq}
			\close
			\open
				\hypo{pr}{P \eand R}
				\have{p2}{P}\ae{pr}
			\close
		\have{con}{P}\oe{prem, pq-p1, pr-p2}
	\end{proof}
An adaptation of the previous proof can be used to establish a proof for this argument: $$(P \eand Q) \eor (P \eand R) \ttherefore (P \eand(Q \eor R)).$$
We begin the cases in the same way as above, but as we continue please note the use of the disjunction introduction rule to get the last line of each subproof in the right format to use disjunction elimination. 
	\begin{proof}
		\hypo{prem}{(P \eand Q) \eor (P \eand R) }
			\open
				\hypo{pq}{P \eand Q}
				\have{p1}{P}\ae{pq}
				\have{q}{Q}\ae{pq}
				\have{qr}{(Q \eor R)}\oi{q}
				\have{ca}{P \eand (Q \eor R)}\ai{p1,qr}
			\close
			\open
				\hypo{pr}{P \eand R}
				\have{p2}{P}\ae{pr}
				\have{r}{R}\ae{pr}
				\have{rq}{(Q \eor R)}\oi{r}
				\have{cb}{P \eand (Q \eor R)}\ai{p2,rq}
			\close
		\have{con}{P \eand (Q \eor R)}\oe{prem, pq-ca, pr-cb}
	\end{proof}

Don't be alarmed if you think that you wouldn't have been able to come up with this proof yourself. The ability to come up with novel proofs will come with practice. The key question at this stage is whether, looking at the proof, you can see that it conforms with the rules that we have laid down. And that just involves checking every line, and making sure that it is justified in accordance with the rules we have laid down.

Another slightly tricky example. Consider:
	$$ A \eand (B \eor C) \ttherefore (A \eand B) \eor (A \eand C).$$
Here is a proof corresponding to this argument:
	\begin{proof}
		\hypo{aboc}{A \eand (B \eor C)}
		\have{a}{A}\ae{aboc}
		\have{boc}{B \eor C}\ae{aboc}
		\open
			\hypo{b}{B}
			\have{ab}{A \eand B}\ai{a,b}
			\have{abo}{(A \eand B) \eor (A \eand C)}\oi{ab}
		\close
		\open
			\hypo{c}{C}
			\have{ac}{A \eand C}\ai{a,c}
			\have{aco}{(A \eand B) \eor (A \eand C)}\oi{ac}
		\close
	\have{con}{(A \eand B) \eor (A \eand C)}\oe{boc, b-abo, c-aco}
	\end{proof}


This disjunction rule is supported by the following valid \TFL\ argument form: \begin{itemize}
	\item If $\meta{D}_{1},…,\meta{D}_{n},\meta{A}\eor\meta{B},\meta{A}\vDash\meta{C}$ and $\meta{D}_{1},…,\meta{D}_{n},\meta{A}\eor\meta{B},\meta{B}\vDash\meta{C}$, then $\meta{D}_{1},…,\meta{D}_{n},\meta{A}\eor\meta{B}\vDash\meta{C}$.
\end{itemize}


\section{Negation Introduction}\label{negint}

Our negation rules are inspired by the form of reasoning known as \emph{reductio} (recall page \pageref{reductio}). In \emph{reductio} reasoning, we make an assumption that \meta{A} for the sake of argument, and show that something contradictory follows from it. Then we can conclude that our assumption was false, and that its negation must be true. There are lots of approaches to negation in natural deduction, but all of them stem from this same basic insight: when an assumption that \meta{A} goes awry, conclude $\enot \meta{A}$.

We see \emph{reductio} reasoning used a lot in mathematics. For example: Suppose there is a largest number, call it $n$. Since $n$ is the largest number, $n+1 \leqslant n$. But then, subtracting $n$ from both sides, $1 \leqslant 0$. And that is absurd. So there is no largest number. Here, we make an assumption, for the sake of argument. We derive from it a claim, in this case that $1 \leqslant 0$. And we note that claim is absurd, \emph{given what we already know}. So we conclude that the negation of our assumption holds, and cease to rely on the problematic assumption.

The only claims in logic that it is safe to say are absurd are logical falsehoods. So in a logical version of \emph{reductio} reasoning, we will want to show that claims that contradict one another will be derivable in the range of an assumption, in order to prove the negation of that assumption. Our \define{negation introduction} rule fits this pattern very clearly: \factoidbox{
	\begin{proof}
	\open
	\hypo[i]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have[\ ]{}{\enot\meta{A}}\ni{a-b,a-c}	
	\end{proof}
} Here, we can prove a sentence and its negation both within the range of the assumption that \meta{A}. So if \meta{A} were assumed, something contradictory would be derivable under that assumption. (We could apply conjunction introduction to lines $j$ and $k$ to make the logical falsehood explicit, but that wouldn't be strictly necessary.) Since logical falsehoods are fundamentally unacceptable as the termination of a chain of argument, we must have begun with an inappropriate starting point when we assumed \meta{A}. So, in fact, we conclude, $\enot \meta{A}$, discharging our erroneous assumption that \meta{A}. There is no need for the line with \meta{B} on it to occur before the line with $\enot\meta{B}$ on it.

Almost always the logical falsehood arises because of a clash between the claim we assume and some bit of prior knowledge – typically, some claim we have established earlier in the proof. We will thus make frequent use of the rule of reiteration in applications of negation introduction, to get the contradictory claims in the right place to make the rule easy to apply. Here is an example of the rule in action, showing that this argument is provable: $$A, \enot B \ttherefore \enot (A \eif B).$$ \begin{proof}
	\hypo{a}{A}
	\hypo{nb}{\enot B}
	\open
	\hypo{ab}{A \eif B}
	\have{b}{B}\ce{ab,a}
	\have{nbb}{\enot B}\by{R}{nb}
	\close
	\have{e}{\enot(A \eif B)}\ni{ab-b,ab-nbb}
\end{proof}

Another example, for practice. Let's prove this argument: $$(C \eif \enot A) \therefore (A \eif \enot C).$$ \begin{proof}
	\hypo{cna}{(C \eif \enot A)}
	\open
	\hypo{a}{A}
	\open
	\hypo{c}{C}
	\have{na}{\enot A}\ce{cna,c}
	\have{ar}{A}\by{R}{a}
	\close
	\have{nc}{\enot C}\ni{c-ar,c-na}
	\close
	\have{ac}{(A \eif \enot C)}\ci{a-nc}
\end{proof}


The correctness of the negation introduction rule is demonstrated by this valid \TFL\ argument form: \begin{itemize}
	\item If $\meta{C}_{1},…,\meta{C}_{n},\meta{A} \vDash \meta{B}$ and $\meta{C}_{1},…,\meta{C}_{n},\meta{A} \vDash \enot\meta{B}$, then $\meta{C}_{1},…,\meta{C}_{n} \vDash \enot\meta{A}$. \end{itemize}

\section{Negation Elimination}\label{negelim}

The rule of negation introduction is interesting, because it is \emph{almost} its own elimination rule too! Consider this schematic proof: \begin{proof}
	\open 
	\hypo[i]{a}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have{}{\enot\enot\meta{A}}\ni{a-b,a-c}
\end{proof} This proof terminates with a sentence that is logically equivalent to \meta{A}, discharging the assumption that $\enot\meta{A}$ because it leads to contradictory conclusions. This looks awfully close to a rule of negation elimination – if only we could find a way to replace a doubly-negated sentence $\enot\enot\meta{A}$ by the logically equivalent sentence \meta{A}, which would have eliminated the negation from the problematic assumption $\enot\meta{A}$.


In our system, we approach this problem by the brute force method – we allow ourselves to use the derivation of contradictory sentences from a negated sentence to motivate the elimination of that negation. This leads to our rule of \define{negation elimination}:
\factoidbox{
	\begin{proof}
	\open
	\hypo[i]{a}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have[\ ]{}{\meta{A}}\ne{a-b,a-c}	
	\end{proof}
} 

This is also \emph{reductio} reasoning, though in this case from a negated assumption. But again, if the assumption of $\enot \meta{A}$ goes awry and allows us to derive contradictory claims (perhaps given what we've already shown), that licenses us to conclude \meta{A}.

With the rule of negation elimination, we can prove some claims that are hard to prove directly. For example, suppose we wanted to prove an instance of the \define{law of excluded middle},\phantomsection\label{excmidd} that $(\meta{A} \eor \enot \meta{A})$ is true for any sentence $\meta{A}$. Suppose we aim at proving the specific instance `$(P \eor \enot P)$'. (It's easy to see that the proof we give can be adapted to any other instance of the law.) You might initially have thought: \emph{it is a disjunction, so should be proved by disjunction introduction}. But we cannot prove either the sentence letter `$P$' or its negation from no assumptions – so we could not prove excluded middle from no assumptions if it was by disjunction introduction from one of its disjuncts. So we proceed indirectly: we show that supposing the negation of the law of excluded middle leads to logical falsehood, and conclude it by negation elimination: \begin{proof}
	\open \hypo{ne}{\enot(P \eor \enot P)}
	\open
	\hypo{p}{P}
	\have{e}{(P \eor \enot P)}\oi{p}
	\have{ner}{\enot(P \eor \enot P)}\by{R}{ne}
	\close
	\have{np}{\enot P}\ni{p-e,p-ner}
	\have{npo}{(P \eor \enot P)}\oi{np}
	\have{ner2}{\enot(P \eor \enot P)}\by{R}{ne}
	\close
	\have{ec}{(P \eor \enot P)}\ne{ne-npo,ne-ner2}
\end{proof}
One interesting feature of this proof is that one of the contradictory sentences is the assumption itself. When the assumption that \enot\meta{A} goes wrong, it might be because we have the resources to prove \meta{A}! Some interesting philosophical controversy surrounds proofs like this: see §\ref{construct}.

To see our negation rules in action, consider:
	$$P \ttherefore (P \eand D) \eor (P \eand \enot D).$$
Here is a proof corresponding with the argument:
	\begin{proof}
		\hypo{a}{P}
		\open
			\hypo{b}{\enot \bigl((P \eand D) \eor (P \eand \enot D)\bigr)}
			\open
				\hypo{d}{D}
				\have{e}{(P \eand D)}\ai{a,d}
				\have{f}{(P \eand D) \eor (P \eand \enot D)}\oi{e}
			\close
			\have{nd}{\enot D}\ni{d-f,d-b}
			\have{pnd}{(P \eand \enot D)}\ai{a,nd}
			\have{ff}{\bigl((P \eand D) \eor (P \eand \enot D)\bigr)}\oi{pnd}
		\close
		\have{dn}{(P \eand D) \eor (P \eand \enot D)}\ne{b-ff}
	\end{proof} I make two comments. In line 6, the justification cites line 2 which lies outside the subproof. That is okay, since  the application of the rule lies within the range of the assumption of line 2. In line 9, the justification only cites the subproof from 2 to 8, rather than two ranges of line numbers. This is because in this application of our rule, we have the special case where the sentence such that both it and its negation can be derived from the assumption \emph{is} that assumption. It would be trivial to derive it from itself.

The negation elimination rule is supported by this valid \TFL\ argument form: \begin{itemize}
	\item If $\meta{C}_{1},…,\meta{C}_{n},\enot\meta{A} \vDash \meta{B}$ and $\meta{C}_{1},…,\meta{C}_{n},\enot\meta{A} \vDash \enot\meta{B}$, then $\meta{C}_{1},…,\meta{C}_{n} \vDash \meta{A}$;
\end{itemize}

\section{Putting it All Together}\label{c:putting}

\emph{We have now explained all of the basic rules for the proof system for \TFL.} Let's return to some of the arguments from §\ref{c:ass} with which we began our exploration of this system, to see how they can be proved. And I will give a third example of a complex proof that uses many of our rules. 

\begin{figure}
 \begin{proof}
	\hypo{aob}{\enot (A \eor B)}
	\open
	\hypo{a}{A}
	\have{ab}{(A \eor B)}\oi{a}
	\have{r}{\enot(A \eor B)}\by{R}{aob}
	\close
	\have{na}{\enot A}\ni{a-ab,a-r}
	\open
	\hypo{b}{B}
	\have{ab2}{(A \eor B)}\oi{b}
	\have{r2}{\enot(A \eor B)}\by{R}{aob}
	\close
	\have{nb}{\enot B}\ni{b-ab2,b-r2}
	\have{c}{(\enot A \eand \enot B)}\ai{na,nb}
\end{proof} 	\caption{Proof of $\enot (A \eor B) \ttherefore (\enot A \eand \enot B)$\label{figa1}}
\end{figure}

\begin{figure}
	\begin{proof}
	\hypo{a1}{A \eor B}
	\open\hypo{a2}{\enot (A\eand C)}
	\open\hypo{a3}{\enot (B \eand \enot D)}
\open \hypo{a}{A}
\open \hypo{c}{C}
\have{ac}{A \eand C}\ai{a,c}
\have{a2r}{\enot(A \eand C)}\by{R}{a2}
\close
\have{nc}{\enot C}\ni{c-ac,c-a2r}
\have{c1}{(\enot C \eor D)}\oi{nc}
\close
\open \hypo{b}{B}
\open\hypo{nd}{\enot D}
\have{bnd}{(B \eand \enot D)}\ai{b,nd}
\have{a3r}{\enot(B \eand \enot D)}\by{R}{a3}
\close
\have{d}{D}\ne{nd-bnd,nd-a3r}
\have{c2}{(\enot C \eor D)}\oi{d}
\close
	\have{con}{(\enot C \eor D)}\oe{a1,a-c1,b-c2}
\end{proof}
\caption{Proof that $(A\eor B), \enot (A\eand C), \enot (B \eand \enot D) \ttherefore (\enot C\eor D)$.\label{figa2}}
\end{figure}

 \begin{figure}
	\begin{proof}
	\open
	\hypo{npvq}{\enot P \eor Q}
		\open
		\hypo{np1}{\enot P}
			\open
			\hypo{p1}{P}
				\open
				\hypo{nq}{\enot Q}
				\have{c}{P\eand\enot P}\ai{p1,np1}
				\have{d}{P}\ae{c}
				\have{e}{\enot P}\ae{c}
				\close
			\have{q1}{Q}\ne{nq-e}
			\close
		\have{ipq1}{P \eif Q}\ci{p1-q1}
		\close
		\open
		\hypo{q2}{Q}
			\open
			\hypo{p2}{P}
			\have{qq}{Q \eand Q}\ai{q2,q2}
			\have{q3}{Q}\ae{qq}
			\close
		\have{ipq2}{P \eif Q}\ci{p2-q3}
		\close
		\have{ipq3}{P\eif Q}\oe{npvq,np1-ipq1,q2-ipq2}
		\close
		\open
		\hypo{ipq4}{P \eif Q}
			\open
			\hypo{nnpvq}{\enot(\enot P \eor Q)}
				\open
				\hypo{p3}{P}
				\have{q4}{Q}\ce{ipq4,p3}
				\have{npvq2}{\enot P \eor Q}\oi{q4}
				\close
			\have{np}{\enot P}\ni{p3-nnpvq,p3-npvq2}
			\have{npvq3}{\enot P \eor Q}\oi{np}
			\close
		\have{npvq4}{\enot P \eor Q}\ne{nnpvq-nnpvq,nnpvq-npvq3}
		\close
		\have{z}{((\enot P \eor Q)\eiff(P \eif Q))}\bi{npvq-ipq3,ipq4-npvq4}
\end{proof}\caption{A complicated proof\label{fig:vi}}
\end{figure}

\begin{enumerate}
	\item One argument we considered earlier was this: $$\enot (A \eor B) \ttherefore (\enot A \eand \enot B).$$ We can now see that the proof we began to construct can be completed can be proved as in Figure \ref{figa1}.
	\item The second proof we began constructing earlier corresponded to this argument: $$(A\eor B), \enot (A\eand C), \enot (B \eand \enot D) \ttherefore (\enot C\eor D).$$ The proof can be completed as in Figure \ref{figa2} on page \pageref{figa2}.
	\item Finally, Figure \ref{fig:vi} shows a long proof involving most of our rules in action (page \pageref{fig:vi}). 
\end{enumerate}

These three proofs are more complex than the others we've considered, because they involve multiple rules in tandem. You should make sure you understand why each rule applies where it does, and that the proofs are correct, before you move on. You probably won't feel that you are able to construct a proof yourself as yet, and that is okay. It is important now to see that these are in fact proofs. Some ideas about how to go about constructing them yourself will be presented in §\ref{c:proof.strat}. But you will also get a sense about how to construct complex proofs as you practice constructing simpler proofs and start to see how they can be slotted together to form larger proofs. There is no substitute for practice.

\keyideas{
	\item The rules for our system are summarised on page \pageref{ProofRules}.
	\item It is important that we keep track of restrictions on when we can make use of claims derived in a subproof, since those subproofs may be making use of assumptions we are no longer accepting.
	\item Our proof rules match the interpretation of \TFL\ we have given – they will not permit us to say that some claim is provable from some assumptions when that claim isn't entailed by those assumptions. 
}

\clearpage
\practiceproblems


\problempart
The following `proof' is \emph{incorrect}. Explain the mistakes it makes.

\begin{proof}
\hypo{abc}{\enot L \eif (A \eand L)}
\open
	\hypo{nl}{\enot L}
	\have{a}{A}\ce{abc, nl}
\close
\open
	\hypo{l}{L}
	\have{red}{L \eand \enot L}\ai{l, nl}
	\open
		\hypo{lala}{\enot A}
		\have{lal}{L}\ci{red}
		\have{lla}{\enot L}\ce{red}
	\close
	\have{a2}{A}\ni{lala-lal,lala-lla}
\close
\have{con}{A}\oe{nl-a, l-a2}
\end{proof}


\problempart
The following proofs are missing their commentaries (rule and line numbers). Add them, to turn them into bona fide proofs. Additionally, write down the argument that corresponds to each proof.
\begin{multicols}{2}\noindent


\begin{proof}
\hypo{ad}{A \eif D}
\open
	\hypo{ab}{A \eand B}
	\have{a}{A}%\ae{ab}
	\have{d}{D}%\ce{ad, a}
	\have{de}{D \eor E}%\oi{d}
\close
\have{conc}{(A \eand B) \eif (D \eor E)}%\ci{ab-de}
\end{proof}

\begin{proof}
\hypo{nlcjol}{\enot L \eif (J \eor L)}
\open
\hypo{nl}{\enot L}
\have{jol}{J \eor L}%\ce{nlcjol, nl}
\open
	\hypo{j}{J}
	\have{jj}{J \eand J}%\ai{j}
	\have{j2}{J}%\ae{jj}
\close
\open
	\hypo{l}{L}
	\open
		\hypo{nj}{\enot J}
	\close
	\have{j3}{J}%\ne{nj–l,nj-nl}
\close
\have{conc}{J}%\oe{jol, j-j2, l-j3}
\end{proof}
\end{multicols}


\newpage\problempart
\label{pr.solvedTFLproofs}
Give a proof representing each of the following arguments:
\begin{earg}
\item $J\eif\enot J \ttherefore \enot J$
\item $Q\eif(Q\eand\enot Q) \ttherefore \enot Q$
\item $A\eif (B\eif C) \ttherefore (A\eand B)\eif C$
\item $K\eand L \ttherefore K\eiff L$
\item $(C\eand D)\eor E \ttherefore E\eor D$
\item $A\eiff B, B\eiff C \ttherefore A\eiff C$
\item $\enot F\eif G, F\eif H \ttherefore G\eor H$
\item $(Z\eand K) \eor (K\eand M), K \eif D \ttherefore D$
\item $P \eand (Q\eor R), P\eif \enot R \ttherefore Q\eor E$
\item $S\eiff T \ttherefore S\eiff (T\eor S)$
\item $\enot (P \eif Q) \ttherefore \enot Q$
\item $\enot (P \eif Q) \ttherefore P$
\end{earg}

\problempart
For each of the following sentences, construct a natural deduction proof which has the sentence as its last line, and contains no undischarged assumptions: \begin{earg}
	\item $J \eiff (J \eor (L \eand \enot L))$
	\item $((P \eand Q) \eiff (Q \eand P))$
	\item $((P \eif P) \eif Q) \eif Q$.
\end{earg}

\chapter{Some Philosophical Issues about Conditionals, Meaning, and Negation}\label{s:philosophy}


\section{Conditional Introduction and the English Conditional} \label{cond.proof}%Æ added

We motivated the conditional introduction rule back on page \pageref{ci.motivate} by giving a English argument, using the English word `if'. Now, {\eif}I is a stipulated rule for our conditional connective \eif; it doesn't really need motivation since we could simply postulate that such a rule is part of our formal proof system. It is justified, if justification is needed, by the Deduction Theorem (a result noted in §\ref{ded.thm}). (Likewise, $\eif$E may be justified by a schematic truth table demonstration that $\meta{A}, \meta{A}\eif\meta{C} \vDash \meta{C}$.)

But if we are to offer a motivation for our rule in English, then we must be relying on the plausibility of this English analog of {\eif}I, known as \define{conditional proof}: \factoidbox{
	If you can establish $\meta{C}$, given the assumption that $\meta{A}$ and perhaps some supplementary assumptions $\meta{B}_{1},…,\meta{B}_{n}$, then you can establish `if $\meta{A}$, $\meta{C}$' solely on the basis of the assumptions $\meta{B}_{1},…,\meta{B}_{n}$.
} Conditional proof captures a significant aspect of the English `if': the way that  conditional helps us neatly summarise reasoning from assumptions, and then store that reasoning for later use in a conditional form.



But if conditional proof is a good rule for English `if', then we can argue that `if' is actually synonymous with `$\eif$':\ \begin{quote}
Suppose that $\meta{A} \eif \meta{C}$. Assume $\meta{A}$. We can now derive $\meta{C}$, by {\eif}E. But we have now established $\meta{C}$ on the basis of the assumption $\meta{A}$, together with the supplementary assumption $\meta{A}\eif \meta{C}$. By conditional proof, then, we can establish `if $\meta{A}$, $\meta{C}$' on the basis of the supplementary assumption $\meta{A}\eif \meta{C}$ alone. But that of course means that we can derive (in English) an English conditional sentence from a \TFL\ conditional sentence, with the appropriate interpretation of the constituents \meta{A} and \meta{C}. Since the English conditional sentence obviously suffices for the \TFL\ conditional sentence, we have shown them to be synonymous. 
\end{quote}  

Our discussion in §\ref{s:ParadoxesOfMaterialConditional} seemed to indicate that the English conditional was not synonymous with `\eif'. That is hard to reconcile with the above argument. Most philosophers have concluded that, contrary to appearances, conditional proof is not always a good way of reasoning for English `if'. Here is an example which seems to show this, though it requires some background.

It is clearly valid in English, though rather pointless, to argue from a claim to itself. So when \meta{C} is some English sentence, $\meta{C} \ttherefore \meta{C}$ is a valid argument in English. And we cannot make a valid argument invalid by adding more premises:  if premises we already have are conclusive grounds for the conclusion, adding more premises while keeping the conclusive grounds cannot make the argument less conclusive. So $\meta{C},\meta{A} \ttherefore \meta{C}$ is a valid argument in English. 

If conditional proof were a good way of arguing in English, we could convert this valid argument into another valid argument with this form: $\meta{C} \ttherefore \text{if }\meta{A}, \meta{C}$. But then conditional proof would then allow us to convert this valid argument: \begin{earg}
	\item[\ex{ski1}] I will go skiing tomorrow;
	\item[\ex{ski2}] I break my leg tonight;
	\item[So:] I will go skiing tomorrow. \qquad(which is just repeating \ref{ski1} again)
\end{earg} into this intuitively invalid argument:
\begin{earg}
	\item[\ref{ski1}.] I will go skiing tomorrow;
	\item[So:] If I break my leg tonight, I will go skiing tomorrow.
\end{earg} This is invalid, because even if the premise is true, the conclusion seems to be actually false. If conditional proof enables us to convert a valid English argument into an invalid argument, so much the worse for conditional proof.\footnote{Suspicion about conditional proof for `if' might also raise suspicion about the principles of importation and exportation (§\ref{import.export}). Indeed, if exportation is assumed to hold for `if', then `if' is the material conditional. (A version of this argument, originally due to Allan Gibbard, is discussed in §2.5 of Dorothy Edgington (2020) `Indicative Conditionals', in Edward N Zalta, ed., \emph{The Stanford Encyclopedia of Philosophy}, \httpurl{plato.stanford.edu/entries/conditionals/}.)

 Let $\meta{A}$ and $\meta{B}$ be some arbitrary sentences. Consider this complex English conditional (I insert parentheses to clarify the structure): 
\begin{earg}
	\item[\ex{impexpa}] If ([either $\meta{B}$ or not-$\meta{A}$] and $\meta{A}$), then $\meta{B}$. 
\end{earg}
This sentence is, intuitively, true, no matter what $\meta{A}$ and $\meta{B}$ are. If that disjunction of $\meta{B}$ and the falsehood of \meta{A} is true along with \meta{A}, then it must be because the disjunct \meta{B} holds. But apply exportation to \ref{impexpa}, and we obtain:
\begin{earg}
	\item[\ex{impexpb}] If [either $\meta{B}$ or not-$\meta{A}$], then (if $\meta{A}$) then $\meta{B})$. 
\end{earg} If every instance of \ref{impexpa} is true in English, and \ref{impexpb} follows by a valid principle governing the meaning of `if', then every instance of \ref{impexpb} is true in English. But the antecedent of \ref{impexpb} is just the truth conditions of the material conditional `$\eif$'. And we already know that `if $\meta{A}$ then $\meta{B}$' implies in English `Either $\meta{B}$ or not-$\meta{A}$'. So we would get the result that `if $\meta{A}$ then $\meta{B}$' is equivalent in English to the material conditional `either $\meta{B}$ or not-$\meta{A}$'.}

There is much more to be said about this example. What is beyond doubt is that {\eif}I is a good rule for \TFL, regardless of the fortunes of conditional proof in English. It does seem that instances of conditional proof in mathematical reasoning are all acceptable, which again shows the roots of natural deduction as a formalisation of existing mathematical practice. This would suggest that \eif\ might be a good representation of mathematical uses of `if'.


\section{Inferentialism}\label{inferentialism}

You will have noticed that our rules come in pairs: an introduction rule that tells you how to introduce a connective into a proof from what you have already, and an elimination rule that tells you how to remove it in favour of its consequences. These rules can be justified by consideration of the meanings we assigned to the connectives of \TFL\ in the schematic truth tables of §\ref{s:SchematicTruthTables}.

But perhaps we should invert this order of justification. After all, the proof rules already seem to capture (more or less) how `and', `or', `not', `if', and `iff' work in conversation. We make some claims and assumptions. The introduction and elimination rules summarise how we might proceed in our conversation against the background of those claims and assumptions. Many philosophers have thought that the meaning of an expression is entirely governed by how it is or might be used in a conversation by competent speakers – in slogan form, \emph{meaning is fixed by use}. If these proof rules describe how we might bring an expression into a conversation, and what we may do with it once it is there, then these proof rules describe the totality of facts on which meaning depends. The meaning of a connective, according to this \define{inferentialist} picture, is represented by its introduction and elimination rules – and not by the truth-function that a schematic truth table represents. On this view, it is the correctness of the schematic proof of $\meta{A}\eor\meta{B}$ from $\meta{A}$ which explains \emph{why} the schematic truth table for `$\eor$' has a T on every row on which at least one of its constituents gets a T. 

There is a significant debate on just this issue in the philosophy of language, about the nature of \emph{meaning}. Is the meaning of a word what it represents, the view sometimes called \define{representationalism}? Or is the meaning of a word, rather, given by some rules for how to use it, as inferentialism says? We cannot go deeply into this issue here, but I will say a little. The representationalist view seems to accomodate some expressions very well: the meaning of a name, for example, seems very plausibly to be identified with what it names; the meaning of a predicate might be thought of as the corresponding property. But inferentialism seems more natural as an approach to the logical connectives: 
\begin{quote}
	Anyone who has learnt to perform [conjunction introduction and conjunction elimination] knows the meaning of ‘and’, for there is simply nothing more to knowing the meaning of ‘and’ than being able to perform these inferences.\footnote{A N Prior (1961) `The Runabout Inference-Ticket', \emph{Analysis} \textbf{21}, p.\ 38.}
\end{quote} It seems rather unnatural, by contrast, to think that the meaning of `and' is some abstract mathematical `thing' represented by a truth table.


Can the inferentialist distinguish good systems of rules, such as those governing `and', from bad systems? The problem is that without appealing to truth tables or the like, we seem to be committed to the legitimacy of rather problematic connectives. The most famous example is Prior's `tonk' governed by these rules:

\begin{multicols}{2}\noindent
	\begin{proof}
		\have[m]{a}{\meta{A}}
		\have[\ ]{}{\vdots}
		\have[n]{}{\meta{A}\text{ tonk }\meta{B}} \by{tonk-I}{a}
	\end{proof}

	\begin{proof}
		\have[m]{a}{\meta{A}\text{ tonk }\meta{B}}
		\have[\ ]{}{\vdots}
		\have[n]{}{\meta{B}} \by{tonk-E}{a}
	\end{proof}
\end{multicols}

You will notice that `tonk' has an introduction rule like `\eor', and an elimination rule like `\eand'. Of course `tonk' is a connective we would not like in a language, since pairing the introduction and elimination rules would allow us to prove any arbitrary sentence from any assumption whatsover: \begin{proof}
	\hypo{a}{P}
	\have{b}{P\text{ tonk }Q}\by{tonk-I}{a}
	\have{c}{Q}\by{tonk-E}{b}
\end{proof}

If we are to rule out such deviant connectives as `tonk', Prior argues, we have to accept that  `an expression must have some independently determined meaning before we can discover whether inferences involving it are valid or invalid' (Prior, \emph{op}. \emph{cit}., p.\ 38). We cannot, that is, accept the inferentialist position that the rules of implication come first and the meaning comes second. Inferentialists have replied, but we must unfortunately leave this interesting debate here for now.\footnote{The interested reader might wish to start with this reply to Prior: Nuel D Belnap, Jr (1962) `Tonk, Plonk and Plink', \emph{Analysis} \textbf{22}, pp.\ 130–34.}

\section{Constructivism} \label{construct}

The proof of excluded middle we saw on p. \pageref{excmidd} is an example of an \define{indirect proof}: even though the main connective of our conclusion is a disjunction, we don't establish it by disjunction introduction. This is typical in fact of \emph{reductio} reasoning in general: we show something is true, by showing that an absurdity would be true if it were false.

An influential group of mathematicians are worried by indirect proofs. There is a view known as \define{constructivism} which regards mathematical objects as  \emph{constructed not discovered}. The closely related view known as \define{intuitionism} agrees, while offering a particular account of the construction as fundamentally deriving from human perception of the passage of time. The Dutch mathematician L E J Brouwer is most famously associated with this view. He says this about the origins of our understanding of the natural numbers: \begin{quote}
…intuitionistic mathematics is an essentially languageless activity of the mind having its origin in the perception of a move of time. This perception of a move of time may be described as the falling apart of a life moment into two distinct things, one of which gives way to the other, but is retained by memory. If the twoity thus born is divested of all quality, it passes into the empty form of the common substratum of all twoities. And it is this common substratum, this empty form, which is the basic intuition of mathematics. (Brouwer 1981, 4–5)	\footnote{L E J Brouwer (1981) \emph{Brouwer’s Cambridge lectures on intuitionism}, D van Dalen, ed., Cambridge University Press, at pp. 4–5.}
\end{quote} 
Regardless of your view of intuitionism, the idea that mathematical objects are not pre-existing inhabitants of some Platonic realm has a lot of appeal. 

Constructivists of all stripes think we shouldn't accept the law of excluded middle, because to think that any claim of the form $\meta{A}\eor\enot\meta{A}$ must be true is to think that there is a pre-existing fact of the matter as to whether or not \meta{A} – and there may not be until we have constructed the mathematical objects in question. A mathematical existence proof, for example showing that a number with a certain property exists, must – according to the constructivist – consist in a construction of the specific number in question. We cannot show that some number has a property just by showing that the supposition that no number has that property leads to contradiction. 

When you show that \enot\meta{A} leads to absurdity, you have constructed a proof of $\enot\enot\meta{A}$, but that is \emph{not} the same as a proof of \meta{A} itself. Constructivists thus typically accept the $\enot$I  rule. If you prove $\enot\meta{A}$ by showing that the assumption \meta{A} leads to a contradiction, you have positively constructed an absurdity on the basis of that assumption, and that proof is acceptable. However, constructivists reject the $\enot$E rule. A proof showing that the assumption $\enot\meta{A}$ leads to a contradiction amounts only to a positive construction of $\enot\enot\meta{A}$ – not a construction of \meta{A}. 

Constructive logic has some interesting features that our logical system does not. For example, constructive logic is typically understood to have the \define{disjunction property}: \factoidbox{If $\meta{A} \eor \meta{B}$ can be proved from no assumptions, then either \meta{A} can be proved from no assumptions, or  $\meta{B}$  can be proved from no assumptions.} Our logic clearly lacks the disjunction property, as the proof of excluded middle demonstrates: `$P\eor\enot P$' can be proved but neither of its disjuncts is provable. 

Constructive and intuitionistic logic is an alternative conception of the nature of logic to that we have advocated. It is typically understood to involve recentering logic around provability rather than truth. So rather than saying `$P\eor\enot P$' is false, constructivists deny that it is provable, and then add that mathematical language should be restricted to what is provable, rather than relying on a Platonistic notion of abstract unworldly truth. The philosophical potential of this alternative way of thinking about logic unfortunately takes us beyond the scope of the present course.\footnote{A brief account of intuitionistic logic and the central role it gives to provability can be found in §§3.1–3.2 of  Rosalie Iemhoff (2020) `Intuitionism in the Philosophy of Mathematics', in Edward N. Zalta, ed., \emph{The Stanford Encyclopedia of Philosophy} \httpurl{plato.stanford.edu/entries/intuitionism/\#BHKInt}.}


\keyideas{
	\item The question of how to understand conditionals in natural language is a tricky one. The natural deduction rules we adopt are suitable to understand the logical conditional `$\eif$', but this may only be an approximation of English `if'.
	\item The philosophical question of whether connectives are given meaning by their truth tables or by their natural deduction rules is an interesting one.
	\item Constructive (or intuitionistic) mathematics is often understood to call for a revision of our logical proof rules, in particular $\enot$E.
}


\chapter{Proof-Theoretic Concepts}\label{s:ProofTheoreticConcepts}

\section{Provability and the Deduction Theorem}

We shall introduce some new vocabulary and notation.
\factoidbox{If there is a proof conforming to our natural deduction rules which ends on a line containing \meta{C}, such that the undischarged assumptions still in effect on that last line are all among $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n}$ then we say that \meta{C} is \define{provable from} $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n}$. This is abbreviated, in our metalanguage, like this:
$$\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n} \proves \meta{C}.$$} Consider this proof: \begin{proof}
	\hypo{a}{A}
	\open
	\hypo{b}{\enot B \eif \enot A}
	\open
	\hypo{nb}{\enot B}
	\have{na}{\enot A}\ce{b,nb}
	\have{ra}{A}\by{R}{a}
	\close
	\have{c}{B}\ne{nb-na,nb-ra}
\end{proof} The undischarged assumptions are `$A$' and `$\enot B \eif \enot A$' – the assumption `$\enot B$' on line 3 is discharged by the application of negation elimination that leads to the last line, `$B$'. So this proof shows that $A, \enot B \eif \enot A \proves B$.

The symbol `$\proves$' is known as the \emph{single turnstile}. I want to emphasise that this is different from the {double turnstile} symbol (`$\entails$') that represents entailment (§\ref{FOL.semantics}). \begin{itemize}
	\item The single turnstile, `$\proves$', concerns the \emph{existence} of a certain kind of formal proof – namely, $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n} \proves \meta{C}$ claims that there is a formal proof which terminates with $\meta{C}$ and has among its undischarged assumptions only sentences among $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n}$.
	\item The double turnstile, `$\entails$', concerns the \emph{non-existence} of a certain kind of interpretation (or valuation, in the special case of \TFL) – namely, that there is no interpretation making each of $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n}$ true while making $\meta{C}$ false.
\end{itemize} \emph{These are very different notions.}

However, if we've designed our proof system well, we shouldn't be able to prove a conclusion from some assumptions \emph{unless} that conclusion validly follows from those assumptions. And if we are really fortunate, we should be able to provide a proof corresponding to any valid argument. (More on this in §\ref{sec:soundcomp}.) But even if our two turnstiles agree on which sentences they relate to other sentences, they still mean different things. Recall the discussion of coextensive predicates in §\ref{predrel} – even if the extensions of `$\proves$' and `$\entails$' are the same, we apply them on quite different grounds. If they coincide despite being defined so differently, that is some evidence that we are uncovering a genuine and important relation between sentences, describable in a number of different ways.

A key result, known as the \define{deduction theorem}, links the notion of provability with the conditional: \factoidbox{$\meta{A}_{1},…,\meta{A}_{n},\meta{B} \proves \meta{C}$ iff $\meta{A}_{1},…,\meta{A}_{n} \proves \meta{B} \to \meta{C}$.} We can show this result by showing how to convert a proof showing $\meta{A}_{1},…,\meta{A}_{n},\meta{B} \proves \meta{C}$ into a proof showing $\meta{A}_{1},…,\meta{A}_{n} \proves \meta{B} \to \meta{C}$, and \emph{vice versa}. So first suppose we have the proof on the left, we can apply conditional introduction to discharge the occurence of \meta{B} and prove a conditional with $\meta{B}$ as antecedent:
    		 \begin{multicols}{2}\noindent\begin{proof}
    				\hypo{a1}{\meta{A}_{1}}
    				\hypo[\ ]{}{\vdots}
    				\open
    				\hypo[n]{an}{\meta{A}_{n}}
    				\open\hypo{b}{\meta{B}}
    				\have[\ ]{}{\vdots}
    				\have[k]{c}{\meta{C}}
    			\end{proof}
    			\begin{proof}
    				\hypo{a1}{\meta{A}_{1}}
    				\hypo[\ ]{}{\vdots}
    				\open
    				\hypo[n]{an}{\meta{A}_{n}}
    				\open\hypo{b}{\meta{B}}
    				\have[\ ]{}{\vdots}
    				\have[k]{c}{\meta{C}}
    				\close
    				\have{bc}{\meta{B}\to\meta{C}}\ci{b-c}
    			\end{proof}\end{multicols}
The other direction is just as easy. Suppose we have the proof on the left, terminating in $\meta{B}\eif\meta{C}$, we can make a new assumption of \meta{B} and use conditional elimination to generate a proof terminating in \meta{C}, with that new assumption remaining undischarged. 
 \begin{multicols}{2}\noindent\begin{proof}
    				\hypo{a1}{\meta{A}_{1}}
    				\hypo[\ ]{}{\vdots}
    				\open
    				\hypo[n]{an}{\meta{A}_{n}}
    				\have[\ ]{}{\vdots}
    				\have[k]{bc}{\meta{B} \eif \meta{C}}
    			\end{proof} 
    			\begin{proof}
    				\hypo{a1}{\meta{A}_{1}}
    				\hypo[\ ]{}{\vdots}
    				\open
    				\hypo[n]{an}{\meta{A}_{n}}
    				\have[\ ]{}{\vdots}
    				\have[k]{bc}{\meta{B} \eif \meta{C}}
    				\open
    				\hypo{b}{\meta{B}}
    				\have{c}{\meta{C}}\ce{bc,b}
    			\end{proof}\end{multicols}

\section{Other Proof-Theoretic Notions}

We now introduce a few notions that can be defined in terms of provability. We write
$$\proves \meta{A}$$
to mean that there is a proof of $\meta{A}$ which ends up having no undischarged assumptions. (You can think of it having no claims on the left hand side of the turnstile – a proof which has all of its undischarged assumptions among no claims must have no undischarged assumptions!) We now define:
	\factoidbox{
		$\meta{A}$ is a \define{theorem} iff $\proves \meta{A}$.
	}
Just as provability is analogous to entailment (and is, we hope, coextensive with it), so theoremhood corresponds to logical truth. 

To illustrate the idea, suppose I want to prove that `$\enot (G \eand \enot G)$' is a theorem. So I must start my proof without \emph{any} assumptions. However, since I want to prove a sentence whose main connective is a negation, I shall want to  immediately begin a subproof by making the additional assumption `$A \eand \enot A$' for the sake of argument, and show that this leads to contradictory consequences. All told, then, the proof looks like this:
	\begin{proof}
		\open
			\hypo{con}{G \eand \enot G}
			\have{a}{G}\ae{con}
			\have{na}{\enot G}\ae{con}
		\close
		\have{lnc}{\enot (G \eand \enot G)}\ni{con-a,con-na}
	\end{proof}
We have therefore constructed a proof of `$\enot (G \eand \enot G)$' with no (undischarged) assumptions, showing that `$\enot (G \eand \enot G)$' is a theorem. This particular theorem is an instance of what is sometimes called the \define{law of non-contradiction}, that for any \meta{A}, $\enot(\meta{A}\eand\enot\meta{A})$. You can see how the proof above could be adapted to demonstrate the theoremhood of any instance of the law of non-contradiction. Simply substitute any sentence $\meta{A}$ for every occurence of `$G$' in the above proof, and the transformed proof will remain correct (any internal sentence connectives featuring in \meta{A} aren't addressed by the proof rules in that proof).\footnote{We have already seen a proof showing an instance of the law of excluded middle is a theorem in §\ref{negelim}, page \pageref{excmidd}.}

Because every proof begins with an assumption, we can only we can only obtain a proof of a theorem if we discharge that opening assumption with a rule which allows one to close a subproof: conditional or biconditional introduction, or either of the negation rules (introduction or elimination): 
  \begin{proof}
    \open
    \hypo{q}{Q}
    \open
    \hypo{p}{P}
    \open
    \hypo{nq}{\enot Q}
    \have{qq}{Q}\by{R}{q}
    \close
    \have{q2}{Q}\ne{nq-qq,nq-nq}
    \close
    \have{pq}{P \eif Q}\ci{p-q2}
    \close
    \have{qpq}{Q \eif (P \eif Q)}\ci{q-pq}
  \end{proof}
There is a connection to the deduction theorem here too. Any correct proof of \meta{C} with one undischarged assumption \meta{A} will demonstrate $\meta{A}\proves\meta{C}$. The deduction theorem then assures us that $\proves \meta{A}\eif\meta{C}$. We see just this in the last line of the above proof, where a proof that $Q \proves (P \eif Q)$ is converted to a proof showing that $\proves Q \eif (P \eif Q)$.

But we cannot say that every theorem has a negation, a conditional or a biconditional as its main connective. For one thing, we could have started with a negated disjunction or conjunction. For another, once we have a proof of a theorem, we can apply disjunction or conjunction introduction to its last line: e.g., we could extend the above proof by conjunction introduction to show that $\proves ((Q \to (P \to Q))\eand (Q \to (P \to Q)))$. 



To show that something is a theorem, you just have to find a suitable proof. It is typically much harder to show that something is \emph{not} a theorem. To do this, you would have to demonstrate, not just that certain proof strategies fail, but that \emph{no} proof is possible. Even if you fail in trying to prove a sentence in a thousand different ways, perhaps the proof is just too long and complex for you to make out. Perhaps you just didn't try hard enough. Even if you come up with a systematic search strategy to show that some sentence $\meta{B}$ isn't a theorem, there is no guarantee your strategy will yield a result. Suppose you tried to construct all well-formed proofs terminating with \meta{B} from shortest to longest, aiming to show there is no proof in which all assumptions have been discharged. As there is no longest proof, there is no guarantee at any stage in this process that your failure to find such a proof shows there is no such proof. It might just be that the shortest such proof is longer than any you've yet considered. On the other hand, if one of the proofs you construct is a proof of \meta{B} with no undischarged assumptions, they you have shown conclusively that it is a theorem, and you can stop your search. Showing that something isn't theorem can be harder than showing that it is, in terms of how many proofs you have to consider. (On the other hand, showing that something is a logical truth can be harder than showing that it is not, in terms of how many interpretations you need to consider.)



Here is another new bit of terminology:
	\factoidbox{
		Two sentences \meta{A} and \meta{B} are \define{provably equivalent} iff each can be proved from the other; i.e., both $\meta{A}\proves\meta{B}$ and $\meta{B}\proves\meta{A}$.
		}

Here is a third new bit of terminology:
	\factoidbox{
		The sentences $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n}$ are \define{jointly contrary} iff a sentence and its negation can be proved from them, i.e., for some $\meta{B}$, $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n} \proves \meta{B}$ and $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n} \proves \enot\meta{B}$. (Sometimes in this case the $\meta{A}_{i}$s are said to be \define{provably inconsistent}.)} Equivalently, some sentences are jointly contrary if you can prove a contradiction from them: $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n} \proves \meta{B}\eand\enot\meta{B}$. 

It is straightforward to show that some sentences $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n}$ are jointly contrary (if they are): you just need to provide two proofs, one terminating in $\meta{B}$ and the other in $\enot\meta{B}$, such that all of the undischarged assumptions in those proofs are among the $\meta{A}_{i}$s. Showing that some sentences are not jointly contrary is much harder. It would require more than just providing a proof or two; it would require showing that no proof of a certain kind is \emph{possible}.


Some sentences are jointly contrary iff \emph{the negation of their conjunction is a theorem}. Suppose we have these proofs showing the $\meta{A}_{i}$s to be jointly contrary: 

\begin{multicols}{2}\noindent
	\begin{proof}
		\hypo{a1}{\meta{A}_{1}}
		\hypo[\ ]{}{\vdots}
		\hypo[n]{an}{\meta{A}_{n}}
		\have[\ ]{}{\vdots}
		\have[k]{c}{\meta{B}}
	\end{proof}

	\begin{proof}
		\hypo{a1}{\meta{A}_{1}}
		\hypo[\ ]{}{\vdots}
		\hypo[n]{an}{\meta{A}_{n}}
		\have[\ ]{}{\vdots}
		\have[k']{c}{\enot \meta{B}}
	\end{proof}
\end{multicols}

These can be adapted to form part of a larger proof:
\begin{proof}
	\open
		\hypo{a1an}{\meta{A}_{1}\wedge … \wedge \meta{A}_{n}}
		\have{a1}{\meta{A}_{1}}\ae{a1an}
		\have[\ ]{}{\vdots}
		\have[n+1]{an}{\meta{A}_{n}}\ae{a1an}
		\have[\ ]{}{\vdots}
		\have[k+1]{c}{\meta{B}}\by{from}{a1-an}
		\have[\ ]{}{\vdots}
		\have[k'+i]{nb}{\meta{\enot B}}\by{from}{a1-an}
		\close
		\have[k'+i+1]{nc}{\enot (\meta{A}_{1}\wedge … \wedge \meta{A}_{n})}\ni{a1an-c,a1an-nb}
	\end{proof}
Conversely, you can extract proofs of $\meta{A}_{1}\wedge … \wedge \meta{A}_{n}\proves \meta{B}$ and $\meta{A}_{1}\wedge … \wedge \meta{A}_{n}\proves \enot\meta{B}$ from the above proof. The pattern is quite general, since any theorem which is a negated conjunction will be proved by some application of negation introduction on the original conjunction, which involves showing that original conjunction to include jointly contrary sentences.

To establish whether these proof-theoretic properties hold for some sentences requires us to construct one or two proofs, and to establish that they do not hold requires us to consider all possible proofs. Table~\ref{proofstable} summarises the requirements for provability, contrariety, etc.
\begin{table}[b]
\begin{tabular}{l l l} \toprule 
%\cline{2-3}
 & \textbf{Yes} & \textbf{No}\\
 \midrule
%\cline{2-3}
theorem? & one proof & all possible proofs\\
%logical falsehood? &  one proof  & all possible proofs\\
equivalent? & two proofs & all possible proofs\\
jointly contrary? & two proofs & all possible proofs\\
provable & one proof & all possible proofs\\
\bottomrule \end{tabular}
\caption{What we need to establish proof-theoretic features.\label{proofstable}}
\end{table}


\section{Structural Rules and the Theory of Proofs}\label{s:substructural}

Our proofs get their main shape from the proof rules governing the connectives. But choices we have made about how to build proofs also contribute. These principles about how to construct proofs give rise to quite abstract and general features of the notion of provability represented by `$\proves$', sometimes known as the \emph{structural rules} governing the notion of provability.\footnote{For more on structural rules, and the various logics that don't have all the structural features of our natural deduction system, see Greg Restall (2018) `Substructural Logics' in Edward N Zalta, ed., \emph{The Stanford Encyclopedia of Philosophy} \httpurl{plato.stanford.edu/entries/logic-substructural/}.}

For example: in our proof system, it does not matter in what order we make assumptions. These two proofs, distinct in their structure, nevertheless both show that $A, B \proves (A \eand B)$. 
\begin{multicols}{2}\noindent\begin{proof}
	\hypo{a}{A}
	\open
	\hypo{b}{B}
	\have{c}{(A \eand B)}\ai{a,b}
\end{proof}\qquad \begin{proof}
	\hypo{a}{B}
	\open
	\hypo{b}{A}
	\have{c}{(A \eand B)}\ai{b,a}
\end{proof}\end{multicols} 

Recall that our definition of provability says that $\meta{A}_{1},…, \meta{A}_{n}\vdash \meta{B}$ just in case there is a proof whose undischarged assumptions are all among the $\meta{A}_{i}$s. No mention is made of the order of those undischarged assumptions. So while both the proofs above show that $A, B \proves (A \eand B)$, they also both show that $B, A \proves (A \eand B)$.

This feature, that you can permute the order of assumptions arbitrarily, is wholly general, and is known as the \define{commutativity} of assumptions. That is to say: \factoidbox{A notion of provability satisfies commutativity just in case $\meta{A}_{1},\ldots,\meta{B},\ldots, \meta{C},\ldots,\meta{A}_{n},  \proves \meta{D}$ iff  $\meta{A}_{1},\ldots,\meta{C},\ldots, \meta{B},\ldots,\meta{A}_{n}, \proves \meta{D}$.}

Commutativity and the deduction theorem seem trivial. But they can be surprisingly powerful. Consider, for example, this trivial proof by reiteration that $P \eif Q \proves P \eif Q$: \begin{proof}
	\hypo{a}{P \eif Q}
	\have{b}{P \eif Q}\by{R}{a}
\end{proof} We can then reason as follows: \begin{enumerate}
	\item $P \eif Q \proves P \eif Q$;
	\item $P \eif Q, P \proves Q$ (by the deduction theorem);
	\item $P, P \eif Q \proves Q$ (by commutativity);
	\item $P \proves (P  \eif Q) \eif Q$ (by the deduction theorem).
\end{enumerate} This argument doesn't construct a formal proof; it just assures you that there will be one. (One of the exercises asks you to construct the formal proof.)

Here's another example of a structural feature of our proof system. We allow a given line of a proof to be reused multiple times, as long as the assumptions on which that line relies remain undischarged. See this proof that $(P \eif(P \eif Q)) \proves (P\eif Q)$: \begin{proof}
	\hypo{a}{P\eif(P \eif Q)}
	\open
	\hypo{b}{P}
	\have{c}{P\eif Q}\ce{a,b}
	\have{d}{Q}\ce{c,b}
	\close
	\have{e}{P \eif Q}\ci{b-d}
\end{proof} Here we appeal to line 2 multiple times: in eliminating the conditional on line 1 and the conditional on line 3. No rule governing any of our connectives is associated with this behaviour: rather, it is built in to the way we allow all of our rules to appeal to any previous line (as long as the line doesn't appear in a closed subproof), even if that line has been appealed to already by some other rule. It is fairly easy to see that the above proof cannot succed without multiple appeal to line 2.

This feature of our proof system is known as \define{contraction}: if there is a proof in which any $\meta{A}$ occurs as an undischarged assumption on two or more distinct lines, there is also a proof in which one of those assumptions of $\meta{A}$ is removed. More concisely: \factoidbox{A notion of provability satisfies contraction just in case: $$\meta{A}_{1},\ldots, \meta{B},\ldots,\meta{B},\ldots,\meta{A}_{n} \proves \meta{C} \text{ iff } \meta{A}_{1},\ldots, \meta{B},\ldots,\meta{A}_{n}  \proves \meta{C}.$$} 
Contraction, or the principle that you can appeal to the same prior line multiple times, is essentially the same as our proof rule of reiteration. In fact the rule of reiteration is strictly dispensible (§\ref{der.reit}), in part because we can always appeal instead to the original sentence multiple times.

The final structural feature I want to point to is that adding additional assumptions doesn't undermine provability. This property is called \define{weakening}: \factoidbox{A notion of provability satisfies weaking just in case: if $\meta{A}_{1},\ldots,\meta{A}_{n} \proves \meta{C}$ then $\meta{A}_{1},\ldots,\meta{A}_{n},\meta{B} \proves \meta{C}$.} This is a very general feature, because any correct natural deduction proof can be embedded within an arbitrary additional assumption and still remain correct. So if a proof with the structure illustrated schematically on the left is correct, showing that $\meta{A}_{1},\ldots,\meta{A}_{n} \proves \meta{C}$, then so is the proof scheme on the right, which has all the same assumptions plus the additional assumption $\meta{B}$, and so shows $\meta{A}_{1},\ldots,\meta{A}_{n},\meta{B} \proves \meta{C}$: \begin{multicols}{2}\noindent\begin{proof}
	\hypo{a1}{\meta{A}_{1}}
	\have[\ ]{}{\vdots}
	\open
	\hypo{an}{\meta{A}_{n}}
	\have[\ ]{}{\vdots}
	\have{c}{\meta{C}}
\end{proof}\qquad\begin{proof}
	\hypo{b}{\meta{B}}
	\open
	\hypo{a1}{\meta{A}_{1}}
	\have[\ ]{}{\vdots}
	\open
	\hypo{an}{\meta{A}_{n}}
	\have[\ ]{}{\vdots}
	\have{c}{\meta{C}}
\end{proof}\end{multicols}

These three structural principles involved in the construction of our natural proofs support our decision to define provability as we did. Our definition was:  $\meta{A}_{1}…\meta{A}_{n}\proves \meta{C}$ when there is a proof with undischarged assumptions \emph{among} the $\meta{A}_{i}$s. We do not require that the undischarged assumptions be exactly the $\meta{A}_{i}$s, nor that the undischarged assumptions don't contain any redundancy, nor that the order in which assumptions are made in the proof is the same as the order of the sentences on the left side of the turnstile. We will briefly mention some alternative logics in which some of these structural rules are abandoned in §\ref{s:nextsteps}.


\keyideas{
	\item Our formal proof system allows us to introduce a notion of provability, symbolised `$\proves$'. This is distinct from entailment `$\entails$', but (if we've done our work correctly) they will parallel one another.
	\item We can introduce further notions in terms of provability: provable equivalence, joint contrariness, and being a theorem.
	\item Some features of our notion of provability derive from structural features of the notion of proof we have employed, such as the ability to appeal to a proof line multiple times, or to make additional assumptions at will.
}

\practiceproblems
\problempart
Give a proof showing that each of the following sentences is a theorem:
\begin{earg}
\item $O \eif O$;
\item $J \eiff \bigl(J\eor (L\eand\enot L) \bigr)$;
\item $((A \eif B) \eif A) \eif A$;
\item $\bigl((P \eif P) \eif Q)\bigr) \eif Q$;
\item $\bigl((C \eand D) \eiff (D \eand C)\bigr)$.
\end{earg}

\newpage\problempart
Provide proofs to show each of the following:
\begin{earg}
\item $P \proves (P  \eif Q) \eif Q$
\item $C\eif(E\eand G), \enot C \eif G \proves G$
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$
\end{earg}

\problempart
Show that each of the following pairs of sentences are provably equivalent:
\begin{earg}
\item $R \eiff E$, $E \eiff R$
\item $G$, $\enot\enot\enot\enot G$
\item $T\eif S$, $\enot S \eif \enot T$
\item $U \eif I$, $\enot(U \eand \enot I)$
\item $\enot (C \eif D), C \eand \enot D$
\item $\enot G \eiff H$, $\enot(G \eiff H)$ 
\end{earg}

\problempart
If you know that $\meta{A}\proves\meta{B}$, what can you say about $(\meta{A}\eand\meta{C})\proves\meta{B}$? What about $(\meta{A}\eor\meta{C})\proves\meta{B}$? Explain your answers.


\problempart In this section, I claimed that it is just as hard to show that two sentences are not provably equivalent, as it is to show that a sentence is not a theorem. Why did I claim this? (\emph{Hint}: think of a sentence that would be a theorem iff \meta{A} and \meta{B} were provably equivalent.)

\problempart Show that $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n} \proves \meta{B}\eand\enot\meta{B}$ iff both $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n} \proves \meta{B}$ and $\meta{A}_{1},\meta{A}_{2},…, \meta{A}_{n} \proves \enot\meta{B}$.



\chapter{Proof Strategies}\label{c:proof.strat}
There is no simple recipe for proofs, and there is no substitute for practice. Here, though, are some rules of thumb and strategies to keep in mind.




\paragraph{Work backwards from what you want}
The ultimate goal is to obtain the conclusion. Look at the conclusion and ask what the introduction rule is for its main connective. This gives you an idea of what should happen \emph{just before} the last line of the proof. Then you can treat this line as if it were your goal. Ask what you could do to get to this new goal.

For example: If your conclusion is a conditional $\meta{A}\eif\meta{B}$, plan to use the {\eif}I rule. This requires starting a subproof in which you assume \meta{A}. The subproof ought to end with \meta{B}. So, what can you do to get $\meta{B}$?


\paragraph{Work forwards from what you have}
When you are starting a proof, look at the premises; later, look at the sentences that you have obtained so far. Think about the elimination rules for the main operators of these sentences. These will tell you what your options are.

For a short proof, you might be able to eliminate the premises and introduce the conclusion. A long proof is formally just a number of short proofs linked together, so you can fill the gap by alternately working back from the conclusion and forward from the premises.

\paragraph{Try proceeding indirectly}
If you cannot find a way to show $\meta{A}$ directly, try starting by assuming $\enot \meta{A}$. If a contradiction follows, then you will be able to obtain $\meta{A}$ by $\enot$E. This will often be a good way of proceeding when the conclusion you are aiming at has a \emph{disjunction} as its main connective.  

\paragraph{Persist}
These are guidelines, not laws. Try different things. If one approach fails, then try something else. Remember that if there is one proof, there are many – different proofs that make use of different ideas.

For example: suppose you tried to follow the idea `work backwards from what you want' in establishing `$P, P \eif (P \eif Q) \proves P \eif Q$'. You would be tempted to start a subproof from the assumption `$P$', and while that proof strategy would eventually succeed, you would have done better to simply apply $\eif$E and terminate after one proof step.

By contrast, suppose you tried to follow the idea `work forwards from what you have' in trying to establish `$(P \eor (Q \eor (R \eor S))) \proves P \eif P$'. You might begin an awkward nested series of subproofs to apply $\eor$E to the disjunctive premise. But beginning with the conclusion might prompt you instead to simply open a subproof from the assumption $P$, and the subsequent proof will make no use of the premise at all, as the conclusion is a theorem.

Neither of these heuristics is sacrosanct. You will get a sense of how to construct proofs efficiently and fluently with practice. Unfortunately there is no quick substitute for practice.


\keyideas{
	\item The nature of natural deduction proofs means that it is sometimes easier to make progress by applying rules to the assumptions, and at other times easier to try and figure out where a conclusion could have come from. 
	\item One may have to try a number of different things in the course of constructing the same proof – there is no simple algorithm to capture logical reasoning in this system.
}






\chapter{Derived Rules for \textnormal{\TFL}}\label{s:Derived}
In §§\ref{s:BasicTFLs}–\ref{s:BasicTFLns}, we introduced the basic rules of our proof system for \TFL. In this section, we shall consider some alternative or additional rules for our system.

None of these rules adds anything fundamentally to our system. They are all \define{derived} rules, which means that anything we can prove by using them, we could have proved using just the rules in our original official system of natural deduction proofs. Any of these rules is a \emph{conservative} addition to our proof system, because none of them would enable us to prove anything we could not already prove. (Adding the rules for `tonk' from §\ref{inferentialism}, by contrast, would allow us to prove many new things – any system which includes those rules is not a conservative extension of our original system of proof rules.)

 But sometimes adding new rules can shorten proofs, or make them more readable and user-friendly. And some of them are of interest in their own right, as arguably independently plausible rules of implication as they stand, or as alternative rules we could have taken as basic instead.

\section{Reiteration}\label{der.reit}
The first derived rule is actually one of our main proof rules: \emph{reiteration}. It turns out that we need not have assumed a rule of reiteration. We can replace each application of the reiteration rule on some line $k+1$ (reiterating some prior line $m$) with the following combination of moves deploying just the \emph{other} basic rules of §§\ref{s:BasicTFLns}–\ref{s:BasicTFLs}:
\begin{proof}
	\have[m]{a}{\meta{A}}
	\have[k]{aa}{\meta{A} \eand \meta{A}}\ai{a}
	\have{a2}{\meta{A}}\ae{aa}
\end{proof}
To be clear: this is not a proof. Rather, it is a proof \emph{scheme}. After all, it uses a variable, $\meta{A}$, rather than a sentence of \TFL. But the point is simple. Whatever sentences of \TFL\ we plugged in for $\meta{A}$, and whatever lines we were working on, we could produce a legitimate proof. So you can think of this as a recipe for producing proofs. Indeed, it is a recipe which shows us that, anything we can prove using the rule R, we can prove (with one more line) using just the \emph{basic} rules of §§\ref{s:BasicTFLs}–\ref{s:BasicTFLns}. So we can describe the rule R as a derived rule, since its justification is derived from our basic rules.

You might note that in lines 5–7 in the complicated proof in Figure~\ref{fig:vi}, we in effect made use of this proof scheme, introducing a conjunction from prior lines only to immediately eliminate again, just to ensure that the relevant sentences appeared directly in the range of the assumption `$Q$'.

We even have an explanation here about why you can't reiterate a line from a closed subproof. If all applications of reiteration are in fact abbreviations of the above schema, then that restriction on reiteration derives from the more general restriction that we cannot appeal to a proof line that relies on an assumption that has been discharged.

\section{Disjunctive Syllogism}
Here is a very natural argument form.
	\begin{quote}
		Mitt is either in Massachusetts or in DC. He is not in DC. So, he is in Massachusetts.
	\end{quote}
This inference pattern is called \define{disjunctive syllogism}. We could add it to our proof system:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A} \eor \meta{B})}
	\have[n]{nb}{\enot \meta{A}}
	\have[\ ]{con}{\meta{B}}\by{DS}{ab, nb}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\meta{A} \eor \meta{B})}
	\have[n]{nb}{\enot \meta{B}}
	\have[\ ]{con}{\meta{A}}\by{DS}{ab, nb}
\end{proof}
\end{minipage}}
This is, if you like, a new rule of disjunction elimination.
But there is nothing fundamentally new here. We can emulate the rule of disjunctive syllogism using our basic proof rules, as the schematic proof in Figure~\ref{fig.dsder} indicates. \begin{figure}
	\begin{proof}
	\have[m]{avb}{\meta{A}\eor\meta{B}}
	\have[n]{na}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\open
		\hypo[k]{a}{\meta{A}}
		\open
			\hypo{nb}{\enot\meta{B}}
			\have{ra}{\meta{A}}\by{R}{a}
			\have{rna}{\enot\meta{A}}\by{R}{na}
			\close
		\have{b}{\meta{B}}\ne{nb-ra, nb-rna}
		\close
		\open
		\hypo{b2}{\meta{B}}
		\have{b3}{\meta{B}}\by{R}{b2}
		\close
	\have{}{\meta{B}}\oe{avb,a-b,b2-b3}
\end{proof}
\caption{Disjunctive syllogism is derivable in the standard proof system.\label{fig.dsder}}
\end{figure}

We have used the rule of reiteration in this schematic proof, but we already know that any uses of that rule can themselves be replaced by more roundabout proofs using conjunction introduction and elimination, if required. So adding disjunctive syllogism would not make any new proofs possible that were not already obtainable in our original system.

\section{\emph{Modus tollens}}
Another useful pattern of inference is embodied in the following argument:
	\begin{quote}
		If Hillary won the election, then she is in the White House. She is not in the White House. So she did not win the election.
	\end{quote}
This inference pattern is called \idefine{modus tollens}. The corresponding rule is:
\factoidbox{\begin{proof}
	\have[m]{ab}{(\meta{A}\eif\meta{B})}
	\have[n]{a}{\enot\meta{B}}
	\have[\ ]{b}{\enot\meta{A}}\mt{ab,a}
\end{proof}}
This is, if you like, a new rule of conditional elimination.

This rule is, again, a conservative addition to our stock of proof rules. Any application of it could be emulated by the form of proof using our original rules shown in Figure~\ref{fig.mtder}. Again, the schmatic proof makes a dispensible use of reiteration.
\begin{figure}
	\begin{proof}
	\have[m]{ab}{\meta{A}\eif\meta{B}}
	\have[n]{nb}{\enot\meta{B}}
	\have[\ ]{}{\vdots}
		\open
		\hypo[k]{a}{\meta{A}}
		\have{b}{\meta{B}}\ce{ab, a}
		\have{nb1}{\enot B}\by{R}{nb}
		\close
	\have{no}{\enot\meta{A}}\ni{a-b,a-nb1}
\end{proof}
\caption{\emph{Modus tollens} is derivable in the standard proof system.\label{fig.mtder}}
\end{figure}



\section{Double Negation Elimination}

In \TFL, the double negation $\enot\enot\meta{A}$ is equivalent to \meta{A}. In natural languages, too, double negations tend to cancel out – Malcolm is not unaware that his leadership is under threat iff he is aware that it is. That said, you should be aware that context and emphasis can prevent them from doing so. Consider: `Jane is not \emph{not} happy'. Arguably, one cannot derive `Jane is happy', since the first sentence should be understood as meaning the same as  `Jane is not \emph{un}happy'. This is compatible with `Jane is in a state of profound indifference'. As usual, moving to \TFL\ forces us to sacrifice certain nuances of English expressions – we have, in \TFL, just one resource for translating negative expressions like `not' and the suffix `un-', even if they are not synonyms in English.

Obviously we can show that $\meta{A} \proves \enot\enot\meta{A}$ by means of the following proof:
\begin{proof}
\hypo{a}{\meta{A}}
\open
\hypo{b}{\enot\meta{A}}
\close
\have{c}{\enot\enot\meta{A}}\ni{b-a,b-b}	
\end{proof}

There is a proof rule that corresponds to the other direction of this equivalence, the rule of \define{double negation elimination}:
\factoidbox{
\begin{proof}
		\have[i]{a}{\enot\enot\meta{A}}
		\have[\ ]{}{\vdots}
		\have[\ ]{}{\meta{A}}\dne{a}
	\end{proof}}


This rule is redundant, given the proof rules of \TFL: \begin{proof}
	\hypo{a}{\enot\enot\meta{A}}
	\open
	\hypo{b}{\enot\meta{A}}
	\have{ar}{\enot\enot\meta{A}}\by{R}{a}
	\have{br}{\enot\meta{A}}\by{R}{b}
	\close
	\have{c}{\meta{A}}\ne{b-br,b-ar}
\end{proof}
Anything we can prove using the $\enot\enot$E rule can be proved almost as briefly using just $\enot$E.



\section{\emph{Tertium non datur}}\label{aprtnd}

Suppose that we can show that if it's sunny outside, then Bill will have brought an umbrella (for fear of burning). Suppose we can also show that, if it's not sunny outside, then Bill will have brought an umbrella (for fear of rain). Well, there is no third way for the weather to be. So, \emph{whatever the weather}, Bill will have brought an umbrella. 

This line of thinking motivates the following rule:
\factoidbox{\begin{proof}
		\have[\ ]{}{\vdots}
			\open
			\hypo[i]{a}{\meta{A}}
			\have[j]{c1}{\meta{B}}
		\close
		\open
			\hypo[k]{b}{\enot\meta{A}}
			\have[l]{c2}{\meta{B}}
		\close
		\have[\ ]{ab}{\meta{B}}\tnd{a-c1,b-c2}
	\end{proof}}
The rule is sometimes called \idefine{tertium non datur}, which means roughly `no third way'. There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first.

\emph{Tertium non datur} is able to be emulated using just our original proof rules. Figure \ref{fig.tndder} contains a schematic proof which demonstrates this. Once again, a dispensible use of reiteration occurs in this proof just to make it more readable.
\begin{figure}
\begin{proof}
		\open
			\hypo[i]{a}{\meta{A}}
			\have[j]{c1}{\meta{B}}
		\close
		\open
			\hypo[k]{b}{\enot\meta{A}}
			\have[l]{c2}{\meta{B}}
		\close
		\have[\ ]{}{\vdots}
		\have[m]{ab}{\meta{A}\eif\meta{B}}\ci{a-c1}
		\have{nab}{\enot\meta{A}\eif\meta{B}}\ci{b-c2}
		\open
			\hypo{nb}{\enot\meta{B}}
			\open
				\hypo{aa}{\meta{A}}
				\have{b2}{\meta{B}}\ce{ab,aa}
				\have{nb2}{\enot\meta{B}}\by{R}{nb}
				\close
			\have{na}{\enot\meta{A}}\ni{aa-nb2}
			\have{b3}{\meta{B}}\ce{nab,na}
			\close
		\have{b4}{\meta{B}}\ne{nb-b3}
	\end{proof}
	\caption{\emph{Tertium non datur} is derivable in the standard proof system.\label{fig.tndder}}
\end{figure}






\section{De Morgan Rules}
Our final additional rules are called De Morgan's Laws. (These are named after the nineteenth century logician August De Morgan.) The first two De Morgan rules show the provable equivalence of a negated conjunction and a disjunction of negations.
\factoidbox{\hspace{-0.3cm}\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\enot (\meta{A} \eand \meta{B})}
	\have[\ ]{dm}{(\enot \meta{A} \eor \enot \meta{B})}\dem{ab}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\enot \meta{A} \eor \enot \meta{B})}
	\have[\ ]{dm}{\enot (\meta{A} \eand \meta{B})}\dem{ab}
\end{proof}
\end{minipage}}
The second pair of De Morgan rules are dual to the first pair: they show the provable equivalence of a negated disjunction and a conjunction of negations.
\factoidbox{\hspace{-0.3cm}\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{\enot (\meta{A} \eor \meta{B})}
	\have[\ ]{dm}{(\enot \meta{A} \eand \enot \meta{B})}\dem{ab}
\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
	\have[m]{ab}{(\enot \meta{A} \eand \enot \meta{B})}
	\have[\ ]{dm}{\enot (\meta{A} \eor \meta{B})}\dem{ab}
\end{proof}
\end{minipage}}

\newpage
The De Morgan rules are no genuine addition to the power of our original natural deduction system. Here is a demonstration of how we could derive the first De Morgan rule:
 	\begin{proof}
		\have[k]{nab}{\enot (\meta{A} \eand \meta{B})}
		\open
		\hypo[m]{n}{\enot(\enot\meta{A}\eor\enot\meta{B})}
			\open
			\hypo{na}{\enot\meta{A}}
			\have{navnb}{\enot\meta{A}\eor\enot\meta{B}}\oi{na}
			\close
		\have{a}{\meta{A}}\ne{na-navnb,na-n}
			\open
			\hypo{nb}{\enot\meta{B}}
			\have{nav2}{\enot\meta{A}\eor\enot\meta{B}}\oi{nb}
			\close
		\have{b}{\meta{B}}\ne{nb-nav2,nb-n}
		\have{aab}{\meta{A}\eand\meta{B}}\ai{a,b}
		\close
	\have{c}{\enot\meta{A}\eor\enot\meta{B}}\ne{n-aab,n-nab}
	\end{proof}
Here is a demonstration of how we could derive the second De Morgan rule:
 	\begin{proof}
		\have[k]{nab}{\enot \meta{A} \eor \enot \meta{B}}
		\open
		\hypo[m]{na}{\enot \meta{A}}
			\open
			\hypo{ab}{\meta{A} \eand \meta{B}}
			\have{a}{\meta{A}}\ae{ab}
			\close
		\have{naab}{\enot(\meta{A}\eand\meta{B})}\ni{ab-a,ab-na}
			\close
			\open
		\hypo{nb}{\enot\meta{B}}
			\open
			\hypo{ab2}{\meta{A} \eand \meta{B}}
			\have{b}{\meta{B}}\ae{ab2}
			\close
			\have{naab2}{\enot(\meta{A}\eand\meta{B})}\ni{ab2-b,ab2-nb}
			\close
		\have{c}{\enot(\meta{A}\eand\meta{B})}\oe{nab,na-naab,nb-naab2}
	\end{proof}
Similar demonstrations can be offered explaining how we could derive the third and fourth De Morgan rules. These are left as exercises.

\emph{Those mentioned above are all of the additional rules of our proof system for \TFL.}

\keyideas{
	\item Our official system of rules can be augmented by additional rules that are strictly speaking unneccessary – nothing is provable with them that couldn't have been proved without them – but that can nevertheless be used sometimes to speed up proofs.
	\item Only make use of derived rules when you are told you may do so.
	\item Some derived rules – such as the rule of double negation elimination – can even be used in place of a rule of our original system, given a different system but with the same things being provable.
}

\practiceproblems

\problempart
\label{pr.justifyTFLproof}
The following proofs are missing their commentaries (rule and line numbers). Add them wherever they are required: you may use any of the original or derived rules, as appropriate.

\begin{multicols}{2}\noindent
\begin{proof}
\hypo{1}{Z \eif (C \eand \enot N)}
\hypo{2}{\enot Z \eif (N \eand \enot C)}
\open
	\hypo{a1}{\enot(N \eor  C)}
	\have{a2}{\enot N \eand \enot C} {}
	\have{a6}{\enot N}{}
	\have{b4}{\enot C}{}
		\open
		\hypo{b1}{Z}
		\have{b2}{C \eand \enot N}{}
		\have{b3}{C}{}
		\have{red}{\enot C}{}
	\close
	\have{a3}{\enot Z}{}
	\have{a4}{N \eand \enot C}{}
	\have{a5}{N}{}
\close
\have{3b}{\enot\enot(N \eor C)}{}
\have{3}{N \eor C}{}
\end{proof}

\begin{proof}
\hypo{1}{W \eif \enot B}
\hypo{2}{A \eand W}
\hypo{2b}{B \eor (J \eand K)}
\have{3}{W}{}
\have{4}{\enot B} {}
\have{5}{J \eand K} {}
\have{6}{K}{}
\end{proof}

\begin{proof}
\hypo{1}{L \eiff \enot O}
\hypo{2}{L \eor \enot O}
\open
	\hypo{a1}{\enot L}
	\have{a2}{\enot O}{}
	\have{a3}{L}{}
	\have{a4}{\enot L}{}
\close
\have{3b}{\enot\enot L}{}
\have{3}{L}{}
\end{proof}\end{multicols}


\newpage
\problempart 
Give a proof representing each of these arguments; you may use any of the original or derived rules, as appropriate:
\begin{earg}
\item $E\eor F$, $F\eor G$, $\enot F \ttherefore E \eand G$
\item $M\eor(N\eif M) \ttherefore \enot M \eif \enot N$
\item $(M \eor N) \eand (O \eor P)$, $N \eif P$, $\enot P \ttherefore M\eand O$
\item $(X\eand Y)\eor(X\eand Z)$, $\enot(X\eand D)$, $D\eor M$ \ttherefore $M$
\end{earg}


\problempart
Provide proof schemes that justify the addition of the third and fourth De Morgan rules as derived rules. 



\problempart
The proofs you offered in response to question \textbf{A} above used derived rules. Replace the use of derived rules, in such proofs, with only basic rules. You will find some `repetition' in the resulting proofs; in such cases, offer a streamlined proof using only basic rules.  (This will give you a sense, both of the power of derived rules, and of how all the rules interact.)

\chapter{Alternative Proof Systems for \textnormal{\TFL}}\label{s:alternates}

We've now developed a system of proof rules, all of which we have supported by showing that they correspond to correct entailments of \TFL. We've also seen that these rules allow us to introduce some derived rules, which make proofs shorter and more convenient but do not allow us to prove anything that we could not have proved already.

This choice of proof system is not forced on us. There are \emph{alternative} proof systems which are nevertheless equivalent to the system we have introduced, in that everything which is provable in our system is provable in the alternative system, and \emph{vice versa}. Indeed, there are lots of alternative systems. In this section, I will discuss just a couple. The alternative systems I will discuss here result from taking one of our derived rules as basic, and showing that doing so allows us to derive a formerly basic rule.

\section{Replacing Negation Elimination by Double Negation Elimination} 

The first alternative system results from replacing the rule of negation elimination {\enot}E by double negation elimination {\enot\enot}E. In combination with the other rules, we can emulate the results of {\enot}E by an application of $\enot$I, followed by a single use of $\enot\enot$E:
 \begin{proof}
	\open 
	\hypo[i]{a}{\enot\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have[\ ]{}{\vdots}
	\have[k]{c}{\enot\meta{B}}
	\close
	\have{d}{\enot\enot\meta{A}}\ni{a-b,a-c}
	\have{}{\meta{A}} \dne{d}
\end{proof}
This proof shows that, in a system with {\enot}I and {\enot\enot}E, we do not need any separate elimination rule for a single negation – the effect of any such rule could be perfectly simulated by the above schematic proof. The addition of a single negation elimination rule would not allow us to prove any more than we already can. So in a sense, the rules of double negation elimination and negation elimination are equivalent, at least given the other rules in our system.

Since we've shown {\enot\enot}E to be a derived rule in our original system, this alternative system proves exactly the same things as our original system. The proofs will look different, but there is a correct proof of \meta{C} from $\meta{A}_{1},…,\meta{A}_{n}$ in one system, there will be a corresponding proof in the other system. Any proof in the one system can be converted into a proof in the other by replacement of the appropriate instances of the rules. 


\section{Replacing Disjunction Elimination with Disjunctive Syllogism} % (fold)
\label{sec:replacing_disjunction_elimination_with_disjunctive_syllogism}

Another alternative system results from replacing disjunction elimination {\eor}E (proof by cases) by disjunctive syllogism DS. We already know that DS is a derived rule: any proof which uses disjunctive syllogism can be converted into a proof that uses just the basic rules. So to show the alternative system proves the same things as our original system, we just need to show that we can emulate the effects of {\eor}E by using DS.

Recall that a schematic proof that makes use of {\eor}E has the structure in Figure \ref{DSa}. To transform this into a proof that makes use instead of DS, we are going to use those two subproofs, but in the scope of an assumption that \enot\meta{C}. The schematic proof looks like that in Figure \ref{DSb}. You can see that the proof is the same as the original at lines $i$–$j$ and $k$–$l$, which mirror the original two subproofs. But they play a quite different role now. The subproof deriving \meta{C} from \meta{A} is now used in the service of a \emph{reductio} of \meta{A}, deriving $\enot\meta{A}$ on line $j+1$ in order to put us in a position to apply DS and derive \meta{B}, and then derive \meta{C} in line with the original subproof, which conflicts with the \emph{reductio} assumption $\enot\meta{C}$ on line 2, allowing us to use {\enot}E to derive the same conclusion as in the original proof. This shows us that – at least if we have our negation rules (or their equivalents) – we can replace {\eor}E with DS and be able to prove all the same things.

\begin{figure}
\begin{minipage}{0.45\textwidth}
	\begin{proof}
	\hypo{d}{\meta{A}\eor\meta{B}}
	\have[\ ]{}{\vdots}
	\open
	\hypo[i]{a}{\meta{A}}
	\have[\ ]{v}{\vdots}
	\have[j]{c1}{\meta{C}}
	\close
	\have[\ ]{vv}{\vdots}
	\open
	\hypo[k]{b}{\meta{B}}
	\have[\ ]{vvv}{\vdots}
	\have[l]{c2}{\meta{C}}
	\close
	\have[\ ]{c}{\meta{C}}\oe{d,a-c1,b-c2}
\end{proof}
\caption{Schematic {\eor}E proof.\label{DSa}}
\end{minipage} \qquad\begin{minipage}{0.45\textwidth}
		\begin{proof}
	\hypo{d}{\meta{A}\eor\meta{B}}
	\open
	\hypo{nc}{\enot\meta{C}}
	\have[\ ]{}{\vdots}
	\open
	\hypo[i]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{c1}{\meta{C}}
	\close
	\have{na}{\enot\meta{A}}\ni{a-c1,a-nc}
	\have[k]{b}{\meta{B}}\by{DS}{d,na}
	\have[\ ]{}{\vdots}
	\have[l]{c2}{\meta{C}}
	\close
	\have[\ ]{c}{\meta{C}}\ne{nc-c2,nc-nc}
\end{proof}
		\caption{Schematic proof using DS to emulate {\eor}E.\label{DSb}}
\end{minipage}
	\end{figure}

There are pros and cons to using DS as our disjunction elimination rule. You can already see that proof by cases is much clunkier – though perhaps you are not too concerned (maybe you suspect proof by cases is not much used in natural argumentation anyway). 
\paragraph{Pro} Using DS as an elimination rule for {\eor} has the nice feature that we eliminate a disjunction in favour of one of its disjuncts, rather than the unprecedented \meta{C} that appears as if from nowhere in the original {\eor}E rule. We also dispense with the use of subproofs in the statement of the disjunction rules.
\paragraph{Con} Adopting DS as a basic rule destroys the nice feature of our standard rules that only one connective is used in any rule. DS needs both disjunction and negation. We cannot, therefore, consider a system which lacks negation rules but has disjunction – the rules are no longer \emph{modular}. This is not especially important for \TFL, but could be important if you go on to consider other logical systems which may vary the rules for one connective independently of all the others. DS shackles negation and disjunction together – even arguments which have, intuitively nothing to do with negation, end up having to use negation in their proof. This can be illustrated by considering proofs that $(A\eor B)\proves (B \eor A)$. The proofs themselves are left for an exercise, but you can see that the proof in a system which has DS as a basic rule makes unavoidable use of negation rules, while the proof in our standard system uses only disjunction rules.

\section{Doing Without Negation Introduction} 

Strangely enough, we don't even need our negation introduction rule. Negation elimination, in the presence of our conditional and conjunction rules, suffices. The negation introduction rule discharges an assumption that \meta{A} when it can be shown to lead to both \meta{B} and $\enot\meta{B}$, and allows us to conclude that $\enot\meta{A}$. To emulate it, we need a rule that will discharge \meta{A}, and then use a combination of rules to deduce $\enot\meta{A}$. We have a subproof that shows \meta{A} to lead to contrary sentences, and we don't want to get rid of that important information. So the natural discharging rule to appeal to is conditional introduction: this enables us to capture the content of that subproof for later use. We cannot introduce a negation to obtain $\meta{A}$, but we can eliminate a negation from $\enot\enot\meta{A}$, if we can show that $\enot\enot\meta{A}$ leads to \meta{A}, which then leads to the contrary sentences again. Here is the whole schematic proof:
\begin{proof}
	\have[\ ]{}{\vdots}
	\open
	\hypo[k]{a}{\meta{A}}
	\have[\ ]{}{\vdots}
	\have[j]{b}{\meta{B}}
	\have{nb}{\enot\meta{B}}
	\have{bnb}{\meta{B}\eand\enot\meta{B}}\ai{b,nb}
	\close
	\have{abn}{\meta{A}\eif(\meta{B}\eand\enot\meta{B})}\ci{a-bnb}
	\open
	\hypo{nna}{\enot\enot\meta{A}}
	\open
	\hypo{na}{\enot\meta{A}}
	\have{rna}{\enot\enot\meta{A}}\by{R}{nna}
	\close
	\have{a2}{\meta{A}}\ne{na-na,na-rna}
	\have{bnb2}{\meta{B}\eand\enot\meta{B}}\ce{abn,a2}
	\have{b2}{\meta{B}}\ae{bnb2}
	\have{nb2}{\enot\meta{B}}\ae{bnb2}
	\close
	\have{na2}{\enot\meta{A}}\ne{nna-b2,nna-nb2}
\end{proof}
One thing to note about this schematic proof is that it is much longer and more complicated than our negation introduction rule. It also relies essentially on rules for the conditional and conjunction, violating our desire that each of our rules be `pure' in the sense that the introduction and elimination of a connective should ideally only involve sentences with it as the main connective. So we will not be availing ourselves of the possible economy of getting rid of the negation introduction rule.

Another interesting thing here is that the negation elimination rule seems to be twinned with the conditional introduction rule. This hints at quite a deep fact, namely, that negation itself can be understood as a disguised conditional. Some alternative formulations of sentential logic include a sentential constant, $\bot$. This is like a sentence letter, but it has a constant truth value in every valuation: it is always F. Given this constant value, we can see that $\enot\meta{A}$ and $\meta{A}\eif\bot$ are logically equivalent in such systems: 
\begin{center}
\begin{tabular}{c|c|ccc} \toprule 
\meta{A} & \enot\meta{A} & $\meta{A}$&$\eif$  &$\bot$ \\
\midrule
T & \textbf{F} & T & \textbf{F} & F\\
F & \textbf{T} & F & \textbf{T} & F\\
\bottomrule \end{tabular}
\end{center}
In this sort of system, we can understand the negation introduction rule as literally a special case of conditional introduction: if we can show \meta{B} and $\meta{B}\eif\bot$ in the scope of the assumption \meta{A}, then conditional elimination leads to $\bot$, and conditional introduction gives us $\meta{A} \to\bot$ while discharging the assumption that \meta{A}. We still need some equivalent to a negation elimination rule, because we need some way of reducing $(\meta{A}\eif\bot)\eif\bot$ to the logically equivalent $\meta{A}$. While it is philosophically interesting and conceptually elegant to treat negation as implication of a logical falsehood, that is not our preferred understanding of negation, and we will leave this sort of system alone. 


\practiceproblems
\problempart Consider an alternative proof system which drops our negation introduction rule, but adopts \emph{tertium non datur} (§\ref{aprtnd}) in its place. Is this alternative system equivalent to our standard system – in particular, can you show how to emulate negation introduction using just negation elimination, \emph{tertium non datur} (and structural rules like reiteration)?

\problempart Construct two proofs showing that $(A\eor B)\proves (B \eor A)$, the first using our standard natural deduction system, the second using the system which has DS in place of disjunction elimination. Comment on any points of interest.



% \begin{multicols}{2}\noindent
% 	\begin{proof}
% 		\hypo{ab}{(A \eor B)}
% 		\open
% 		\hypo{a}{A}
% 		\have{c1}{(B \eor A)}\oi{a}
% 		\close
% 		\open
% 		\hypo{b}{B}
% 		\have{c2}{(B \eor A)}\oi{b}
% 		\close
% 		\have{c}{(B \eor A)}\oe{ab,a-c1,b-c2}
% 	\end{proof}

% 	\begin{proof}
% 		\hypo{ab}{(A \eor B)}
% 		\open
% 		\hypo{nc}{\enot (B \eor A)}
% 		\open
% 		\hypo{a}{A}
% 		\have{c1}{(B \eor A)}\oi{a}
% 		\have{cr}{\enot(B \eor A)}\by{R}{nc}
% 		\close
% 		\have{na}{\enot A}\ni{a-c1,a-cr}
% 		\have{b}{B}\by{DS}{ab,na}
% 		\have{c2}{(B \eor A)}\oi{b}
% 		\have{nr}{\enot (B \eor A)}\by{R}{nc}
% 		\close
% 		\have{c}{(B \eor A)}\ne{nc-c2,nc-nr}
% 	\end{proof}
% \end{multicols}





% section replacing_disjunction_elimination_with_disjunctive_syllogism (end)