%!TEX root = forallx-adl.tex
\part{Natural Deduction for \FOL}
\label{ch.NDFOL}
\addtocontents{toc}{\protect\mbox{}\protect\hrulefill\par}

\chapter{Basic Rules for \textnormal{\FOL}}\label{s:BasicFOL}

\section{Proof-Theoretic Concepts in \FOL}



\FOL\ makes use of all of the connectives of \TFL. Helpfully, our natural deduction proof system for \FOL\ will simply import all of the basic rules from chapter \ref{ch.NDTFL}. (Obviously we will get all of the derived rules for free by doing this, but we won't make use of the derived rules.) We will define a correctly formed natural deduction proof for \FOL\ to be a structured sequence of sentences of \FOL\ such that each sentence is either an assumption or follows from the previous sentences by any of the \TFL\ rules \emph{or} by any of the new rules governing the quantifiers and identity that we will introduce in this chapter.\footnote{Though there is a category `formulae which are not sentences' in \FOL, no member of this class will ever appear in any correctly formed proof.} 

The notion of provability of \meta{A} from undischarged assumptions $\meta{C}_{1},…,\meta{C}_{n}$ was introduced for \TFL\ in §\ref{s:ProofTheoreticConcepts}. The rules for \FOL\ are different, but the earlier definitions go through unchanged, once we remember that the notion of proof in \FOL\ involves a sequence of \FOL\ sentences justified by the rules for \TFL\ \emph{and} those for \FOL. 

So in what follows I will make use of the single turnstile `$\proves$' to mean that there is a correctly formed proof using only the rules of \TFL\ and \FOL. (And once we introduce the identity rules in §\ref{ch.identity}, I will tacitly assume that proofs can make use of those rules too in justifying a claim using `$\proves$'.) 

Likewise, the notions of theoremhood, provable equivalence, and joint contrariness all carry over their definitions, because they are defined in terms of the single turnstile.


Some proofs in \FOL\ don't need any new rules. Consider this: $$\enot(\forall x Px \eor \exists y Py) \ttherefore \enot \forall x Px.$$ \begin{proof}
	\hypo{a}{\enot(\forall x Px \eor \exists y Py)}
	\open
	\hypo{b}{\forall x Px}
	\have{e}{(\forall x Px \eor \exists y Py)}\oi{b}
	\have{ar}{\enot(\forall x Px \eor \exists y Py)}\by{R}{a}
	\close
	\have{c}{\enot \forall x Px}\nintro{b-e,b-ar}
\end{proof} The sentences on each line are \FOL\ sentences that are not sentences of \TFL, but the main connectives involved are just those governed by the rules we already introduced to handle \TFL\ proofs.

However, not every \FOL\ sentence has a \TFL\ connective as its main connective. So we will also need some new basic rules to govern the quantifiers, and to govern the identity sign, to deal with those sentences where the main connective is a quantifier and where the sentence is an identity predication.


\section{Universal Elimination}\label{unielim}

Holding fixed the claim that everything is F, you can conclude that any particular thing is F. \emph{You name it; it's F.} The same is true for many-place predicates: if every human is shorter than 3km tall, then Amy is shorter than 3km tall, and Bob is, and Jonquil is,  and everyone else you can name.

Accordingly, the following reasoning should be fine for the corresponding symbolisations in \FOL:
\begin{proof}
	\hypo{a}{\forall xRxxd}
	\have{c}{Raad} \Ae{a}
\end{proof}
We obtained line 2 by dropping the universal quantifier and replacing every instance of `$x$' with `$a$'. Equally, the following should be allowed:
\begin{proof}
	\hypo{a}{\forall xRxxd}
	\have{c}{Rddd} \Ae{a}
\end{proof}
We obtained line 2 here by dropping the universal quantifier and replacing every instance of `$x$' with `$d$'. We could have done the same with any other name we wanted. 

This motivates the \define{universal elimination} rule ($\forall$E), using the notion for uniform substitution we introduced in §\ref{fol.truth.quant}:
\factoidbox{
\begin{proof}
	\have[m]{a}{\forall \meta{x}\meta{A}}
	\have[\ ]{}{\vdots}
	\have[\ ]{c}{\meta{A}\subs{\meta{c}}{\meta{x}}} \Ae{a}
\end{proof}
Where \meta{c} can be any name.}
The intent of the rule is that you can obtain any \emph{substitution instance} of a universally quantified formula: replace every occurrence of the free variable \meta{x} in \meta{A} with any chosen name. (If there are any – the rule is also good when \meta{A} has no free variable, because then the quantifier $\forall\meta{x}$ is redundant.) Remember here that the expression `\meta{c}' is a metalanguage variable over names: you are not required to replace the variable \meta{x} by the \FOL\ name `$c$', but you can select any name you like!

I should emphasise that (as with every elimination rule) you can only apply the $\forall$E rule when the universal quantifier is the main connective. Thus the following is outright banned:
\begin{proof}
	\hypo{a}{(\forall x Bx \eif Bk)}
	\have{c}{(Bb \eif Bk)}\by{naughtily attempting to invoke $\forall$E}{a}
\end{proof}
This is illegitimate, since `$\forall x$' is not the main connective in line 1. (If you need a reminder as to why this sort of inference should be banned, reread §\ref{s:MoreMonadic}.)

Here is an example of the rule in action. Suppose we wanted to show that $\forall x \forall y (Rxx \eif Rxy), Raa  \ttherefore Rab$ is provable. The proof might go like this: \begin{proof}
	\hypo{a}{\forall x \forall y (Rxx \eif Rxy)}
	\hypo{b}{Raa}
	\have{c}{\forall y (Raa \eif Ray)}\Ae{a}
	\have{d}{Raa \eif Rab}\Ae{c}
	\have{e}{Rab}\ce{d,b}
\end{proof}
Here on line 3 we substitute the previously used name `$a$' for the variable `$x$' in `$\forall y (Rxx \eif Rxy)$'; and then on line 4 we substitute the new name `$b$' for the variable `$y$' in `$Raa \eif Ray$'. The rule of universal elimination doesn't discriminate between new and old names.


\section{Existential Introduction}\label{exint}
Given the assumption that some specific named thing is an F, you can conclude that something is an F: `Sylvester reads, so someone reads' seems like a conclusive argument. So we ought to allow the inference from a claim about some particular thing being F, to a general claim that something or other is F:
\begin{proof}
	\hypo{a}{Raad}
	\have{b}{\exists x Raax} \Ei{a}
\end{proof}
Here, we have replaced the name `$d$' with a variable `$x$', and then existentially quantified over it. Equally, we would have allowed:
\begin{proof}
	\hypo{a}{Raad}
	\have{c}{\exists x Rxxd} \Ei{a}
\end{proof}
Here we have replaced both instances of the name `$a$' with a variable, and then existentially generalised. 

There are some pitfalls with this description of what we have done. The following argument is invalid: `Someone loves Alice; so someone is such that someone loves themselves'. So we ought not to be able to conclude `$\exists x \exists x Rxx$' from `$\exists x Rxa$'. Accordingly, our rule cannot be \emph{replace a name by a variable, and stick a corresponding quantifier out the front} – since that would would permit the proof of the invalid argument.

We take our cue from the $\forall$E rule. This rule says: take a sentence $\forall\meta{xA}$, then we can remove the quantifier and substitute an arbitrary name for some free variable in the formula \meta{A} (assuming there is one). The $\exists$I rule is in some sense a mirror image of this rule: it allows us to move from a sentence with an arbitrary name – that might be thought of as the result of substituting a name for a free variable in some formula \meta{A} – to a quantified sentence $\exists \meta{xA}$. So here is how we formulate our rule of \define{existential introduction}:
\factoidbox{
	\begin{proof}
		\have[m]{c}{\meta{A}\subs{\meta{c}}{\meta{x}}} 
		\have[\ ]{}{\vdots}
			\have[\ ]{a}{\exists \meta{x}\meta{A}} \Ei{c}
	\end{proof}
} So really we should think that the proof just above should be thought of as concluding $\exists x Rxxd$ from `$Rxxd$'$\subs{a}{x}$ (i.e., `$Raad$').

If we have this rule, we cannot provide a proof of the invalid argument. For `$\exists x Rxa$' is not a substitution instance of `$\exists x \exists x Rxx$' – both instances of `$x$' in `$Rxx$' are bound by the second existential quantifier, so neither is free to be substituted. So the premise is not of the right form for the rule of $\exists$I to apply. 

On the other hand, this proof is correct:
\begin{proof}
	\hypo{a}{Raa}
	\have{b}{\exists x Rax}\Ei{a}
\end{proof} Why? Because the assumption `$Raa$' is in fact not only a substitution instance of $\exists x Rxx$, but also a substitution instance of `$\exists x Rax$', since `$Rax$'$\subs{a}{x}$ is just `$Raa$' too. So we can vindicate the intuitively correct argument `Narcissus loves himself, so there is someone who loves Narcissus'. 

As we just saw, applying this rule requires some skill in being able to recognise substitution instances. Thus the following is allowed:
\begin{proof}
	\hypo{a}{Raad}
	\have{d}{\exists x Rxad} \Ei{a}
	\have{e}{\exists y \exists x Rxyd} \Ei{d}
\end{proof} This is okay, because `$Raad$' can arise from substitition of `$a$' for `$x$' in `$Rxad$', and `$\exists x Rxad$' can arise from substitition of `$a$' for `$y$' in `$\exists x Rxyd$'.
But this is banned:
\begin{proof}
	\hypo{a}{Raad}
	\have{d}{\exists x Rxad} \Ei{a}
	\have{e}{\exists x \exists x Rxxd}\by{naughtily attempting to invoke $\exists$I}{d}
\end{proof} This is because `$\exists x Rxad$' is not a substitution instance of `$\exists x \exists x Rxxd$', since (again) both occurrences of `$x$' in `$Rxxd$' are already bound and so not available for free substitution.

Here is an example which shows our two proof rules in action, a proof showing that $$\forall x \forall y (Rxy \eand Ryx) \proves \exists x Rxx\text{:}$$ \begin{proof}
	\hypo{a}{\forall x \forall y (Rxy \eand Ryx)}
	\have{b}{\forall y (Ray \eand Rya)}\Ae{a}
	\have{c}{(Raa \eand Raa)}\Ae{b}
	\have{d}{Raa}\ae{c}
	\have{e}{\exists x Rxx}\Ei{d}
\end{proof}

For another example, consider this proof of `$\exists x (Px \eor \enot Px)$' from no assumptions:\phantomsection\label{exexmid}
\begin{proof}
	\open\hypo{a}{\enot(Pd \eor \enot Pd)}
	\open\hypo{b}{\enot Pd}
	\have{c}{(Pd \eor \enot Pd)}\oi{b}
	\have{ar}{\enot(Pd \eor \enot Pd)}\by{R}{a}
	\close
	\have{c1}{Pd}\nelim{b-c,b-ar}
	\have{e}{(Pd \eor \enot Pd)}\oi{c1}
	\have{arr}{\enot(Pd \eor \enot Pd)}\by{R}{a}
	\close
	\have{f}{(Pd \eor \enot Pd)}\nelim{a-e,a-arr}
	\have{con}{\exists x (Px \eor \enot Px)}\Ei{f}
\end{proof}
One final example, a proof that $\forall x \forall y (Rxy \eif Ryx) \proves \forall x (\forall y Rxy \eif \exists y Ryx)$:
 \begin{proof}
        \hypo{a}{\forall x \forall y (Rxy \eif Ryx)}
        \have{b}{\forall y (Ray \eif Rya)}\Ae{a}
        \have{c}{(Rab \eif Rba)}\Ae{b}
        \open
            \hypo{d}{\forall y Ray}
            \have{e}{Rab}\Ae{d} 
            \have{f}{Rba}\ce{c,e} 
            \have{g}{\exists y Rya}\Ei{f} 
            \close
        \have{h}{(\forall y Ray \to \exists y Rya)}\ci{d-g}
        \have{i}{\forall x(\forall y Rxy \to \exists y Ryx)}\Ai{h}
    \end{proof}


\section{Empty Domains}
The following proof combines our two new rules for quantifiers:
	\begin{proof}
		\hypo{a}{\forall x Fx}
		\have{in}{Fa}\Ae{a}
		\have{e}{\exists x Fx}\Ei{in}
	\end{proof}
Could this be a bad proof? If anything exists at all, then certainly we can infer that something is F, from the fact that everything is F. But what if \emph{nothing} exists at all? Then it is surely vacuously true that everything is F; however, it ought not follow that something is F, for there is nothing to \emph{be} F. So if we claim that, as a matter of logic alone, `$\exists x Fx$' follows from `$\forall x Fx$', then we are claiming that, as a matter of \emph{logic alone}, there is something rather than nothing. This might strike us as a bit odd.

Actually, we are already committed to this oddity. In §\ref{s:FOLBuildingBlocks}, we stipulated that domains in \FOL\ must have at least one member. We then defined a logical truth (of \FOL) as a sentence which is true in every interpretation. Since `$\exists x\ x=x$' will be true in every interpretation, this \emph{also} had the effect of stipulating that it is a matter of logic that there is something rather than nothing.

Since it is far from clear that logic should tell us that there must be something rather than nothing, we might well be cheating a bit here. 

If we refuse to cheat, though, then we pay a high cost. Here are three things that we want to hold on to:
	\begin{itemize}
		\item $\forall x Fx \proves Fa$: after all, that was $\forall$E.
		\item $Fa \proves \exists x Fx$: after all, that was $\exists$I.
		\item the ability to copy-and-paste proofs together: after all, reasoning works by putting lots of little steps together into rather big chains.
	\end{itemize}
If we get what we want on all three counts, then we have to countenance that $\forall xFx \proves \exists x Fx$. So, if we get what we want on all three counts, the proof system alone tells us that there is something rather than nothing. And if we refuse to accept that, then we have to surrender one of the three things that we want to hold on to!

In fact the choice is even starker. Consider this proof: \begin{proof}
	\open
	\hypo{fa}{Fa}
	\have{far}{Fa}\by{R}{fa}
	\close
	\have{fafa}{(Fa \eif Fa)}\ci{fa-far}
	\have{efx}{\exists x (Fx \eif Fx)}\Ei{fafa}
\end{proof} This proof uses only the obvious rule of conditional introduction, and our existential introduction rule. It terminates in a claim that a certain thing exists: a thing that is $F$ if it is $F$, and has no undischarged assumptions. Again the existence of something is a theorem of our logic. The real source of the existential commitment here seems to be the use of the name `$a$', because our rules implicitly assume that every name has a referent, and hence as soon as you use a name you assume that there is something in the domain for the name to latch on to.

Before we start thinking about which to surrender,\footnote{In light of the second proof, many will opt for restricting $\exists$I. If we permit an empty domain, we will also need `empty names' – names without a referent. When the name \meta{c} is empty, it seems problematic to conclude from `$\meta{c}$ is F' that there is something which is F. (Does `Santa Claus drives a flying sleigh' entail `Someone drives a flying sleigh'?) But empty names are not cost-free; understanding how a name that doesn't name anything can have any meaning at all has vexed many philosophers and linguists.} we might want to ask how \emph{much} of a cheat this is. Granted, it may make it harder to engage in theological debates about why there is something rather than nothing. But the rest of the time, we will get along just fine. So maybe we should just regard our proof system (and \FOL, more generally) as having a very slightly limited purview. If we ever want to allow for the possibility of \emph{nothing}, then we shall have to cast around for a more complicated proof system. But for as long as we are content to ignore that possibility, our proof system is perfectly in order. (As, similarly, is the stipulation that every domain must contain at least one object.)



\section{Universal Introduction}\label{uniint}
Suppose you had shown of each particular thing that it is F (and that there are no other things to consider). Then you would be justified in claiming that everything is F. This would motivate the following proof rule. If you had established each and every single substitution instance of `$\forall x Fx$', then you can infer `$\forall x Fx$'. 

Unfortunately, that rule would be utterly unusable. To establish each and every single substitution instance would require proving `$Fa$', `$Fb$', $…$, `$Fj_2$', $…$, `$Fr_{79002}$', $…$, and so on. Indeed, since there are infinitely many names in \FOL, this process would never come to an end. So we could never apply that rule. We need to be a bit more cunning in coming up with our rule for introducing universal quantification. 

Our cunning thought will be inspired by considering:
$$\forall x Fx \ttherefore\ \forall y Fy$$
This argument should \emph{obviously} be valid. After all, alphabetical variation in choice of variables ought to be a matter of taste, and of no logical consequence. But how might our proof system reflect this? Suppose we begin a proof thus:
\begin{proof}
	\hypo{x}{\forall x Fx} 
	\have{a}{Fa} \Ae{x}
\end{proof}
We have proved `$Fa$'. And, of course, nothing stops us from using the same justification to prove `$Fb$', `$Fc$', $…$, `$Fj_2$', $…$, `$Fr_{79002}, …$, and so on until we run out of space, time, or patience. But reflecting on this, we see that this is a way to prove $F\meta{c}$, for any name \meta{c}. And if we can do it for \emph{any} thing, we should surely be able to say that `$F$' is true of \emph{everything}. This therefore justifies us in inferring `$\forall y Fy$', thus:
\begin{proof}
	\hypo{x}{\forall x Fx}
	\have{a}{Fa} \Ae{x}
	\have{y}{\forall y Fy} \Ai{a}
\end{proof}
The crucial thought here is that `$a$' was just some \emph{arbitrary} name. There was nothing special about it – we might have chosen any other name – and still the proof would be fine. And this crucial thought motivates the universal introduction rule ($\forall$I):
\factoidbox{
\begin{proof}
	\have[m]{a}{\meta{A}\subs{\meta{c}}{\meta{x}}}
	\have[\ ]{}{\vdots}
	\have[\ ]{c}{\forall \meta{x}\meta{A}} \Ai{a}
\end{proof}
	\meta{c} must not occur in any undischarged assumption, or elsewhere in \meta{A}}

A crucial aspect of this rule, though, is bound up in the accompanying constraint. In English, a name like `Sylvester' can play two roles: it can be introduced as a name for a specific thing (`let me dub thee Sylvester'!), or as an \emph{arbitrary name}, introduced by this sort of stipulation: \emph{let `Sylvester' name some arbitrarily chosen man}. The name doesn't tell us, when it subsequently appears, whether it was introduced in one way or the other. But if it was introduced as an arbitrary name, then any conclusions we draw about this Sylvester aren't really dependent on the particular arbitrarily chosen referent – they all depend rather on the stipulation used in introducing the name, and so (specifically) they will all be consequences of the only fact we know for sure about this Sylvester, that he is male. If all men are mortal, then an arbitrarily chosen man, whom we temporarily call `Sylvester', is mortal. If Sylvester is mortal, then there is a date he will die. But since he was selected arbitrarily, without reference to any further particulars of his life, then for any man, there exists a date he will die. And that is appropriate reasoning from a universal generalisation, to another generalisation, via claims about a specific but arbitrarily chosen person.\footnote{The details about how this sort of arbitrary reference works are interesting. A controversial but nevertheless attractive view of how it might work is  Wylie Breckenridge and Ofra Magidor (2012) `Arbitrary Reference', \emph{Philosophical Studies} \textbf{158}, pp.\ 377–400.}

An informal example of this sort of reasoning from arbitrary names is this: \begin{quote}
	Consider an arbitrary somebody who travelled from London to Munich in 2016. Call them J Doe. \begin{itemize}
		\item  If J Doe took the train, then they had to go via Paris, and that leg of the journey alone takes 3 hours.
		\item  If J Doe flew, then they would have spent at least an hour in airport transfers at each end, even setting aside the flight time itself.
		\item The other options – driving, walking, etc., – are all even slower.
	\end{itemize}
So J Doe’s journey took over two hours in every possible case. Therefore –
since J Doe is an arbitrary person – \emph{every} traveller’s journey from London to Munich in 2016 took over two hours.
\end{quote}


We don't have stipulations like the above to introduce a name as an arbitrary name in \FOL. But we do have a way of ensuring that the name has no prior associations other than those linked to a prior universal generalisation, if we insist that,  when the name is about to be eliminated from the proof, no assumption about what that name denotes is being relied on. That way, we can know that however it was introduced to the proof, it was not done in a way that involved making specific assumptions about whatever the name arbitrarily picks out. \factoidbox{If you can conclude something about a named object that doesn’t involve making any assumptions about it \emph{other than assumptions which we are making more generally}, then you can conclude that same something about everything.}

The simplest way for ensure that a name is not subject to any specific assumptions is if the name was introduced by an application of $\forall$E, as an arbitrary name in the standard sense. But there are other ways too. In general what we need is that the name not occur in the range of any assumption which uses the name. If the name has been introduced without making any assumptions about what it denotes, then we are not relying on any special features of what the name happens to denote when we conclude that if this arbitrary thing is F, then everything is F. 

Consider the following proof to see how this works in action. \begin{proof}
    	\hypo{a}{\forall x{(Ax \wedge Bx)}}
    	\have{Aa}{Aa\wedge Ba}\Ae{a}
    	\have{aa}{Aa}\ae{Aa}
    	\have{fa}{\forall x Ax}\Ai{aa}
    \end{proof}
The crucial step is applying the $\forall$I rule  to the name `$a$' on the last line. While the name `$a$' does appear on lines 2 and 3, it doesn't occur in the assumption – it was introduced on line 2 as an \emph{arbitrary instance} of the universal assuption. 

This constraint ensures that we are always reasoning at a sufficiently general level. To see the importance of the constraint in action, consider this terrible argument:
	\begin{quote}
		Everyone loves Kylie Minogue; therefore everyone loves themselves.
	\end{quote}
We might symbolise this obviously invalid inference pattern as:
$$\forall x Lxk \ttherefore \forall x Lxx$$
Now, suppose we tried to offer a proof that vindicates this argument:
\begin{proof}
	\hypo{x}{\forall x Lxk}
	\have{a}{Lkk} \Ae{x}
	\have{y}{\forall x Lxx} \by{naughtily attempting to invoke $\forall$I}{a}
\end{proof}\noindent
This is not allowed, because `$k$' occurred already in an undischarged assumption, namely, on line 1. The crucial point is that, if we have made any assumptions about the object we are working with (including assumptions embedded in \meta{A} itself), then we are not reasoning generally enough to license  the use of $\forall$I.

Although the name may not occur in any \emph{undischarged} assumption, it may occur as a discharged assumption. That is, it may occur in a subproof that we have already closed. For example:
\begin{proof}
	\open
		\hypo{f1}{Gd}
		\have{f2}{Gd}\by{R}{f1}
	\close
	\have{ff}{Gd \eif Gd}\ci{f1-f2}
	\have{zz}{\forall z(Gz \eif Gz)}\Ai{ff}
\end{proof}
This tells us that `$\forall z (Gz \eif Gz)$' is a \emph{theorem}. And that is as it should be. 

Here is another proof featuring an application of $\forall$I after discharging an assumption about some name `$a$':
\begin{proof}
	\open
	\hypo{a}{Fa \eand \enot Fa}
	\have{b}{Fa}\ae{a}
	\have{c}{\enot Fa}\ae{a}
	\close
	\have{d}{\enot(Fa \eand \enot Fa)}\nintro{a-b,a-c}
	\have{e}{\forall x \enot(Fx \eand \enot Fx)}\Ai{d}
\end{proof} Here we were able to derive that something could not be true of $a$, no matter what $a$ is. We cannot make a coherent assumption that $a$ is both $F$ and isn't $F$, so it doesn't really matter what `$a$' denotes. So the open sentence `$\enot(F \eand \enot Fx)$' could not be true of anything at all. That is why we are entitled to discharge that assumption, and then any subsequent use of `$a$' in the proof must be depending not on particular facts about this $a$, but about anything at all, including whatever it is that `$a$' happens to pick out. 

You might wish to recall the proof of `$\exists x (Px \eor \enot Px)$' from page \pageref{exexmid}. Note that, by the second-last line, we had already discharged any assumption which relied on the specific name chosen (in that case, `$d$'). The existential introduction rule has no constraints on it, so that it was not necessary to discharge any assumptions using the name before applying that rule. But we see now that, since those assumptions were in fact discharged, we could have applied universal introduction at that second last line, to yield a proof of `$\forall x (Px \eor \enot Px)$'.

We can also use our universal rules together to show some things about how quantifier order doesn't matter, when the strings of quantifiers are of the same type. For example $$\forall x \forall y \forall z Syxz \ttherefore \forall z \forall y \forall x Syxz$$ can be proved as follows: \begin{proof}
	\hypo{a}{\forall x \forall y \forall z Syxz}
	\have{b}{\forall y \forall z Syaz}\Ae{a}
	\have{c}{\forall z Sbaz}\Ae{b}
	\have{d}{Sbac}\Ae{c}
	\have{e}{\forall x Sbxc}\Ai{d}
	\have{f}{\forall y \forall x Syxc}\Ai{e}
	\have{g}{\forall z \forall y \forall x Syxz}\Ai{f}
\end{proof} Here we successively eliminate the quantifiers in favour of arbitrarily chosen names, and then reintroduce the quantifiers (though in a different order). The only undischarged assumption throughout the proof is the first line, with no names at all, so all of the uses of universal introduction are acceptable.


\section{Existential Elimination}\label{exelim}
Suppose we know that \emph{something} is F. The problem is that simply knowing this does not tell us which thing is F. So it would seem that from `$\exists x Fx$' we cannot immediately conclude `$Fa$', `$Fe_{23}$', or any other substitution instance of the sentence. What can we do?

Suppose we know that something is F, and that everything which is F is G. In (almost) natural English, we might reason thus:
	\begin{quote}
		Since something is F, there is some particular thing which is an F. We do not know anything about it, other than that it's an F, but for convenience, let's call it `Obbie'. So: Obbie is F. Since everything which is F is G, it follows that Obbie is G. But since Obbie is G, it follows that something is G. And nothing depended on which object, exactly, Obbie was – no matter which F we picked for `Obbie' to denote, it would have been G. So, as long as something is F, then something is G.
	\end{quote}
This is a kind of generic \emph{proof by cases} – a generalisation of the rule of disjunction elimination. This is because an existential claim is actually kind of like a generalised disjunction: $\exists \meta{xF}$ is true iff either the first thing in the domain is \meta{F}, or the second thing in the domain is \meta{F}, or…. Of course it cannot \emph{really} be a disjunction, since in large domains there is no way to even enumerate all the individuals, let alone construct an infinite sentence disjoining the claims that each of them is \meta{F}.

Rather than attempting to push the analogy with proof by cases too far, and attempting to enumerate all possible cases of F, we can make use of the device of arbitrary names again to reason \emph{generically} about all those cases without having to enumerate them. Just like a proof by cases, we eliminate our existential assumption by showing that some one thing follows from each potential case, which here involves showing that thing follows from the generic hypothesis about an arbitrary F. 

We try to capture this reasoning pattern in a proof as follows:
\begin{proof}
	\hypo{es}{\exists x Fx}
	\open\hypo{ast}{\forall x(Fx \eif Gx)}
	\open
		\hypo{s}{Fo}
		\have{st}{Fo \eif Go}\Ae{ast}
		\have{t}{Go} \ce{st, s}
		\have{et1}{\exists x Gx}\Ei{t}
	\close
	\have{et2}{\exists x Gx}\Ee{es,s-et1}
\end{proof}\noindent
Breaking this down: we started by writing down our assumptions. At line 3, we made an additional assumption: `$Fo$'. This was just a substitution instance of `$\exists x Fx$'. On this assumption, we established `$\exists x Gx$'. But note that we had made no \emph{special} assumptions about the object named by `$o$'; we had \emph{only} assumed that it satisfies `$Fx$'. So nothing depends upon which object it is. And line 1 told us that \emph{something} satisfies `$Fx$'. So our reasoning pattern was perfectly general. We can discharge the specific assumption `$Fo$', and simply infer `$\exists x Gx$' on its own.

Putting this together, we obtain the existential elimination rule ($\exists$E):
\factoidbox{
\begin{proof}
	\have[m]{a}{\exists \meta{x}\meta{A}}
	\open	
		\hypo[i]{b}{\meta{A}\subs{\meta{c}}{\meta{x}}}
		\have[\ ]{}{\vdots}
		\have[j]{c}{\meta{B}}
	\close
	\have[\ ]{}{\vdots}
	\have[\ ]{d}{\meta{B}} \Ee{a,b-c}
\end{proof}
\meta{c} must not occur in any assumption undischarged before line $i$\\
\meta{c} must not occur in $\exists \meta{x}\meta{A}$\\
\meta{c} must not occur in \meta{B}}
As with universal introduction, the constraints are extremely important. To see why, consider the following terrible argument:
	\begin{quote}
		Tim Button is a lecturer. There is someone who is not a lecturer. So Tim Button is both a lecturer and not a lecturer.
	\end{quote}
We might symbolise this obviously invalid inference pattern as follows:
$$Lb, \exists x \enot Lx \ttherefore Lb \eand \enot Lb$$
Now, suppose we tried to offer a proof that vindicates this argument:
\begin{proof}
	\hypo{f}{Lb}
	\hypo{nf}{\exists x \enot Lx}	
	\open	
		\hypo{na}{\enot Lb}
		\have{con}{Lb \eand \enot Lb}\ae{f, na}
	\close
	\have{econ1}{Lb \eand \enot Lb}\by{naughtily attempting to invoke $\exists$E }{nf, na-con}
\end{proof}
The last line of the proof is not allowed. The name that we used in our substitution instance for `$\exists x \enot Lx$' on line 3, namely `$b$', occurs in line 4. And the following proof would be no better:
\begin{proof}
	\hypo{f}{Lb}
	\hypo{nf}{\exists x \enot Lx}	
	\open	
		\hypo{na}{\enot Lb}
		\have{con}{Lb \eand \enot Lb}\ae{f, na}
		\have{con1}{\exists x (Lx \eand \enot Lx)}\Ei{con}		
	\close
	\have{econ1}{\exists x (Lx \eand \enot Lx)}\by{naughtily attempting to invoke $\exists$E }{nf, na-con1}
\end{proof}
The last line of the proof would still not be allowed. For the name that we used in our substitution instance for `$\exists x \enot Lx$', namely `$b$', occurs in an undischarged assumption, namely line 1. 

The moral of the story is this. \factoidbox{If you want to squeeze information out of an existentially quantified claim $\exists\meta{x}\meta{A}$, choose a \emph{new} name, never before used in the proof, to substitute for the variable in \meta{A}.} That way, you can guarantee that you meet all the constraints on the rule for $\exists$E. A new name functions like an arbitrary name – it carries no prior baggage with it, apart from what we stipulate or assume to hold of it. 

Here's an example using this newly introduced rule: $\exists x (Fx \eand Gx) \proves (\exists x Fx \eand \exists x Gx)$: 
\begin{proof}
    	\hypo{e}{\exists x (Fx \eand Gx)}
    	\open
    	\hypo{ei}{Fa \eand Ga}
    	\have{fa}{Fa}
    	\have{f}{\exists x Fx}\Ei{fa}
    	\have{gb}{Ga}
    	\have{i}{\exists x Gx}\Ei{gb}
    	\have{d}{\exists x Fx \eand \exists x Gx}\ai{f,i}
    	\close
    	\have{con}{\exists x Fx \eand \exists x Gx}\Ee{e,ei-d}
    \end{proof} The use of $\exists$E on the last line relies on the fact that we've gotten rid of any occurence of the new arbitrary name by the second last line. We have derived something that is generic from the generic existential assumption, so it is safe to conclude that generic claim holds regardless of the identity of the individual that makes the existential claim true.

An argument that makes use of both patterns of arbitrary reasoning is this example due to Breckenridge and Magidor: `from the premise that there is someone who loves everyone to the conclusion that everyone is such that someone loves them'. Here is a proof in our system, letting `$Lxy$' symbolise `\gap{1} loves \gap{2}', letting `$h$' symbolise the arbitrarily chosen name `Hiccup' and letting `$a$' symbolise the arbitrarily chosen name `Astrid': \begin{proof}
	\hypo{a}{\exists x \forall y Lxy}
	\open
		\hypo{b}{\forall y Lhy}
		\have{f}{Lha}\Ae{b}
		\have{e}{\exists x Lxa}\Ei{f}
		\have{d}{\forall y \exists x Lxy}\Ai{e}
	\close
	\have{c}{\forall y \exists x Lxy}\Ee{a,b-d}
\end{proof} At line 3, both our arbitrary names are in play – $h$ was newly introduced to the proof in line 2 as the arbitrary person Hiccup who witnesses the truth of `$\exists x \forall y Lxy$', and `$a$' at line 3 as an arbitrary person Astrid beloved by Hiccup. We can apply $\exists$I without restriction at line 4, which takes the name `$h$' out of the picture – we no longer rely on the specific instance chosen, since we are back at generalities about someone who loves everyone, being such that they also love the arbitrarily chosen someone Astrid. So we can safely apply $\forall$I at line 5, since the name `$a$' appears in no assumption nor in `$\forall y\exists x Lxy$'. But now we have at line 5 a claim that doesn't involve the arbitrary name `$h$' either, which was newly chosen to not be in any undischarged assumption or in $\exists x \forall y Lxy$. So we can safely say that the name Hiccup was just arbitrary, and nothing in the proof of `$\forall y\exists x Lxy$' depended on it, so we can discharge the specific assumption about $h$ that was used in the course of that proof and nevertheless retain our entitled ment to `$\forall y\exists x Lxy$'. 



\section{The Barber}\label{barber}

Let's try something a little more complicated, the so-called `Barber paradox': \begin{quote}
	 in a certain remote Sicilian village, approached by a long ascent up a precipitous mountain road, the barber shaves all and only those villagers who do not shave themselves. Who shaves the barber? If he himself does, then he does not (since he shaves only those who do not shave themselves); if he does not, then he indeed does (since he shaves all those who do not shave themselves).  The unacceptable supposition is that there is such a barber – one who shaves himself if and only if he does not. The story may have sounded acceptable: it turned our minds, agreeably enough, to the mountains of inland Sicily. However, once we see what the consequences are, we realize that the story cannot be true: there cannot be such a barber, or such a village. The story is unacceptable.\footnote{R M Sainsbury (2009) \emph{Paradoxes} 3rd ed., Cambridge University Press, pages 1–2.}
\end{quote} This uses some of our tricky quantifer rules, disjunction elimination (proof by cases) and negation introduction (\emph{reductio}), so it is really a showcase of many things we've learned so far. 

Let's first try to symbolise the argument. \begin{ekey}
	\item[\text{Domain}] residents of a certain remote Sicilian village
	\item[B]\gap{1} is a barber
	\item[S]\gap{1} shaves \gap{2}
\end{ekey}
The argument then revolves around the claim that there is a barber who shaves everyone who doesn't shave themselves. Semi-formally paraphrased: someone x exists such that x is a barber and for all people y: y does not shave themselves iff x shaves y. That is: $$\exists x (Bx \eand \forall y (\enot Syy \eiff Sxy)).$$ The argument takes the form of a \emph{reductio}, so we will begin the proof by assuming this claim for the sake of argument and see what happens: \begin{proof}
	\open
	\hypo{a}{\exists x (Bx \eand \forall y (\enot Syy \eiff Sxy))}
	\open
	\hypo{b}{(Ba \eand \forall y (\enot Syy \eiff Say))}
	\have{c}{\forall y (\enot Syy \eiff Sxy)}\ae{b}
	\have{d}{(\enot Saa \eiff Saa)}\Ae{c}
	\open
		\hypo{i}{\enot(P \eand \enot P)}
		\open 
		\hypo{e}{\enot Saa}
		\have{f}{Saa}\be{d,e}
		\have{g}{\enot Saa}\by{R}{e}
		\close
	\have{h}{Saa}\nelim{e-f,e-g}
	\have{k}{\enot Saa}\be{d,h}
		\close
	\have{l}{(P \eand \enot P)}\nelim{i-h,i-k}
	\close
	\have{m}{(P \eand \enot P)}\Ee{a,b-l}
	\have{n}{P}\ae{m}
	\have{o}{\enot P}\ae{m}
	\close
	\have{p}{\enot \exists x (Bx \eand \forall y (\enot Syy \eiff Sxy))}\nintro{a-n,a-o}
\end{proof}
One trick to this proof is to be sure to instantiate the universally quantified claim at line 3 by using the same name `$a$' as was already used in line 2. This is because, intuitively, the problem case for this supposed barber arises when you think about whether they shaves themselves or not. But themselves trickiest part of this proof occurs at lines 5–11. By line 4, we've already derived a contradictory biconditional. But if we just use it to derive `$Saa$' and `$\enot Saa$', the contradictory claims we obtain would end up involving the name `$a$'. That would mean we couldn't apply the $\exists$E rule, since the final line of the subproof would contain the chosen name, so we couldn't get our logical falsehood out of the subproof beginning on line 2, and hence could perform the desired \emph{reductio} on line 1 via {\enot}I. So our trick is to suppose the negation of an \emph{unrelated} logical falsehood on line 5, derive the logical falsehood from line 4 in the range of that assumption, and hence use {\enot}E to derive the logical falsehood `$P \eand \enot P$' on line 11. This doesn't contain the name `$a$', and hence can be extracted from the subproof to show that line 1 by itself suffices to derive a logical falsehood, and that shows the supposition that there is such a barber is a logical falsehood.


\section{Justification of these Quantifier Rules}

Above, I offered informal arguments for each of our quantifier rules that seem to exemplify the pattern of argument in the rule, and to be intuitively valid. But we can also offer justifications for our rules in terms of interpretations of the sentences involved, and the principles governing truth of quantified sentences introduced in §\ref{fol.truth.quant}. 

For example, consider any interpretation which makes $\forall\meta{x}\meta{A}$ true. In any such interpretation, there will be a nonempty domain, and every name will denote some member of this domain. $\forall\meta{x}\meta{A}$ is true just in case for any name we like, it will denote something of which $\meta{A}\subs{\meta{c}}{\meta{x}}$ is true. So in any such interpretation, for each name in the language \meta{c}, $\meta{A}\subs{\meta{c}}{\meta{x}}$ will also be true. So the proof rule of $\forall$E corresponds to a valid argument form.

For $\exists$E, the case is only a little more involved. Suppose $\exists\meta{x}\meta{A}$ is true in an interpretation. Then there is some interpretation, otherwise just like the original one, in which some new name $\meta{c}$ is assigned to some object in the domain, and where $\meta{A}\subs{\meta{c}}{\meta{x}}$ is true. Suppose that, in fact, every interpretation which makes $\meta{A}\subs{\meta{c}}{\meta{x}}$ true also makes  $\meta{B}$ true, where the new name \meta{c} does not appear in \meta{B}. Could \meta{B} be false in our original interpretation? No – for everything that appears in \meta{B} is already interpreted in the original interpretation, with the same interpretation as in the interpretation which makes it true. So it must be true in our original interpretation too. So $\meta{C}_{1},…,\meta{C}_{n},\exists\meta{x}\meta{A}\entails \meta{B}$ (when the name \meta{c} makes no appearance in any sentence in this argument), and the proof rule of $\exists$E corresponds to a valid argument form.

You may also offer arguments from intepretations to the effect that our other quantifier proof rules correspond to valid arguments in \FOL:\begin{itemize}
	\item $\meta{A}\subs{\meta{c}}{\meta{x}} \entails \exists \meta{x}\meta{A}$ – if \meta{c} is used in the proof, it must have an interpretation as something in the domain, and so something in the domain satisfies \meta{A};
	\item If this entailment holds: $$\meta{C}_{1},…,\meta{C}_{n} \entails \meta{A}\subs{\meta{c}}{\meta{x}},$$ where the name \meta{c} occurs nowhere among $\meta{C}_{i}$ or elsewhere in \meta{A}, then this entailment also holds: $$\meta{C}_{1},…,\meta{C}_{n} \entails \forall\meta{x}\meta{A}.$$ For we could have substituted any other name for \meta{c} and the original entailment would still have succeeded, since it could not have depended on the specific name chosen. So it doesn't matter what the interpretation of \meta{c} happens to be, and if that doesn't matter, it must be because everything is \meta{A}.
\end{itemize} 


So we are again comforted: our proof rules can never lead us from true assumptions to false claims, if correctly applied.



\keyideas{
	\item We augment our natural deduction proof system for \TFL\ by allowing \FOL\ sentences to occur in proofs, and adding rules governing quantifiers to go partway towards a natural deduction system for \FOL.
	\item The notation `$\proves$' for provability carries over from its earlier use in \TFL\ unchanged, once we understand that a proof can now use the new rules for the new logical connectives in \FOL.
	\item The rules for  $\forall$E and $\exists$I are straightforward and can be applied regardless of which names we deploy.
	\item But the other quantifier rules $\forall$I and $\exists$E  contain some important restrictions on which names we can use. These restrictions are motivated by considerations about arbitrary reference which inform us when we can introduce `dummy names' in the course of our proofs and what we can do with them.
	\item Our proof rules match the interpretation of \FOL\ we have given – they will not permit us to say that some claim is provable from some assumptions when that claim isn't entailed by those assumptions. 
}


\practiceproblems
\problempart
The following three `proofs' are \emph{incorrect}. Explain why they are incorrect. If the argument `proved' is invalid, provide an interpretation which shows that the assumptions involved do not entail the conclusion:
\begin{enumerate}
	\item 
	\begin{proof}
		\hypo{Rxx}{\forall x Rxx}
		\have{Raa}{Raa}\Ae{Rxx}
		\have{Ray}{\forall y Ray}\Ai{Raa}
		\have{Rxy}{\forall x \forall y Rxy}\Ai{Ray}
	\end{proof}
\item	\begin{proof}
		\hypo{AE}{\forall x \exists y Rxy}
		\have{E}{\exists y Ray}\Ae{AE}
		\open
			\hypo{ass}{Raa}
			\have{Ex}{\exists x Rxx}\Ei{ass}
		\close
		\have{con}{\exists x Rxx}\Ee{E, ass-Ex}
	\end{proof}
\item \begin{proof}
	\open\hypo{a}{\exists y \enot(Ty \eor \enot Ty)}
	\open
	\hypo{b}{\enot(Td \eor \enot Td)}
	\open
	\hypo{c}{Td}
	\have{d}{Td \eor \enot Td}\oi{c}
	\have{e}{\enot(Td \eor \enot Td)}\by{R}{b}
	\close
	\have{f}{\enot Td}\nelim{c-d,c-e}
	\have{g}{(Td \eor \enot Td)}\oi{f}
	\have{h}{\enot(Td \eor \enot Td)}\by{R}{b}
	\have{c2}{((Td \eor \enot Td)\eand \enot(Td \eor \enot Td))}\ai{g,h}
	\close
	\have{c1}{((Td \eor \enot Td)\eand \enot(Td \eor \enot Td))}\Ee{a,b-c2}
	\close
	\have{con}{\enot\exists y \enot(Ty \eor \enot Ty)}\nintro{a-c1}
\end{proof}
\end{enumerate}

\problempart 
\label{pr.justifyFOLproof}
The following three proofs are missing their commentaries (rule and line numbers). Add them, to turn them into bona fide proofs. 
\begin{multicols}{2}\begin{proof}
\hypo{p1}{\forall x\exists y(Rxy \eor Ryx)}
\open\hypo{p2}{\forall x \enot Rmx}
\have{3}{\exists y(Rmy \eor Rym)}%\Ae{p1}
	\open
		\hypo{a1}{Rma \eor Ram}
\open
\hypo{ram}{Ram}
\close
\open
\hypo{rma}{Rma}
\open
\hypo{nram}{\enot Ram}
\have{a2}{\enot Rma}%\Ae{p2}
\close
\have{r}{Ram}%\nelim{nram-rma,nram-a2}
\close
\have{a3}{Ram}%\oe{a1,ram-ram,rma-r}
		\have{a4}{\exists x Rxm}%\Ei{a3}
	\close
\have{n}{\exists x Rxm}%\Ee{3,a1-a4}
\end{proof}
\begin{proof}
\hypo{1}{\forall x(\exists yLxy \eif \forall zLzx)}
\open\hypo{2}{Lab}
\have{3}{\exists y Lay \eif \forall zLza}{}
\have{4}{\exists y Lay} {}
\have{5}{\forall z Lza} {}
\have{6}{Lca}{}
\have{7}{\exists y Lcy \eif \forall zLzc}{}
\have{8}{\exists y Lcy}{}
\have{9}{\forall z Lzc}{}
\have{10}{Lcc}{}
\have{11}{\forall x Lxx}{}
\end{proof}
\begin{proof}
\hypo{a}{\forall x(Jx \eif Kx)}
\open\hypo{b}{\exists x\forall y Lxy}
\open	\hypo{c}{\forall x Jx}
\open
	\hypo{2}{\forall y Lay}
	\have{3}{Laa}{}
	\have{d}{Ja}{}
	\have{e}{Ja \eif Ka}{}
	\have{f}{Ka}{}
	\have{4}{Ka \eand Laa}{}
	\have{5}{\exists x(Kx \eand Lxx)}{}
\close
\have{j}{\exists x(Kx \eand Lxx)}{}
\end{proof}
\end{multicols}


\problempart
\label{pr.BarbaraEtc.proof1}
In §\ref{s:MoreMonadic} problem part A, we considered fifteen syllogistic figures of Aristotelian logic. Provide proofs for each of the argument forms. NB: You will find it \emph{much} easier if you symbolise (for example) `No F is G' as `$\forall x (Fx \eif \enot Gx)$'.


\problempart
\label{pr.BarbaraEtc.proof2}
Aristotle and his successors identified other syllogistic forms which depended upon `existential import'. Symbolise each of the following argument forms in \FOL\ and offer proofs.
\begin{itemize}
	\item \textbf{Barbari.} Something is H. All G are F. All H are G. So: Some H is F
	\item \textbf{Celaront.} Something is H. No G are F. All H are G. So: Some H is not F
	\item \textbf{Cesaro.} Something is H. No F are G. All H are G. So: Some H is not F.
	\item \textbf{Camestros.} Something is H. All F are G. No H are G. So: Some H is not F.
	\item \textbf{Felapton.} Something is G. No G are F. All G are H. So: Some H is not F.
	\item \textbf{Darapti.} Something is G. All G are F. All G are H. So: Some H is F.
	\item \textbf{Calemos.} Something is H. All F are G. No G are H. So: Some H is not F.
	\item \textbf{Fesapo.} Something is G. No F is G. All G are H. So: Some H is not F.
	\item \textbf{Bamalip.} Something is F. All F are G. All G are H. So: Some H are F.
\end{itemize}
\newpage
\problempart
\label{pr.someFOLproofs}
Provide a proof of each claim.
\begin{earg}
\item $\proves \forall x Fx \eor \enot \forall x Fx$
\item $\proves\forall z (Pz \eor \enot Pz)$
\item $\forall x(Ax\eif Bx), \exists x Ax \proves \exists x Bx$
\item $\forall x(Mx \eiff Nx), Ma\eand\exists x Rxa\proves \exists x Nx$
\item $\forall x \forall y Gxy\proves\exists x Gxx$
\item $\proves\forall x Rxx\eif \exists x \exists y Rxy$
\item $\proves\forall y \exists x (Qy \eif Qx)$
\item $Na \eif \forall x(Mx \eiff Ma), Ma, \enot Mb\proves \enot Na$
\item $\forall x \forall y (Gxy \eif Gyx) \proves \forall x\forall y (Gxy \eiff Gyx)$
\item $\forall x(\enot Mx \eor Ljx), \forall x(Bx\eif Ljx), \forall x(Mx\eor Bx)\proves \forall xLjx$
\end{earg}

\problempart
\label{pr.likes}
Write a symbolisation key for the following argument, symbolise it, and prove it:
\begin{quote}
There is someone who likes everyone who likes everyone that she likes. Therefore, there is someone who likes herself.
\end{quote}

\problempart
\label{pr.FOLequivornot}
For each of the following pairs of sentences: If they are provably equivalent, give proofs to show this. If they are not, construct an interpretation to show that they are not logically equivalent.
\begin{earg}
\item $\forall x Px \eif Qc, \forall x (Px \eif Qc)$
\item $\forall x\forall y \forall z Bxyz, \forall x Bxxx$
\item $\forall x\forall y Dxy, \forall y\forall x Dxy$
\item $\exists x\forall y Dxy, \forall y\exists x Dxy$
\item $\forall x (Rca \eiff Rxa), Rca \eiff \forall x Rxa$
\end{earg}

\problempart
\label{pr.FOLvalidornot}
For each of the following arguments: If it is valid in \FOL, give a proof. If it is invalid, construct an interpretation to show that it is invalid.
\begin{earg}
\item $\exists y\forall x Rxy \ttherefore \forall x\exists y Rxy$
\item $\exists x(Px \eand \enot Qx) \ttherefore \forall x(Px \eif \enot Qx)$
\item $\forall x(Sx \eif Ta), Sd \ttherefore Ta$
\item $\forall x(Ax\eif Bx), \forall x(Bx \eif Cx) \ttherefore \forall x(Ax \eif Cx)$
\item $\exists x(Dx \eor Ex), \forall x(Dx \eif Fx) \ttherefore \exists x(Dx \eand Fx)$
\item $\forall x\forall y(Rxy \eor Ryx) \ttherefore Rjj$
\item $\exists x\exists y(Rxy \eor Ryx) \ttherefore Rjj$
\item $\forall x Px \eif \forall x Qx, \exists x \enot Px \ttherefore \exists x \enot Qx$
\end{earg}


\chapter{Derived Rules for \textnormal{\FOL}}\label{s:CQ}

In this section, we shall add some additional rules to the basic rules of the previous section. These govern the interaction of quantifiers and negation. But they are no substantive addition to our basic rules: for each of the proposed additions, it can be shown that their role in any proof can be wholly emulated by some suitable applications of our basic rules from §\ref{s:BasicFOL}. (The point here is as in §\ref{s:Derived}.)
 
\section{Conversion of Quantifiers}\label{cq.rules}

In §\ref{s:FOLBuildingBlocks}, we noted that $\enot\exists x\meta{A}$ is logically equivalent to $\forall x \enot\meta{A}$. We shall add some rules to our proof system that govern this. In particular, we add two rules, one for each direction of the equivalence:
	\factoidbox{\begin{minipage}{0.35\textwidth}
		\begin{proof}
		\have[m]{a}{\forall \meta{x} \enot\meta{A}}
		\have[\ ]{con}{\enot \exists \meta{x} \meta{A}}\by{CQ$_{\forall/\enot\exists}$}{a}
	\end{proof}
	\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
		\begin{proof}
		\have[m]{a}{ \enot \exists \meta{x} \meta{A}}
		\have[\ ]{con}{\forall  \meta{x} \enot \meta{A}}\by{CQ$_{\enot\exists/\forall}$}{a}
	\end{proof}
	\end{minipage}
	}

 Here is a schematic proof corresponding to our first conversion of quantifiers rule, CQ$_{\forall/\enot\exists}$:
\begin{proof}
	\hypo{An}{\forall \meta{x} \enot \meta{A}}
	\open
		\hypo{E}{\exists \meta{x} \meta{A}}
		\open
			\hypo{c}{\meta{A}\subs{\meta{c}}{\meta{x}}}%\by{for $\exists$E}{}
			\open
			\hypo{nb}{\enot(\meta{B}\eand\enot\meta{B})}
			\have{nc}{\enot \meta{A}\subs{\meta{c}}{\meta{x}}}\Ae{An}
		\close
		\have{b}{\meta{B}\eand\enot\meta{B}}\nintro{nb-nc,nb-c}
		\close
		\have{red2}{\meta{B}\eand\enot\meta{B}}\Ee{E,c-b}
		\close
		\have{cc}{\enot\exists\meta{x}\meta{A}}\nintro{E-red2}
\end{proof}
A couple of things to note about this proof. \begin{enumerate}
\item I was hasty at line 9 – officially I ought to have applied $\eand$E to line 8, obtaining the contradictory conjuncts in the subproof, and then applied $\enot$I to the assumption opening that subproof. (But then the proof would have gone over the page.)
\item Note that we had to introduce the new name \meta{c} at line 3. Once we did so, there was no obstacle to applying $\forall$E on that newly introduced name in line 5. But if we had done things the other way around, applying $\forall$E first to some new name \meta{c}, we would have had to open the subproof with yet another new name \meta{d}. 
	\item The sentence $\meta{B}$ cannot contain the name \meta{c} if the application of $\exists$E at line 8 is to be correct. We introduce this arbitrary logical falsehood precisely so we can show that the contradictoriness of our initial assumptions does not depend on the particular choice of name. The alternative would have been to show that the assumption $\meta{A}\subs{\meta{c}}{\meta{x}}$ leads to logical falsehood, and then applied $\enot$I – but that would have left the name \meta{c} outside the scope of a subproof and would not have allowed us to apply $\exists$E.
\end{enumerate}

A similar schematic proof could be offered for the second conversion rule, CQ$_{\enot\exists/\forall}$.

Equally, we might add rules corresponding to the equivalence of $\exists\meta{x}\enot\meta{A}$ and $\enot\forall\meta{x}\meta{A}$:
\factoidbox{\begin{minipage}{0.35\textwidth}
	\begin{proof}
		\have[m]{a}{\exists \meta{x}\enot \meta{A}}
		\have[\ ]{con}{\enot \forall \meta{x} \meta{A}}\by{CQ$_{\exists/\enot\forall}$}{a}
	\end{proof}
\end{minipage}\qquad\begin{minipage}{0.35\textwidth}
	\begin{proof}
		\have[m]{a}{\enot \forall \meta{x} \meta{A}}
		\have[\ ]{con}{\exists \meta{x} \enot \meta{A}}\by{CQ$_{\enot\forall/\exists}$}{a}
	\end{proof}
\end{minipage}
	}

Here is a schematic basic proof showing that the third conversion of quantifiers rule just introduced, CQ$_{\exists/\enot\forall}$, can be emulated just using the standard quantifier rules in combination with the other rules of our system, in which some of the same issues arise as in the earlier schematic proof: 
\begin{proof}
	\hypo{nEna}{\exists \meta{x}  \enot \meta{A}} 
	\open
		\hypo{Aa}{\forall \meta{x} \meta{A}}
		\open
			\hypo{nac}{\enot \meta{A}\subs{\meta{c}}{\meta{x}}}
			\open
				\hypo{nb}{\enot(\meta{B}\eand\enot\meta{B})}
				\have{a}{\meta{A}\subs{\meta{c}}{\meta{x}}}\Ae{Aa}
				\have{r}{\enot\meta{A}\subs{\meta{c}}{\meta{x}}}\by{R}{nac}
			\close
			\have{con}{\meta{B}\eand\enot\meta{B}}\nelim{nb-a,nb-r}
		\close
		\have{con1}{\meta{B}\eand\enot\meta{B}}\Ee{nEna, nac-con}
	\close
	\have{dada}{\enot \forall \meta{x} \meta{A}}\nintro{Aa-con1}
\end{proof}
A similar schematic proof can be offered for the final CQ rule.

\section{Alternative Proof Systems for \FOL}

We saw in §\ref{s:alternates} that it is possible to formulate alternative proof systems that can nevertheless establish the same arguments are provable in \TFL. The same is true for \FOL. The idea is to get rid of the rules for one quantifier, retaining the rules governing the other quantifier, but then to take the conversion of quantifier rules as basic. 

So, for example, we could consider the system which has $\exists$I and $\exists$E, and also has CQ$_{\exists/\enot\forall}$ and CQ$_{\enot\forall/\exists}$. With these rules, we can emulate $\forall$E and $\forall$I. A schematic proof showing how to emulate $\forall$E using our other basic rules is this:
\begin{proof}
	\have{a}{\forall \meta{x}\meta{A}}
	\open
	\hypo{b}{\enot\meta{A}\subs{\meta{c}}{\meta{x}}}
	\have{c}{\exists \meta{x}\enot\meta{A}}\Ei{b}
	\have{d}{\enot\forall \meta{x}\meta{A}}\by{CQ$_{\exists/\enot\forall}$}{c}
	\have{e}{\forall \meta{x}\meta{A}}\by{R}{a}
	\close
	\have{f}{\meta{A}\subs{\meta{c}}{\meta{x}}}\nelim{b-e,b-d}
\end{proof}

A schematic proof emulating $\forall$I using our other basic rules is trickier. Here it is: 
\begin{proof}
	\hypo{aaa}{\Gamma}
	\have[\ ]{}{\vdots}
	\open
	\hypo[m]{a}{\enot\forall \meta{x}\meta{A}}
	\have{b}{\exists \meta{x}\enot\meta{A}}\by{CQ$_{\enot\forall/\exists}$}{a}
	\open
	\hypo{c}{\enot\meta{A}\subs{\meta{c}}{\meta{x}}}
	\open
	\hypo{d}{\enot(\meta{B}\eand\enot\meta{B})}
	\have[\ ]{}{\vdots}
	\have[n]{e}{\meta{A}\subs{\meta{c}}{\meta{x}}}\by{Original proof from}{aaa}
	\have{r}{\enot\meta{A}\subs{\meta{c}}{\meta{x}}}\by{R}{c}
	\close
	\have{f}{\meta{B}\eand\enot\meta{B}}\nelim{d-e,d-r}
	\close
	\have{g}{\meta{B}\eand\enot\meta{B}}\Ee{b,c-f}
	\close
	\have{h}{\forall \meta{x}\meta{A}}\nelim{a-g}
\end{proof}
To understand this schematic proof, what we need to remember is that, in order for the original $\forall$I rule to apply, we must already have a proof of $\meta{A}\subs{\meta{c}}{\meta{x}}$ which relies on assumptions $\Gamma$ that do not mention \meta{c} at all. The trick is to make use of that proof \emph{inside} an assumption about an existential witness. We don't try to perform that proof to derive $\meta{A}\subs{\meta{c}}{\meta{x}}$ and then attempt to manipulate $\enot\forall \meta{xA}$ to generate a logical falsehood. Rather, we first assume $\enot\forall \meta{xA}$, apply quantifier conversion to obtain $\exists \meta{x}\enot\meta{A}$, assume that \meta{c} witnesses that existential claim so that $\enot\meta{A}\subs{\meta{c}}{\meta{x}}$, and then use our original proof to derive $\meta{A}\subs{\meta{c}}{\meta{x}}$ at line $n$. To avoid problems with the name appearing at the bottom of the existential witness subproof, we perform the same trick of assuming the falsehood of an arbitrary logical falsehood (so long as \meta{B} doesn't include \meta{c}), and then we manage to derive from $\Gamma$ what we had hoped to: that $\forall \meta{xA}$.

\keyideas{
	\item The derived rules for \FOL\ concern the interaction of quantifiers with negation.
	\item Do not make use of these derived rules unless you are explicitly told you may do so.
}

\practiceproblems

\problempart
Show that the following are jointly contrary:
\begin{earg}
\item $Sa\eif Tm, Tm \eif Sa, Tm \eand \enot Sa$
\item $\enot\exists x Rxa, \forall x \forall y Ryx$
\item $\enot\exists x \exists y Lxy, Laa$
\item $\forall x(Px \eif Qx), \forall z(Pz \eif Rz), \forall y Py, \enot Qa \eand \enot Rb$
\end{earg}

\problempart
Show that each pair of sentences is provably equivalent:
\begin{earg}
\item $\forall x (Ax\eif \enot Bx), \enot\exists x(Ax \eand Bx)$
\item $\forall x (\enot Ax\eif Bd), \forall x Ax \eor Bd$
\end{earg}

\problempart
In §\ref{s:MoreMonadic}, I considered what happens when we move quantifiers `across' various connectives. Show that each pair of sentences is provably equivalent:
\begin{earg}
\item $\forall x (Fx \eand Ga), \forall x Fx \eand Ga$
\item $\exists x (Fx \eor Ga), \exists x Fx \eor Ga$
\item $\forall x(Ga \eif Fx), Ga \eif \forall x Fx$
\item $\forall x(Fx \eif Ga), \exists x Fx \eif Ga$
\item $\exists x(Ga \eif Fx), Ga \eif \exists x Fx$
\item $\exists x(Fx \eif Ga), \forall x Fx \eif Ga$
\end{earg}
NB: the variable `$x$' does not occur in `$Ga$'.

When all the quantifiers occur at the beginning of a sentence, that sentence is said to be in \emph{prenex normal form}. These equivalences are sometimes called \emph{prenexing rules}, since they give us a means for putting any sentence into prenex normal form.



\problempart
Offer proofs which justify the addition of the other CQ rules as derived rules.



\chapter{Rules for Identity}\label{ch.identity}

\section{Identity Introduction}\label{idint}
In §\ref{s:Interpretations}, I mentioned the philosophically contentious thesis of the \emph{identity of indiscernibles}. This is the claim that objects which are indiscernible in every way – which means, for us, that exactly the same predicates are true of both objects – are, in fact, identical to each other. I also mentioned that in \FOL, this thesis is not true. There are interpretations in which, for each property denoted by any predicate of the language, two distinct objects either both have that property, or both lack that property. They may well differ on some property `in reality', but there is nothing in \FOL\ which guarantees that that property is assigned as the interpretation of any predicate of the language. 
It follows that, no matter how many sentences of \FOL\ about those two objects I assume, those sentences will not entail that these distinct objects are identical (luckily for us). Unless, of course, you tell me that the two objects are, in fact, identical. But then the argument will hardly be very illuminating.

The consequence of this, for our proof system, is that there are no sentences that do not already contain the identity predicate that could justify the conclusion `$a=b$'. This means that the identity introduction rule will not justify `$a=b$', or any other identity claim containing two different names.

However, every object is identical to itself. No premises, then, are required in order to conclude that something is identical to itself. So this will be the identity introduction rule:
\factoidbox{
\begin{proof}
	\have[\ \,\,\,]{x}{\meta{c}=\meta{c}} \idi{}
\end{proof}}
Notice that this rule does not require referring to any prior lines of the proof, nor does it rely on any assumptions. For any name \meta{c}, you can write $\meta{c}=\meta{c}$ at any point, with only the {=}I rule as justification. 

Recall that a relation is reflexive iff it holds between anything in the domain and itself (§§\ref{lli} and\ref{binary}). Let's see this rule in action, in a proof that identity is reflexive:
\begin{proof}
	\have{a}{a=a}\idi{}
	\have{b}{\forall x x=x}\Ai{a}
\end{proof} This seems like magic! But note that the first line is not an assumption (there is no horizontal line), and hence not an undischarged assumption. So the constant `$a$' appears in no undischarged assumption or anywhere in the proof other than in `$x=x$'$\subs{a}{x}$, so the conclusion `$\forall x x=x$' follows by legitimate application of the $\forall$I rule. So we've established the reflexivity of identity: $\proves \forall x x=x$.

This proof can seem equally magic: \begin{proof}
	\have{a}{a=a}\idi{}
	\have{b}{\exists x x=x}\Ei{a}
\end{proof} Again we've shown that there is something rather than nothing on the basis of no assumptions. Again, of course, it is the implicit assumption that every name in the proof refers that does the heavy lifting here.


\section{Identity Elimination} % (fold)
\label{idelim}


Our elimination rule is more fun. If you have established `$a=b$', then anything that is true of the object named by `$a$' must also be true of the object named by `$b$', since it just is the object named by `$a$'. \begin{quote}
	Superman is strong. But Superman is actually Clark Kent.\\
So, Clark Kent is strong.
\end{quote} So for any sentence with `$a$' in it, given the prior claim that `$a=b$', you can replace some or all of the occurrences of `$a$' with `$b$' and produce an equivalent sentence. For example, from `$Raa$' and `$a = b$', you are justified in inferring `$Rab$', `$Rba$' or `$Rbb$'. More generally:
\factoidbox{\begin{proof}
	\have[m]{e}{\meta{a}=\meta{b}}
	\have[n]{a}{\meta{A}\subs{\meta{a}}{\meta{x}}}
	\have[\ ]{ea1}{\meta{A}\subs{\meta{b}}{\meta{x}}} \ide{e,a}
\end{proof}}This uses our standard notion of substitution – it basically says that if you have some sentence which arises from substituting $\meta{a}$ for some variable in a formula, then you are entitled to another substitution instance of the same formula using $\meta{b}$ instead. 
 Lines $m$ and $n$ can occur in either order, and do not need to be adjacent, but we always cite the statement of identity first. 

Note that nothing in the rule forbids the constant $\meta{b}$ from occurring in \meta{A}. So this is a perfectly good instance of the rule: \begin{proof}
	\hypo{ab}{a=b}
	\hypo{a}{Rab}
	\have{b}{Rbb}\ide{ab,a}
\end{proof} Here, `$Rab$' is `$Rxb$'$\subs{a}{x}$, and the conclusion `$Rbb$' is `$Rxb$'$\subs{b}{x}$, which conforms to the rule. This formulation allows us, in effect, to replace some-but-not-all occurrences of a name in a sentence by a co-referring name.
This rule is sometimes called \emph{Leibniz's Law} – though recall §\ref{lli}, where we used that name for a claim about the interpretation of `$=$'. Here's a slightly more complex example of the rule in action: \begin{proof}
	\hypo{ax}{\forall x x = d}
	\open
	\hypo{fa}{Fd}
	\have{ba}{j=d}\Ae{ax}
	\have{aa}{j=j}\idi{}
	\have{ab}{d=j}\by{=E}{ba,aa}
	\have{fa}{Fj}\ide{ab,fa}
	\have{af}{\forall x Fx}\Ai{fa}
\end{proof} This proof has two features worth commenting on. First, the name `$j$' occurs on the second last line, but no undischarged assumption uses it, so it is correct to apply $\forall$I on the last line. 

The second thing to note is the curious sequence of steps at lines 3–5. We need to do that because the $=$E rule takes an identity statement of the form `$\meta{a}=\meta{b}$', and a sentence containing `$\meta{a}$' – the name on the \emph{left} of the identity – and generates a sentence containing `$\meta{b}$', the name of the right of the identity. But in our proof we ended up with `$j=d$' and `$Fd$' – strictly speaking, the identity rule doesn't apply to these sentences, because that would be to substitute the name on the right of the identity into `$Fd$'. The sequence of steps at lines 3–5 allows us to `flip' an identity. We start with `$j=d$' and we want to substitute one occurence of `$j$' in `$j=j$' for `$d$'. That is allowed, because `$j=j$' is the same as `$x=j\subs{j}{x}$'. That yields `$x=j\subs{d}{x}$', or `$d=j$' on line 5, which is just what we need to yield line 6.

To see the rules in action, we shall prove some quick results. Recall that a relation is symmetric iff whenever it holds between x and y in one direction, it holds also between y and x in the other direction (§\ref{graph}). This condition can be expressed as a sentence of \FOL: 


 So first, we shall prove that identity is symmetric, a result we already noted on semantic grounds in §§\ref{lli} and \ref{binary}. That is, $\proves \forall x \forall y (x = y \eif y = x)$:
\begin{proof}
	\open
		\hypo{ab}{a = b}
		\have{aa}{a = a}\idi{}
		\have{ba}{b = a}\ide{ab, aa}
	\close
	\have{abba}{a = b \eif b =a}\ci{ab-ba}
	\have{ayya}{\forall y (a = y \eif y = a)}\Ai{abba}
	\have{xyyx}{\forall x \forall y (x = y \eif y = x)}\Ai{ayya}
\end{proof}
Line 2 is just `$x=a$'$\subs{a}{x}$, as well as being of the right form for $=$I, and line 3 is just `$x=a$'$\subs{b}{x}$, so the move from 2 to 3 is in conformity with the $=$E rule given the opening assumption. This is the same sequence of moves we saw in the proof above, in a more general setting.

Having noted the symmetry of identity, note that we can use this to establish the following schematic proof that allows us to use $\meta{a}=\meta{b}$ to also move from a claim about $\meta{b}$ to a claim about $\meta{a}$, not just \emph{vice versa} as in our $=$E rule:
\begin{proof}
	\have[m]{j}{\meta{a}=\meta{b}}
	\have{i}{\meta{a}=\meta{a}}\idi{}
	\have{e}{\meta{b}=\meta{a}}\ide{j,i}
	\have[n]{a}{\meta{A}\subs{\meta{b}}{\meta{x}}}
	\have[\ ]{ea1}{\meta{A}\subs{\meta{a}}{\meta{x}}} \ide{e,a}
\end{proof} This schematic proof justifies this derived rule, to save time:\label{id.es} \factoidbox{
	\begin{proof}
		\have[m]{e}{\meta{a}=\meta{b}}
	\have[n]{a}{\meta{A}\subs{\meta{b}}{\meta{x}}}
	\have[\ ]{ea1}{\meta{A}\subs{\meta{a}}{\meta{x}}} \by{=ES}{e,a}
	\end{proof}
} 

Recall the rule of Conjunction Elimination (§\ref{conjelim}), which had two versions: from $\meta{A} \eand \meta{B}$ derive \meta{A} (the `left' version) and from $\meta{A} \eand \meta{B}$ derive \meta{B} (the `right' version). Likewise we can think of Disjunction Introduction (§\ref{disjint}) as having two versions: from $\meta{C}$ derive $\meta{C}\eor \meta{D}$ (the `left' version), and   from $\meta{C}$ derive $\meta{D}\eor \meta{C}$ (the `right' version). Our derived rule $=$ES shows that our original $=$E rule could be understood in a parallel way: \begin{itemize}
	\item There is a `left' version, where from $\meta{a}=\meta{b}$ and $\meta{A}\subs{\meta{a}}{\meta{x}}$ we derive $\meta{A}\subs{\meta{b}}{\meta{x}}$ (swapping the thing on the left for the thing on the right – this is the `official' $=$E rule); 
	\item and there is a `right' version where from $\meta{a}=\meta{b}$ and $\meta{A}\subs{\meta{b}}{\meta{x}}$ we derive $\meta{A}\subs{\meta{a}}{\meta{x}}$ (swapping the thing on the right for the thing on the left – this is the derived $=$ES rule).
\end{itemize} 
Given that we don't distinguish the left and right versions of rules for conjunction and disjunction (or biconditional), you are permitted in your own proofs to use $=$E for either version of the rule – once you understand the difference between the official $=$E rule and the derived rule $=$ES, you should feel free to conflate them and call them both `$=$E' in your proof commentaries.

A relation is transitive (§\ref{binary}) iff whenever it holds between x and y and between y and z, it also holds between x and z. (In the directed graph representation of the relation introduced in §\ref{graph}, if there is a path along arrows going from node a to node b via a third node, there is also a direct arrow from a to b.) Second, we shall prove that identity is transitive, that $\proves \forall x \forall y \forall z((x = y \eand y = z) \eif x = z)$.
\begin{proof}
	\open
		\hypo{abc}{a = b \eand b = c}
		\have{bc}{b = c}\ae{abc}
		\have{ab}{a = b}\ae{abc}
		\have{ac}{a = c}\ide{bc,ab}
	\close
	\have{con}{(a = b \eand b =c) \eif a = c}\ci{abc-ac}
	\have{conz}{\forall z((a = b \eand b = z) \eif a = z)}\Ai{con}
	\have{cony}{\forall y\forall z((a = y \eand y = z) \eif a = z)}\Ai{conz}
	\have{conx}{\forall x \forall y \forall z((x = y \eand y = z) \eif x = z)}\Ai{cony}
\end{proof}
We obtain line 4 by replacing `$b$' in line 3 with `$c$'; this is justified given line 2, `$b=c$'. We could alternatively have used the derived rule =ES to replace `$b$' in line 2 with `$a$', justified by line 3, `$a=b$'. 

Recall from §\ref{binary} that a relation that is reflexive, symmetric, and transitive is an equivalence relation. So we've formally proved that identity is an equivalence relation. We can also give formal proofs of other features of identity, such as a proof that identity is serial.

\subsection{Definite Descriptions in Proofs}

I want to close this section by giving an example of how proofs involving definite descriptions work. Remember from §\ref{subsec.defdesc} that we symbolised a definite description `the F is G', following Russell, as $$\exists x (\meta{F}x  \eand \forall y(\meta{F}y \eiff x=y) \eand \meta{G}x).$$ (I omit interior parentheses to aid legibility.) 

Using this kind of approach, let us consider this argument: \begin{quote}
	The F is G; the H isn't G; so the F isn't H.
\end{quote} The argument, symbolised, looks like this: 
\begin{align*}
	\exists x (Fx  \eand \forall y(Fy \eiff x=y)\eand Gx),& \exists x (Hx \eand \forall y(Hy \eiff x=y) \eand \enot Gx) \\ &\proves \exists x (Fx \eand \forall y(Fy \eiff x=y)\eand \enot Hx).
\end{align*} Here's the proof: \begin{proof}
	\hypo{a}{\exists x (Fx  \eand \forall y(Fy \eiff x=y)\eand Gx)}
	\hypo{b}{\exists x (Hx \eand \forall y(Hy \eiff x=y) \eand \enot Gx)}
	\open
	\hypo{c}{(Fa  \eand \forall y(Fy \eiff a=y)\eand Ga)}
	\open
	\hypo{d}{(Hb \eand \forall y(Hy \eiff b=y) \eand \enot Gb)}
	\open
	\hypo{e}{Ha}
	\have{f}{\forall y(Hy \eiff b=y)}\ae{d}
	\have{g}{(Ha \eiff b=a)}\Ae{f}
	\have{h}{b=a}\be{g,e}
	\have{i}{Ga}\ae{c}
	\have{j}{\enot Gb}\ae{d}
	\have{k}{\enot Ga}\ide{h,j}
	\close
	\have{l}{\enot Ha}\nintro{e-i,e-k}
	\close
	\have{m}{\enot Ha}\Ee{b,d-l}
	\have{n}{(Fa \eand \forall y (Fy \eiff a=y))}\ae{c}
	\have{o}{(Fa \eand \forall y (Fy \eiff a=y) \eand \enot Ha)}\ai{n,m}
	\have{p}{\exists x (Fx \eand \forall y (Fy \eiff x=y) \eand \enot Hx)}\Ei{o}
	\close
	\have{q}{\exists x (Fx \eand \forall y (Fy \eiff x=y) \eand \enot Hx)}\Ee{a,c-p}
\end{proof}

\keyideas{
	\item Adding rules for identity completes our presentation of the natural deduction system for \FOL.
	\item The identity elimination rule is basically Leibniz' Law. The idenity introduction rule expresses how trivial identity is when the same name flanks the identity symbol.
	\item We can prove all the properties we traditionally ascribe to identity – reflexivity, symmetry, and transitivity – so identity is an equivalence relation.
}
\newpage
\practiceproblems
\problempart
\label{pr.identity}
Provide a proof of each of the following.
\begin{earg}
\item $Pa \eor Qb, Qb \eif b=c, \enot Pa \proves Qc$
\item $m=n \eor n=o, An \proves Am \eor Ao$
\item $\forall x\ x=m, Rma\proves \exists x Rxx$
\item $\forall x\forall y(Rxy \eif x=y)\proves Rab \eif Rba$
\item $\enot \exists x\enot x = m \proves \forall x\forall y (Px \eif Py)$
\item $\exists x Jx, \exists x \enot Jx\proves \exists x \exists y\ \enot x = y$
\item $\forall x(x=n \eiff Mx), \forall x(Ox \eor \enot Mx)\proves On$
\item $\exists x Dx, \forall x(x=p \eiff Dx)\proves Dp$
\item $\exists x\bigl( (Kx \eand \forall y(Ky \eif x=y)) \eand Bx\bigr), Kd\proves Bd$
\item $\proves Pa \eif \forall x(Px \eor \enot x = a)$
\end{earg}

\problempart
Show that the following are provably equivalent:
\begin{itemize}
\item $\exists x \bigl(\bigl(Fx \eand \forall y (Fy \eif x = y) \bigr) \eand x = n\bigr)$
\item $Fn \eand \forall y (Fy \eif n= y)$
\end{itemize}
And hence that both have a decent claim to symbolise the English sentence `Nick is the F'.



\problempart
In §\ref{sec.identity}, I claimed that the following are logically equivalent symbolisations of the English sentence `there is exactly one F':
\begin{itemize}
\item $\exists x Fx \eand \forall x \forall y \bigl( (Fx \eand Fy) \eif x = y\bigr)$
\item $\exists x \bigl(Fx \eand \forall y (Fy \eif x = y) \bigr)$
\item $\exists x \forall y (Fy \eiff x = y)$
\end{itemize}
Show that they are all provably equivalent. (\emph{Hint}: to show that three claims are provably equivalent, it suffices to show that the first proves the second, the second proves the third and the third proves the first; think about why.)



\problempart
Symbolise the following argument
	\begin{quote}
		There is exactly one F. There is exactly one G. Nothing is both F and G. So: there are exactly two things that are either F or G.
	\end{quote}
And offer a proof of it.
%\begin{itemize}
%\item  $\exists x \bigl(Fx \eand \forall y (Fy \eif x = y) \bigr), \exists x \bigl(Gx \eand \forall y ( Gy \eif x = y) \bigr), \forall x (\enot Fx \eor \enot Gx) \proves \exists x \exists y \bigl(\enot x = y \eand \forall z ((Fz \eor Gz) \eif (x = y \eor x = z)) \bigr)$
%\end{itemize}

\problempart
What condition on the directed graph of a relation corresponds to that relation being an equivalence relation?

\chapter{Proof-Theoretic Concepts and Semantic Concepts}\label{sec:soundcomp}
We have used two different turnstiles in this book.  This:
$$\meta{A}_1, \meta{A}_2, …, \meta{A}_n \proves \meta{C}$$
means that there is some proof which starts with assumptions $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$ and ends with $\meta{C}$ (and no undischarged assumptions other than $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$). This is a \emph{proof-theoretic notion}.

By contrast, this:
$$\meta{A}_1, \meta{A}_2, …, \meta{A}_n \entails \meta{C}$$
means that there is no valuation (or interpretation) which makes all of $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$ true and makes $\meta{C}$ false. This concerns assignments of truth and falsity to sentences. It is a \emph{semantic notion}.

I cannot emphasise enough that these are different notions. But I can emphasise it a bit more: \emph{They are different notions.}

Once you have internalised this point, continue reading. 

\section{Connecting Entailment and Provability}

Although our semantic and proof-theoretic notions are different, there is a deep connection between them. To explain this connection, I shall start by considering the relationship between logical truths and theorems.

To show that a sentence is a theorem, you need only construct a proof. Granted, it may be hard to produce a twenty line proof, but it is not so hard to check each line of the proof and confirm that it is legitimate; and if each line of the proof individually is legitimate, then the whole proof is legitimate. Showing that a sentence is a logical truth, though, requires reasoning about all possible interpretations. Given a choice between showing that a sentence is a theorem and showing that it is a logical truth, it would be easier to show that it is a theorem.

Contrawise, to show that a sentence is \emph{not} a theorem is hard. We would need to reason about all (possible) proofs. That is very difficult. But to show that a sentence is not a logical truth, you need only construct an interpretation in which the sentence is false. Granted, it may be hard to come up with the interpretation; but once you have done so, it is relatively straightforward to check what truth value it assigns to a sentence. Given a choice between showing that a sentence is not a theorem and showing that it is not a logical truth, it would be easier to show that it is not a logical truth.

Fortunately, \emph{a sentence is a theorem if and only if it is a logical truth}. As a result, if we provide a proof of $\meta{A}$ on no assumptions, and thus show that $\meta{A}$ is a theorem, we can legitimately infer that $\meta{A}$ is a logical truth; i.e., $\entails\meta{A}$. Similarly, if we construct an interpretation in which \meta{A} is false and thus show that it is not a logical truth, it follows that \meta{A} is not a theorem.

More generally, we have the following powerful result:\factoidbox{
$$\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n} \proves\meta{B} \textbf{ iff }\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n} \entails\meta{B}.$$}
The left-to-right direction of this result, that provable argument really is a valid entailment, is known as \define{soundness} (different from soundness of an argument from §\ref{s:sound}). The right-to-left direction, that every entailment has a proof, is known as \define{completeness}. 


Soundness and completeness together show that, whilst provability and entailment are \emph{different} notions, they are extensionally equivalent, holding between just the same sentences in our languages. As such:
	\begin{itemize}
		\item An argument is \emph{valid} iff \emph{the conclusion can be proved from the premises}.
		\item Two sentences are \emph{logically equivalent} iff they are \emph{provably equivalent}.
		\item Sentences are \emph{jointly consistent} iff they are \emph{not jointly contrary}.
	\end{itemize}
For this reason, you can pick and choose when to think in terms of proofs and when to think in terms of valuations/interpretations, doing whichever is easier for a given task. Table \ref{sctable} summarises which is (usually) easier.

It is intuitive that provability and semantic entailment should agree. But – let me repeat this – do not be fooled by the similarity of the symbols `$\entails$' and `$\proves$'. These two symbols have very different meanings. And the fact that provability and semantic entailment agree is not an easy result to come by. 

We showed part of this result along the way, actually. All those little observations I made about how our proof rules were good each took the form of an argument that whenever $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n} \proves\meta{B}$, then also $\meta{A}_{1}, \meta{A}_{2}, …, \meta{A}_{n} \entails\meta{B}$. So in effect we've already established the soundness of our natural deduction proof system, when we justified those rules in terms of the existing understanding of the semantics we possess.



\begin{table}

\begin{tabular}[c]{p{.2\textwidth}p{.36\textwidth}p{.34\textwidth}}
 & \textbf{Yes}  & \textbf{No}\\
\\
Is \meta{A} a \textbf{logical truth}? 
& give a proof which shows $\proves\meta{A}$ 
& give an interpretation in which \meta{A} is false\\
\\
Is \meta{A} a \textbf{logical falsehood}? &
give a proof which shows $\proves\enot\meta{A}$ & 
give an interpretation in which \meta{A} is true\\
\\
%Is \meta{A} contingent? & 
%give two interpretations, one in which \meta{A} is true and another in which \meta{A} is false & give a proof which either shows $\proves\meta{A}$ or $\proves\enot\meta{A}$\\
%\\
Are \meta{A} and \meta{B} \textbf{equivalent}? &
give two proofs, one for $\meta{A}\proves\meta{B}$ and one for $\meta{B}\proves\meta{A}$  
& give an interpretation in which \meta{A} and \meta{B} have different truth values\\
\\
Are $\meta{A}_{1}, \meta{A}_2, …, \meta{A}_n$ \textbf{jointly consistent}? 
& give an interpretation in which all of $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$ are true 
& prove a logical falsehood from assumptions $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$\\
\\
Is $\meta{A}_1, \meta{A}_2, …, \meta{A}_n \ttherefore \meta{C}$ \textbf{valid}? 
& give a proof with assumptions $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$ and concluding with \meta{C}
& give an interpretation in which each of $\meta{A}_1, \meta{A}_2, …, \meta{A}_n$ is true and \meta{C} is false\\
\end{tabular}
\caption{Summary of what most efficiently demonstates various semantic and proof-theoretic properties, given the coextensiveness of `$\proves$' and `$\entails$'.\label{sctable}}
\end{table}

\keyideas{
	\item The two turnstiles are distinct concepts. But they are coextensive: every provable argument corresponds to an entailment, and \emph{vice versa}.
	\item This means that we can pick and choose our methods to suit our task. If we want to show an entailment, we can sometimes most effectively proceed by providing a proof. And if we want to show something isn't provable, it can be more efficient to provide a countermodel.
}

\chapter{Next Steps}\label{s:nextsteps}

That brings you to the end of the material in this course. But we've barely scratched the surface of the subject, and there are a number of further directions in which you could pursue the further study of logic. I will briefly indicate some of the further things you can do in logic, as well as some of the applications that logic has found in other fields. I'll also mention some further reading.

\section{Next Steps \emph{in} Logic}

The soundness and completeness theorems mentioned in the last section mark the stage at which introductory logic becomes intermediate logic. More than one of the authors of this book have written distinct sequels in which the completeness of \TFL\ is established, along with other results of a `metalogical' flavour. The interested student is directed to these open access texts: \begin{itemize}
	\item Tim Button's book is \emph{Metatheory}; it covers just \TFL, and some of the results there were actually covered in this book already, notably results around expressiveness of \TFL\ in §\ref{s:expressiveness}. \emph{Metatheory} is available at \href{http://www.homepages.ucl.ac.uk/~uctytbu/Metatheory.pdf}{\nolinkurl{www.homepages.ucl.ac.uk/~uctytbu/Metatheory.pdf}} It should be accessible for self-study by students who have successfully completed this course.

\item Antony Eagle's book \emph{Elements of Deductive Logic} goes a little further than \emph{Metatheory}, including direct proofs of Compactness for \TFL, consideration of alternative derivation systems, discussion of computation and decidability, and metatheory for \FOL. Not updated recently, it is available at \httpurl{github.com/antonyeagle/edl}.
\item The most comprehensive open access logic texts are those belonging to the Open Logic Project (\httpurl{openlogicproject.org}). There are open texts on intermediate logic, set theory, modal logic, non-classical logics. For most of the topics I touch on below, the Open Logic Project texts are reliable and accessible sources. There are many texts, remixing a common core of resources which together make up the whole open logic text. 
\end{itemize}

The metatheory of classical logic, the logic we've discussed, is well-understood. The subject of logic itself goes well beyond classical logic, in at least three ways.

\subsection{Using Logic} The original spur to Frege and colleagues in creating modern logic was to provide a framework in which mathematics could be formally regimented and mathematical proof could be systematically represented and checked. So a very standard next step in logic is to formalise various mathematical theories. This is standardly done by fixing a symbolisation key to give a mathematical interpretation to predicates and names of \FOL, and adding some axioms that carry substantive information about the interpretation. Often some new expressive resources are added too, such as \define{function symbols} and other term-forming operators. Some familiar binary function symbols include `$+$' and `$\cdot$' (multiplication): these take two terms and yield a complex term. They operate recursively, so complex terms can themselves be given as arguments. So `$7+5$', `$7+x$', and `$(3\cdot x) + 9$' are all complex terms. 

One standard formalised mathematical theory is \define{Robinson arithmetic} $\mathbf{Q}$.\footnote{Robinson arithmetic is weaker than full ordinary arithmetic, but occupies a special place in formal mathematics because of its role in the Gödel incompleteness theorems. Robinson arithmetic in more or less this form is discussed by George Boolos, John P Burgess and Richard C Jeffrey (2007) \emph{Computability and Logic}, 5th ed., Cambridge University Press, chapter 16.} This is a theory in the language of \FOL\ plus function symbols `$+$', `$\cdot$' and the symbol `$'$' for successor (the number immediately after a given number). There is one interpreted name, `$\mathbf{0}$', which names zero in the intended interpretation.
The axioms of the theory – sentences that are assumed to be true, and so serve to delimit the possible interpretations under consideration – are these, listed here together with their intended interpretation:
\begin{itemize}
	\item $\forall x\; x' \neq \mathbf{0}$ (zero isn't the successor of any number);
	\item $\forall x \forall y (x' = y' \eif x = y)$ (if $x$ and $y$ have the same successor, then $x$ is $y$);
	\item $\forall y (y = \mathbf{0} \eor \exists x y = x')$ (every number is either zero or the successor of a number);
	\item $\forall x\; x + \mathbf{0} = x$ (adding zero to a number results in that number);
	\item $\forall x \forall y\; x + y' = (x + y)'$ (the sum of $x$ and the successor of $y$ is the successor of the sum of $x$ and $y$);
	\item $\forall x\; x\cdot\mathbf{0} = \mathbf{0}$ (multiplying a number by zero results in zero);
	\item $\forall x \forall y\; x\cdot y' = (x\cdot y) + x$ (the product of $x$ and the successor of $y$ is the product of $x$ and $y$ plus $x$).
\end{itemize} These axioms serve to fix the intepretation of addition, multiplication, and successor. These axioms are all true in interpretations of \FOL\ in which the domain is the natural numbers, and the function symbols have their intended interpretation. The consequences of these axioms, those sentences that are true in every interpretation in which all the axioms are true, are the arithmetical truths that hold in Robinson arithmetic.\footnote{Another project of the same sort is the symbolisation of set theory in \FOL\ (see Patrick Suppes (1972) \emph{Axiomatic Set Theory}, Dover), or the symbolisations of mereology, the formal theory of part and whole (see Achille C Varzi (2016) `Mereology' in Edward N Zalta, ed., \emph{Stanford Encyclopedia of Philosophy}, \httpurl{plato.stanford.edu/entries/mereology/}).}

Once you have a theory like this, a collection of axioms with an intended interpretation symbolised in \FOL, you can ask questions about soundness and completeness of these theories with respect to that model. Robinson arithmetic is sound with respect to its intended interpretation in the natural numbers. But it is incomplete: there are arithmetical truths it does not include. 

The most striking aspect of this result is that any arithmetical theory that includes Robinson arithmetic will be incomplete, in that there are truths that are not consequences of the axioms. This is the upshot of the famous limitative results that are the central target of most intermediate logic courses: Gödel's incompleteness theorem and Tarski's theorem on the indefinability of truth. These results rely essentially on the fact that even simple arithmetical theories include a device of \define{self-reference}, enabling them to define arithmetical formulae that `talk about' sentences of the language of arithmetic. By a diagonal argument reminiscent of the Liar paradox (`this sentence is not true'), Gödel and Tarski show that, as long as arithmetic is consistent, there will be true sentences that are not provable.\footnote{See Boolos, Burgess and Jeffrey, \emph{op. cit.}, and Raymond M Smullyan (2001) `Gödel’s Incompleteness Theorems', pp. 72–89 in Lou Goble, ed., \emph{The Blackwell Guide to Philosophical Logic}, Blackwell.} 

\subsection{Extending Logic}

The logic we have was designed to model mathematical arguments, so the uses mentioned above are unsurprising. But there are arguments on many topics outside of mathematics, and it is not obvious that the logics we have are suitable for these arguments.\footnote{Good sources for the logics discussed in this section and the next – especially tense, modal, and non-classical logics – are John P Burgess (2008) \emph{Philosophical logic}, Princeton University Press; JC Beall and Bas C van Fraassen (2003) \emph{Possibilities and Paradox}, Oxford University Press; Graham Priest (2008) \emph{An Introduction to Non-Classical Logic}, Cambridge University Press.}

We've already seen in §\ref{FOL.semantics} an example which we might hope is valid, but which isn't symbolised as a valid argument in \FOL: \begin{earg}
\item[] It is raining
\item[So:] It will be that it was raining.
\end{earg}
Given our liberal attitude to structural words (§\ref{ss.idargstr}), the natural idea is to take the tenses expressions – in this case, the future `will' and the past `was' – to be structural words. The standard approach of \define{tense logic} is to treat those tenses as monadic sentential connectives. Then if `$P$' means `it is raining', the argument could be symbolised like so: $P \ttherefore \mathop{\mathsf{will}}\mathop{\mathsf{was}}P$.

This symbolisation isn't especially helpful without some semantic understanding of what these tense operators mean. In the classical logic of this text, sentences don't change their truth values within a valuation. This is not the way we introduced them, but we can think of a valuation as a snapshot, capturing a momentary assignment of truth values at a time. A \define{history} will be an ordered sequence of valuations, representing the way that the truth values of the atomic sentences change over time. (We may also need to mark a special valuation, the \emph{present} one, which represents how things actually are.) Then `$\mathop{\mathsf{will}}\meta{A}$' is true at a valuation in a history iff \meta{A} is true at some later valuation in that history; `$\mathop{\mathsf{was}}\meta{A}$' is true at a valuation in a history iff \meta{A} is true at some earlier valuation in that history. An argument is valid in a history iff every valuation at which the premises is true is also valuation in which the conclusion is true; and an argument is valid iff it is valid on every history. The argument above will turn out to be valid. Suppose `$P$' is true at a valuation $v$ in a history. Then at any valuation later than $v$, `$\mathop{\mathsf{was}}P$' is true. But then at $v$, there is a later valuation at which `$\mathop{\mathsf{was}}P$' is true, so that `$\mathop{\mathsf{will}}\mathop{\mathsf{was}}P$' is true at $v$. The history was chosen arbitrarily (it does need time to extend arbitrarily in both directions though), so the argument is valid.

This kind of structured collection of valuations we've called a history is also used in other extensions of classical logic. This is a non-exhaustive list of examples \begin{itemize}
	\item The logic that results from taking the sentence operators `necessarily' and `possibly' to be structural words, known as \define{modal logic}, uses a collection of valuations, which modal logicians tend picturesquely to call the \define{possible worlds};
	\item The logic that result from taking `obligatorily' and `permissibly' to be structural words, or \define{deontic logic}, also uses the same framework. There a collection of valuations corresponds to ideal possible worlds; \meta{A} is obligatory iff it is true at all ideal worlds, permissible iff it is true at some. This is an interesting logic because the actual world is not typically thought to be among the ideal worlds. 
\end{itemize} 
These constructions using valuations are extensions of \TFL. There are also quantified temporal/modal/deontic logics, where each moment of history (or each possible world) is an interpretation, not a valuation. There are some rich issues about how the domain should be permitted to vary over time, or just the extensions of predicates, as well as disputes over whether the temporal/modal operators should be permitted to occur within the scope of quantifiers. For example, should `$\exists x \mathop{\mathsf{was}}Fx$' be acceptable – that there is something which has the temporal property \emph{was $F$}? Or should we confine the temporal properties to tensed truth, i.e., it is only sentences which have the temporal properties of previously and subsequent truth.

A huge literature has grown up on the topic of \define{conditional logics}, logics which add new connectives, distinct from `$\eif$', hoping to better represent the behaviour of conditionals in English. Perhaps the most celebrated are the logics associated with Lewis-Stalnaker conditionals. In Lewis' version, the counterfactual conditional (§\ref{s:IndicativeSubjunctive}) has a certain modal force: it tells you about how things \emph{would have been} under alternative possible circumstances. So the logic of conditionals he develops draws on the framework of modal logic, with some innovations of his own. The Stalnaker conditional is similar, but Stalnaker wants to claim that the indicative conditional too has modal force.\footnote{See David Lewis (1973) \emph{Counterfactuals}, Blackwell, and Robert C Stalnaker (1975) ‘Indicative conditionals’, \emph{Philosophia} \textbf{5}: 269–286, \httpurl{doi.org/10.1007/bf02379021}.}

Finally, another common extension of \FOL\ is to allow quantification over possible extensions (i.e., collections of individuals in the domain). This is \define{second-order logic}, the thought being that first-order logic quantifies over individuals, second-order logic quantifies over collections of individuals, third-order logic over collections of collections of individuals, and so on. This allows us to represent arguments like this: \begin{earg}
	\item There is a property Bob has and Alice does not. $\exists X (Xb \eand \enot Xa)$
	\item[So:] Bob isn't Alice. $b≠a$
\end{earg}
This sort of example can seem trivial, but in fact second-order logic has many powerful features that allow it to vastly outstrip the expressive capacity of \FOL. It has so much power that the philosopher W V O Quine once said that second-order logic is `set theory in sheep's clothing' – it has substantial mathematical content, disguised up as if it were nothing but pure logic.\footnote{W V O Quine (1970) \emph{Philosophy of Logic}, Harvard University Press, at pp. 64–8.}

\subsection{Modifying Logic}\label{s:modifying}

Another direction we could take is not adding extra expressive resources to logic, but modifying the logic we already have in some way. We've already seen some attempts to do this, when we briefly considered intuitionistic logic and minimal logic and their restrictions on negation elimination in §\ref{construct}. 

More recently, influential alternative logics have been explored which result from restricting the structural rules permitted in proofs (§\ref{s:substructural}). These logics, even though they are purely sentential and lack quantifiers, turn out to be very complex and puzzling.

One prominent alternative is \define{linear logic}, which result from restrictions to the rule of contraction. Linear logic is sometimes said to be \emph{resource conscious}; it matters, in linear logic, how many times you need to appeal to an assumption in order to construct a proof. Some claims which might be provable by appealing twice to an assumption, may fail to be provable if you were permitted only to appeal once to that assumption. So in the assumptions of a linear logic proof it is important to note how many times an assumption appears – contraction, which says that the same things can be proved even if you throw away duplicate assumptions, is not compatible with keeping track of resources in this way. Linear logic has found uses in computer science, in modelling the behaviour of algorithms – in an actual computation, it can be very important to be efficient in the calls you make on resources. If appealing to an assumption involves reading that assumption from disk, for example, then the fewer appeals to the assumption you can make, the faster your algorithm will run.

Amongst philosophers, however, the most prominent substructural logic is \define{relevance logic} (US), aka \define{relevant logic} (UK/Australasia).\footnote{See Burgess, \emph{op. cit.}, Priest, \emph{op. cit.}, and Edwin Mares `Relevance Logic' in Edward N Zalta, ed., \emph{Stanford Encyclopedia of Philosophy}, \httpurl{plato.stanford.edu/entries/logic-relevance/}.} Relevant logicians argue that the so-called `paradoxes of material implication' (§\ref{s:ParadoxesOfMaterialConditional}) are symptoms of a broader failure of classical logic to require that the premises of a valid argument must be relevant to its conclusion. Relevant logicians are particularly incensed by the explosive fact that $A \eand \enot A \proves B$; accordingly, they need to block the standard proof:\footnote{You'll recall we saw more or less this exact proof in our discussion of the natural deduction rule of explosion in §\ref{exfalso}.} \begin{proof}
	\hypo{ana}{A \eand \enot A}
	\open
	\hypo{b}{\enot B}
	\have{c}{A}\Ae{ana}
	\have{d}{\enot A}\Ae{ana}
	\close
	\have{e}{B}\nelim{b-c,b-d}
\end{proof} The relevant logician doesn't wish to abandon any of these rules, or at least, not the intuition behind them. But they do generally want to resist our being able to introduce a new irrelevant assumption like `$\enot B$' whenever we want. So relevant logics are a class of substructural logic which don't satisfy weakening, and which block this proof by restricting the opening of arbitrary subproofs, precluding the irrelevant derivation of $B$ merely because of inconsistent prior assumptions. 

A final class of modifications to classical logic I will consider are \define{many-valued logics}: logics that go beyond two truth values.\footnote{See Priest, \emph{op. cit.}, and Beall and van Fraassen, \emph{op. cit.}.} Some many-valued logics introduce just a third truth value, \emph{indeterminate}, to represent the truth values of unsettled matters such as contingent future outcomes. Such logics also gained some purchase as conditional logics – not extending classical logic this time, but changing the behaviour of the existing conditional. For example, many resist the idea that a conditional should be true just because its antecedent is false. Perhaps we should say rather that the conditional is unsettled when the antecedent is false, because we just can't tell whether the consequent follows. 

Another use of many valued logics appeals to not just a third truth value, but infinitely many, sometimes called \define{degrees of truth}. Such logics have had some appeal to philosophers working on vagueness, a topic we will turn to shortly.

\section{Next Steps \emph{with} Logic}\label{stepwith}

There are lots of places where you might wish to apply logic: anywhere that a persuasive case needs to be made, or clarity and precision of expression is vital. But some areas are more active than others. One is mathematics, as already mentioned; and issues of decidability and effective provability lead quickly into theoretical computer science. 

But logic has possibly more suprising applications too: \begin{itemize}
	\item  In \emph{law}, where ambiguity needs to be resolved and the job of an advocate is (though only partly) to persuade through rational argument.\footnote{Henry Prakken and Giovanni Sartor (2015) ‘Law and Logic: a Review from an Argumentation Perspective’, \emph{Artificial Intelligence} \textbf{227}: 214–45, \httpurl{doi.org/10.1016/j.artint.2015.06.005}.} The skills of logical analysis are important in deciding whether legal argument is conclusive. Certainly the judge needs logic to see through sophistry and rhetoric. 
\item  In \emph{linguistics}, where methods from logic are foundational to formal theories of meaning.\footnote{See Paul Portner (2005) \emph{What is Meaning?}, Blackwell.} Many formal semantic theories invoke a level of sub-surface grammatical structure which is the input to semantic analysis, often called \define{logical form}. The properties and nature of this postulated logical form are heavily endebted to the kinds of logical resources available to the theorist. We saw a prominent example of different logical frameworks being brought to bear on a hypothesis about the `real' meaning of a natural language expression in our brief discussion of the analysis of definite descriptions in §\ref{subsec.defdesc}. 
\item  In \emph{management consulting} and related fields, the topic‐neutrality of logic and the habits of rigorous thought it encourages are very useful for people who have to offer recommendations about complex areas without necessarily having much subject-specific knowledge.
\end{itemize}

But I am a philosopher, and I'm interested primarily in applications of logic to philosophical puzzles. I'll close this discussion, and this book, with a brief account of one very prominent puzzle where logic illuminates the structure of the problem and the space of available solutions. It does not, unfortunately, decide everything for us – we need to make theoretical choices on substantive grounds, and logic, because topic-neutral, won't generally be decisive on such matters. 

The problem I'm talking about is the puzzle of \emph{vagueness}. (We briefly encountered this puzzle earlier in §\ref{s:ParadoxesOfMaterialConditional}, in connection with the topic of bivalence.) Consider the excerpt from the visible spectrum depicted in Figure~\ref{spectrum}. The spectrum looks continuous, but the discrete medium of the page suggests it is actually made up of many many thin slivers of apparently constant colour, laid adjacent to one another. Let there be 1000 slivers of colour in this spectrum. The spectrum looks continuous because the colour doesn't seem to change between adjacent slivers of colour. There are no sharp cutoffs or discontinuities in the spectrum.

\begin{figure}
\centering
\includegraphics[keepaspectratio,width=0.8\textwidth]{spectrum.png}
\caption{A red-yellow spectrum. \label{spectrum}}
\end{figure}

The left hand end is clearly red. But because the red areas shades smoothly into orange and then yellow, it seems clear that the word `red' denotes a predicate with no sharp cutoffs. It is a \define{vague expression}. Its vagueness, accordingly to many, arises because `red' and other vague words exhibit this feature: \factoidbox{A predicate `$F$' is \define{tolerant} if extremely similar things are either both $F$ or neither $F$.}
In the case of 'red', adjacent slivers of colour in the spectrum are very similar – they are visually indistinguishable, i.e., they \emph{look the same}. And surely if two things look the same, they cannot be different colours. How could there be two distinct colours that are indistinguishable in appearance? – colours are appearance properties, most say.

If `red' is tolerant, then any two adjacent slivers will be either both red, or both not red.  So where `$R$' is interpreted to mean `\gap{1} is red', and `$A$' means `\gap{1} is adjacent to \gap{2}', in the domain of slivers of colour in this spectrum, this \FOL\ sentence appears to be true: 
     $$\forall x \forall y ((Rx \eand Axy) \eif Ry).$$
This is an instance of the principle of Tolerance, because adjacent slivers are extremely similar. Let the slivers of colour be denoted `$a_{1},…,a_{1000}$', with `$a_{1}$' the leftmost sliver. Then the above tolerance principle, together with, `$Ra_{1}$', and many premises of the form `$Aa_{i}a_{i+1}$' will entail `$\forall x Rx$', given that there is a case of red, and we have enough adjacent cases: as we do in the spectrum.  The formal proof is long, but basically: if there is a case of red, and a case of non-red, then Tolerance tells us they cannot be linked by a sequence of adjacent cases. But in the spectrum any two slivers can be linked by a sequence of adjacent cases. 

There are a number of responses, some of which we've mentioned already.\footnote{For more on the puzzle of vagueness, see Roy Sorensen's 2022 entry `Vagueness' in \emph{The Stanford Encyclopedia of Philosophy} (\httpurl{plato.stanford.edu/entries/vagueness/}).} The degree-theory solution says that each adjacent sliver to the right is red to a slightly less degree than the sliver to the left, and that it is true to a lower degree that it is red. Such views abandon tolerance for vague predicates in favour of a principle of `closeness': that extremely similar things are both $F$ to an extremely similar degree.\footnote{See N J J Smith (2008) \emph{Vagueness and Degrees of Truth}, Oxford University Press.} There are also solutions that appeal to truth-value `gaps', resembling many-valued logics in some ways – most prominent among these is the approach known as \define{supervaluationism}.\footnote{See Kit Fine (1975) `Vagueness, truth and logic', \emph{Synthese} \textbf{54}: 235–59, \httpurl{doi.org/10.1007/bf00485047}.} Finally, there is a purely classical logic solution, Williamson's theory of \define{epistemicism}, which says that there are sharp cutoffs (so that the tolerance premise is false), but explains the appearance that there are no sharp cutoffs as a byproduct of our essential inability to know where the sharp boundaries lie. Logic won't decide between these approaches. But it does help us in formulating them precisely and in enabling us to classify their strengths and weaknesses precisely. This is an advantage in most areas of controversy and debate.


\keyideas{
	\item With the basics in hand, there are many next steps in logic. 
	\item It is natural to try to establish more metatheoretical results about the systems we have discussed, and some resources are indicated in the text to help you do this if you wish.
	\item There are various attempts to extend, modify and make use of logic, in mathematics, philosophy, computer science, and linguistics. The field is vast and many topics are underexplored.
	\item Logic is useful in clarifying controversies and debates, and is a tool you will likely use long after you finish with this book.
}

